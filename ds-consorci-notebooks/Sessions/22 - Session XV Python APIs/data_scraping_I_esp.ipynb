{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Búsqueda y Recopilación de Datos (Parte 1)\n",
    "\n",
    "![Web Scraping](http://unadocenade.com/wp-content/uploads/2012/09/cavalls-de-valltorta.jpg)\n",
    "\n",
    "Bienvenidos a la primera parte de nuestro viaje al mundo del web scraping. El web scraping, también conocido como recolección web o extracción de datos web, es una técnica utilizada para extraer datos de sitios web. Este proceso implica obtener la página web y luego extraer datos de ella.\n",
    "\n",
    "## ¿Por Qué Aprender Web Scraping?\n",
    "Entender cómo raspar datos de la web es una habilidad valiosa para cualquier profesional de datos. En la era digital, los datos son el nuevo oro, y el web scraping es el equipo de minería. Aquí está el porqué es esencial:\n",
    "\n",
    "- **Disponibilidad de Datos**: Internet es una vasta fuente de datos para todo tipo de análisis, desde tendencias del mercado hasta investigación académica.\n",
    "- **Automatización**: El web scraping puede automatizar el proceso de recopilación de datos, ahorrando tiempo y esfuerzo.\n",
    "- **Ventaja Competitiva**: En muchos campos, tener datos oportunos y relevantes puede ser un cambio de juego.\n",
    "\n",
    "## Aplicaciones en el Mundo Real\n",
    "- **Investigación de Mercado**: Analizar competidores, entender sentimientos de los clientes e identificar tendencias del mercado.\n",
    "- **Comparación de Precios**: Agregar datos de precios de varios sitios web para comparación de compras.\n",
    "- **Análisis de Redes Sociales**: Recopilar datos de redes sociales para análisis de sentimientos o detección de tendencias.\n",
    "\n",
    "## Consideraciones Éticas en el Web Scraping\n",
    "\n",
    "El web scraping, aunque es una técnica poderosa para la extracción de datos, viene con responsabilidades éticas y legales significativas. Como futuros científicos de datos y raspadores web, es crucial navegar este paisaje con una comprensión profunda y respeto por estas consideraciones.\n",
    "\n",
    "### Respetando las Políticas de los Sitios Web y las Leyes\n",
    "\n",
    "- **Adherencia a los Términos de Servicio**: Cada sitio web tiene su propio conjunto de reglas, generalmente delineadas en sus Términos de Servicio (ToS). Es importante leer y entender estas reglas antes de raspar, ya que violarlas puede tener implicaciones legales.\n",
    "\n",
    "- **Siguiendo las Leyes de Derechos de Autor**: Los datos que raspa a menudo están protegidos por derechos de autor. Asegúrese de que su uso de los datos raspados cumpla con las leyes de derechos de autor y respete los derechos de propiedad intelectual.\n",
    "\n",
    "- **Preocupaciones de Privacidad**: Tenga en cuenta los datos personales. Raspar y usar información personal sin consentimiento puede violar las leyes de privacidad y los estándares éticos.\n",
    "\n",
    "### Ejemplo: Entendiendo el `robots.txt` de Google\n",
    "\n",
    "El archivo `robots.txt` de Google es un excelente ejemplo de cómo los sitios web comunican sus políticas de raspado. Accesible en [Google's robots.txt](https://www.google.com/robots.txt), este archivo proporciona directivas a los rastreadores web sobre qué páginas pueden o no pueden raspar.\n",
    "\n",
    "#### Implicaciones del `robots.txt` de Google\n",
    "\n",
    "- **Acceso Selectivo**: Google permite que ciertas partes de su sitio sean rastreadas mientras restringe otras. Por ejemplo, generalmente se prohíbe rastrear las páginas de resultados de búsqueda.\n",
    "\n",
    "- **Naturaleza Dinámica**: El contenido de los archivos `robots.txt` puede cambiar, reflejando la postura evolutiva del sitio web sobre el web scraping. Se necesitan chequeos regulares para cumplir.\n",
    "\n",
    "- **Respetando los Límites**: Incluso si un archivo `robots.txt` permite el raspado de algunas páginas, no significa automáticamente que todas las actividades de raspado sean legal o éticamente aceptables. Es una guía, no un permiso general.\n",
    "\n",
    "# 1. Introducción a la Búsqueda de Datos en la Era Digital\n",
    "\n",
    "## La Evolución de la Obtención de Datos\n",
    "\n",
    "En este curso, nos centramos en los datos como nuestro elemento fundamental. Tradicionalmente, los datos se han obtenido de formatos estructurados como hojas de cálculo de experimentos científicos o registros en bases de datos relacionales dentro de las organizaciones. Pero con la revolución digital, particularmente el advenimiento de internet, nuestro enfoque para la recopilación de datos debe evolucionar. Internet es un vasto reservorio de datos no estructurados, presentando tanto desafíos como oportunidades para la recuperación y análisis de datos.\n",
    "\n",
    "## Entendiendo el Paisaje de los Datos Web\n",
    "\n",
    "Al buscar datos en internet, es esencial considerar primero cómo el sitio web en cuestión proporciona acceso a sus datos. Muchos sitios web a gran escala como Google, Facebook y Twitter ofrecen una **Interfaz de Programación de Aplicaciones (API)**. Las API están diseñadas para facilitar el acceso fácil a los datos de un sitio web en un formato estructurado, simplificando el proceso de extracción de datos.\n",
    "\n",
    "### El Papel de las API\n",
    "\n",
    "- **APIs como Herramienta Primaria**: Una API actúa como un puente entre el buscador de datos y la base de datos del sitio web, permitiendo una recuperación de datos simplificada.\n",
    "- **Limitaciones**: Sin embargo, no todos los sitios web proporcionan una API. Además, incluso cuando una API está disponible, puede no conceder acceso a todos los datos que un usuario podría necesitar.\n",
    "\n",
    "### La Necesidad del Web Scraping\n",
    "\n",
    "En casos donde una API está ausente o es insuficiente, recurrimos al **web scraping**. El web scraping implica extraer datos brutos directamente del frontend de un sitio web - esencialmente, la misma información presentada a los usuarios en sus navegadores web.\n",
    "\n",
    "#### Sumergiéndose en el Scraping\n",
    "\n",
    "- **Tratando con Datos No Estructurados**: El scraping nos obliga a interactuar con datos no estructurados, necesitando técnicas de codificación personalizadas y análisis de datos.\n",
    "- **Consideraciones Legales y Éticas**: Es crucial abordar el web scraping con conciencia de las implicaciones legales y éticas, respetando las políticas del sitio web y la privacidad del usuario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"border-radius:20px;\" src=\"./files/big_picture.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comenzando Nuestro Viaje\n",
    "\n",
    "Nuestro primer paso práctico en este viaje será explorar cómo conectarnos a internet y recuperar una página web básica. Comenzaremos utilizando el módulo `urllib.request` de Python, una herramienta poderosa para interactuar con URLs y manejar solicitudes web.\n",
    "\n",
    "Únete a nosotros mientras nos embarcamos en este emocionante viaje para dominar el arte de la búsqueda de datos en la era digital, donde navegaremos por las complejidades de las API, el web scraping y las consideraciones éticas que vienen con ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<http.client.HTTPResponse object at 0x000001FFC97A3C10>\n"
     ]
    }
   ],
   "source": [
    "# Import the 'urlopen' function from the 'urllib.request' module.\n",
    "# This function is used for opening URLs, which is the first step in web scraping.\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Use the 'urlopen' function to open the URL 'http://www.google.com/'.\n",
    "# The function returns a response object which can be used to read the content of the page.\n",
    "# Here, 'source' is a variable that holds the response object from the URL.\n",
    "source = urlopen(\"http://www.google.com/\")\n",
    "\n",
    "# Print the response object.\n",
    "# This command does not print the content of the webpage.\n",
    "# Instead, it prints a representation of the response object, \n",
    "# which includes information like the URL, HTTP response status, headers, etc.\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este fragmento de código demuestra el uso básico de la función `urlopen` para acceder a una página web. Sin embargo, es importante notar que `print(source)` no mostrará el contenido HTML de la página web, sino la representación del objeto de respuesta HTTP. Para ver el contenido real de la página, necesitarías leer del objeto `source` usando métodos como `source.read()`.\n",
    "\n",
    "# Explorando el Contenido Recuperado por `urlopen`\n",
    "\n",
    "Tras abrir una URL utilizando la función `urlopen` del módulo `urllib.request`, típicamente queremos acceder al contenido real de la página web. Aquí es donde `source.read()` entra en juego.\n",
    "\n",
    "## Entendiendo `source.read()`\n",
    "\n",
    "Cuando llamas a `urlopen`, devuelve un objeto HTTPResponse. Este objeto, que hemos nombrado `source` en nuestro ejemplo, contiene varios datos y metadatos sobre la página web. Para extraer el contenido HTML real de la página, utilizamos el método `read` en este objeto.\n",
    "\n",
    "### ¿Qué Hace `source.read()`?\n",
    "\n",
    "- **Recupera el Contenido de la Página Web**: `source.read()` lee el contenido completo de la página web a la que apunta la URL. Este contenido suele estar en formato HTML, que es el lenguaje estándar para crear páginas web.\n",
    "\n",
    "- **Formato Binario**: Los datos recuperados están en formato binario. Para trabajar con ellos como una cadena en Python, podrías necesitar decodificarlos usando un método como `.decode('utf-8')`.\n",
    "\n",
    "- **Operación de Única Vez**: Es importante notar que puedes leer el contenido de la respuesta solo una vez. Después de ejecutar `source.read()`, el objeto de respuesta no retiene el contenido de una forma legible. Si necesitas acceder al contenido de nuevo, debes reabrir la URL.\n",
    "\n",
    "Aquí tienes un ejemplo simple para ilustrar esto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us check what is in\n",
    "something = source.read()\n",
    "print(something)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check type\n",
    "type(something)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios de Calentamiento\n",
    "\n",
    "¡Manos a la obra con algunos ejercicios iniciales para calentar con el web scraping!\n",
    "\n",
    "## Ejercicios\n",
    "\n",
    "1. **Verificación de Contenido en Python.org**: ¿Contiene [https://www.python.org](https://www.python.org) la palabra `Python`?  \n",
    "   _Pista: Puedes usar la palabra clave `in` para verificar._\n",
    "\n",
    "2. **Búsqueda de Imagen en Google.com**: ¿Contiene [http://google.com](http://google.com) una imagen?  \n",
    "   _Pista: Busca la etiqueta `<img>`._\n",
    "\n",
    "3. **Primeros Caracteres de Python.org**: ¿Cuáles son los primeros diez caracteres de [https://www.python.org](https://www.python.org)?\n",
    "\n",
    "4. **Verificación de Palabra Clave en Pyladies.com**: ¿Está la palabra 'python' en [https://pyladies.com](https://pyladies.com)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EX1: Check if 'Python' is in the content of http://www.python.org/\n",
    "\n",
    "# Import the urlopen function from the urllib.request module\n",
    "# This function is used to open a URL and retrieve its contents\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Use the urlopen function to access the webpage at http://www.python.org/\n",
    "# The function returns an HTTPResponse object which is stored in the variable 'source'\n",
    "source = urlopen(\"http://www.python.org/\")\n",
    "\n",
    "# Read the content of the response object using the read() method\n",
    "# The read() method retrieves the content of the webpage in binary format\n",
    "# The binary content is then decoded to a string using the 'latin-1' encoding\n",
    "# The decoded string is stored in the variable 'something'\n",
    "something = source.read().decode('latin-1')\n",
    "\n",
    "# Check if the word \"Python\" is in the decoded string\n",
    "# This is done using the 'in' keyword, which checks for the presence of a substring in a string\n",
    "# The result is a boolean value: True if \"Python\" is found, False otherwise\n",
    "\"Python\" in something\n",
    "\n",
    "# Note: The choice of 'latin-1' for decoding might not always be appropriate\n",
    "# It's often better to use 'utf-8', which is a more common encoding for webpages\n",
    "# For example: something = source.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EX2: Check if 'https://www.google.com/' contains an image tag (\"<img>\")\n",
    "\n",
    "# Import the urlopen function from the urllib.request module.\n",
    "# This function is used to open a URL and retrieve its contents.\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Use the urlopen function to open the webpage at 'https://www.google.com/'.\n",
    "# The function returns an HTTPResponse object, which we store in the variable 'source'.\n",
    "source = urlopen(\"https://www.google.com/\")\n",
    "\n",
    "# Read the content of the response object using the read() method.\n",
    "# The read() method retrieves the content of the webpage in binary format.\n",
    "# After reading, the content is in bytes, which is not human-readable.\n",
    "# We then decode this binary content into a string using the 'latin-1' encoding.\n",
    "# The resulting string, which contains the HTML of the page, is stored in 'something'.\n",
    "something = source.read().decode('latin-1')\n",
    "\n",
    "# Check if the string \"img\" is in the decoded HTML content.\n",
    "# This is a simple way to check if there is an <img> tag in the HTML,\n",
    "# as \"img\" is part of the standard HTML tag for images.\n",
    "# The result will be True if \"img\" is found (indicating the presence of an image),\n",
    "# and False if not.\n",
    "\"img\" in something\n",
    "\n",
    "# Note: Decoding with 'latin-1' might not be suitable for all websites,\n",
    "# especially if the website uses a different character set.\n",
    "# 'utf-8' is a more commonly used encoding and is often a better choice.\n",
    "# For instance: something = source.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now is your turn for EX3 and EX4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando `urlopen` vs. `Request` en Web Scraping\n",
    "\n",
    "Al realizar tareas de web scraping en Python, tienes la opción de usar la función `urlopen` del módulo `urllib.request` o el objeto `Request` en combinación con `urlopen`. Aquí explicaremos por qué podrías elegir un enfoque sobre el otro.\n",
    "\n",
    "## Usando `urlopen` Directamente\n",
    "\n",
    "**Ventajas**:\n",
    "\n",
    "- **Simplicidad**: Es una forma directa de acceder a una página web y recuperar su contenido sin la necesidad de objetos adicionales o personalización.\n",
    "  \n",
    "- **Comportamiento Predeterminado**: `urlopen` utiliza la configuración predeterminada para la solicitud HTTP, lo cual es adecuado para muchos casos de uso comunes.\n",
    "\n",
    "- **Conveniencia**: Para tareas simples de web scraping, proporciona una solución concisa y legible.\n",
    "\n",
    "## Usando `Request` con `urlopen`\n",
    "\n",
    "**Ventajas**:\n",
    "\n",
    "- **Personalización**: Puedes establecer encabezados personalizados, utilizar diferentes métodos HTTP (por ejemplo, POST, PUT) y configurar opciones avanzadas como manejar redirecciones, cookies y tiempos de espera.\n",
    "\n",
    "- **Control Detallado**: Ofrece mayor flexibilidad para manejar escenarios complejos.\n",
    "\n",
    "En resumen, la elección entre usar `urlopen` directamente y crear un objeto `Request` depende de la complejidad de tu tarea de web scraping. Para tareas simples como obtener el contenido de una página web, `urlopen` a menudo es suficiente y más sencillo. Sin embargo, si necesitas personalizar encabezados, usar métodos HTTP que no sean GET, o manejar escenarios avanzados, crear un objeto `Request` permite un control detallado sobre tus solicitudes HTTP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution exercise 4\n",
    "# import urllib\n",
    "# url ='https://www.pyladies.com'\n",
    "# req = urllib.request.Request(url, headers = {'User-Agent': 'Magic Browser'})\n",
    "# con = urllib.request.urlopen(req)\n",
    "# html = con.read().decode()\n",
    "\n",
    "# 'Python' in html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling y Scraping: Revelando los Secretos de la Web\n",
    "\n",
    "El rastreo y el scraping son dos técnicas fundamentales en el mundo de la adquisición de datos web. Forman la columna vertebral de muchas aplicaciones basadas en datos y son habilidades cruciales para analistas de datos y desarrolladores web.\n",
    "\n",
    "## Crawling/Rastreo: Navegando por la Web\n",
    "\n",
    "El rastreo, a menudo referido como crawling o scraping web, es el proceso de navegar sistemáticamente por la World Wide Web para recuperar páginas web. Piénsalo como un robot web o araña, incansablemente atravesando internet para descubrir e indexar contenido web. Esta técnica está en el corazón de motores de búsqueda como Google y Bing.\n",
    "\n",
    "### ¿Por Qué Rastreamos?\n",
    "\n",
    "El rastreo sirve varios propósitos importantes:\n",
    "\n",
    "- **Indexación**: Permite a los motores de búsqueda indexar y catalogar páginas web, haciéndolas buscables por los usuarios.\n",
    "  \n",
    "- **Descubrimiento de Enlaces**: Los rastreadores extraen enlaces de páginas web, ayudando a construir una vasta red de recursos web interconectados. Esta estructura de enlaces es crucial para comprender la arquitectura de la web.\n",
    "  \n",
    "- **Recuperación de Datos**: Los rastreadores pueden raspar o extraer datos de páginas web, pero su objetivo principal es descubrir y navegar a otras páginas web.\n",
    "\n",
    "## Scraping: Recolectando Datos\n",
    "\n",
    "El scraping es el proceso de extraer datos o información específica de una sola página web. A diferencia del rastreo, que se centra en navegar por la web, el scraping se enfoca en una sola página web para recolectar datos valiosos.\n",
    "\n",
    "### Casos de Uso del Scraping\n",
    "\n",
    "El scraping se usa para una variedad de propósitos, tales como:\n",
    "\n",
    "- **Extracción de Datos**: Nos permite extraer datos estructurados como precios de productos, titulares de noticias o información del mercado de valores de sitios web.\n",
    "\n",
    "- **Monitoreo de Contenido**: El scraping se puede emplear para rastrear cambios en el contenido de páginas web específicas, como monitorear cambios de precios en sitios de comercio electrónico o seguir actualizaciones de noticias.\n",
    "\n",
    "- **Análisis Competitivo**: Las empresas a menudo usan el scraping para recopilar datos sobre competidores, como estrategias de precios o listados de productos.\n",
    "\n",
    "- **Investigación y Análisis**: Analistas de datos e investigadores usan el scraping para recopilar datos para estudios, informes y perspectivas basadas en datos.\n",
    "\n",
    "## Sinergia entre Rastreo y Scraping\n",
    "\n",
    "En la práctica, el rastreo y el scraping a menudo trabajan juntos. Los rastreadores recorren la web para encontrar nuevas páginas, y una vez que alcanzan una página de interés, se aplican técnicas de scraping para extraer datos valiosos. Esta sinergia es lo que potencia motores de búsqueda, agregadores de noticias y aplicaciones basadas en datos en internet.\n",
    "\n",
    "## Conclusión\n",
    "\n",
    "Entender los conceptos de rastreo y scraping es esencial para cualquiera que busque trabajar con datos web. Ya sea que desees construir un motor de búsqueda, reunir investigación de mercado o simplemente automatizar la recolección de datos, estas técnicas son tu puerta de entrada para desbloquear la riqueza de información disponible en la web.\n",
    "\n",
    "**PROYECTO DE CALENTAMIENTO: Construyendo una Sencilla Araña Web**\n",
    "\n",
    "En este proyecto de calentamiento, nos adentraremos en el mundo de las arañas web o rastreadores. Estos son programas especializados diseñados para explorar sistemáticamente la World Wide Web, recuperando páginas web y sus contenidos. Las arañas web juegan un papel crucial en varias aplicaciones, incluyendo la indexación de páginas web para motores de búsqueda, extracción de datos de sitios web y más. En este proyecto, nos centraremos en construir una araña web básica.\n",
    "\n",
    "---\n",
    "\n",
    "**Visión General del Proyecto:**\n",
    "\n",
    "Una araña web, también conocida como rastreador web, es esencialmente un agente digital que navega por el vasto paisaje de internet. Su misión primaria es:\n",
    "\n",
    "- Explorar la web siguiendo enlaces de una página web a otra.\n",
    "- Recuperar contenido de páginas web.\n",
    "- Almacenar datos valiosos para análisis u otros propósitos.\n",
    "\n",
    "Piensa en ello como un explorador robótico, incansablemente recorriendo la web para recopilar información. En nuestro proyecto, apuntamos a crear una versión simplificada de tal araña web.\n",
    "\n",
    "**Tareas Clave:**\n",
    "\n",
    "1. **Identificando Enlaces**: El desafío inicial para nuestro rastreador es identificar qué enlaces debe seguir y explorar más a fondo. Considera cómo instruirías a la araña para localizar y rastrear estos enlaces dentro de una página web.\n",
    "\n",
    "2. **Creando la Clase Spider**: Para dar vida a nuestra araña, comenzaremos creando una clase en Python adecuadamente llamada \"Spider\". Esta clase servirá como el motor central de nuestro rastreador web, y su constructor aceptará tres parámetros cruciales:\n",
    "   - `starting_url`: La URL inicial desde la cual nuestra araña emprende su viaje.\n",
    "   - `crawl_domain`: Una restricción de dominio para asegurar que solo se consideren enlaces relevantes para el rastreo.\n",
    "   - `max_iter`: Un límite en el número máximo de elementos web que la araña recopilará.\n",
    "\n",
    "3. **Método Principal: Spider.run()**: Para poner en movimiento nuestra araña, implementaremos un método llamado `run` dentro de la clase Spider. Este método orquestará las acciones de la araña, y es aquí donde delinearemos las funcionalidades básicas o bloques de construcción que empoderan a nuestro rastreador.\n",
    "\n",
    "A través de este proyecto, ganarás experiencia práctica en la creación de una araña web simplificada, proporcionando una comprensión fundamental de las técnicas de rastreo web.\n",
    "\n",
    "### Flujo de Trabajo del Proyecto de Scraping/Crawling Web\n",
    "\n",
    "El scraping y crawling web involucran una serie de pasos para acceder, recuperar y procesar datos web de manera eficiente y responsable. El flujo de trabajo típico para tal proyecto incluye las siguientes etapas:\n",
    "\n",
    "1. **Acceder a la Web (`Acceder web`)**:\n",
    "   - El paso inicial es acceder a los sitios web objetivo(s) de los cuales se necesita raspar datos.\n",
    "   - Esto implica enviar una solicitud HTTP y recibir la respuesta del servidor web.\n",
    "\n",
    "2. **Descargar Contenido Web (`Bajar web`)**:\n",
    "   - Una vez concedido el acceso, el siguiente paso es descargar el contenido de la página web.\n",
    "   - Esto puede incluir HTML, CSS, JavaScript y archivos multimedia que componen la página web.\n",
    "\n",
    "3. **Buscar Enlaces (`Buscar enlaces`)**:\n",
    "   - Este paso implica analizar el contenido web descargado en busca de hipervínculos.\n",
    "   - Los hipervínculos se identifican por la etiqueta HTML `<a href=\"...\">` y son punteros a otras páginas web.\n",
    "\n",
    "4. **Almacenar Contenido Web (`Almacenar web`)**:\n",
    "   - El contenido web recuperado se almacena localmente para su procesamiento.\n",
    "   - Este almacenamiento puede ser en forma de archivos crudos o formatos más estructurados como bases de datos.\n",
    "\n",
    "5. **Almacenar Enlaces Extraídos (`Almacenar enlaces`)**:\n",
    "   - Los hipervínculos extraídos también se almacenan.\n",
    "   - Esto forma la base del proceso de rastreo, donde cada enlace se puede seguir para recuperar más contenido.\n",
    "\n",
    "6. **Verificar la Calidad de los Enlaces (`Verificar enlaces de saldo`)**:\n",
    "   - No todos los enlaces pueden ser relevantes o funcionales.\n",
    "   - Este paso asegura que los enlaces almacenados sean válidos y conduzcan al contenido necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from urllib.request import urlopen  # For opening URLs\n",
    "from urllib.error import HTTPError  # To handle HTTP errors\n",
    "import time  # To implement delays if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract links from HTML content\n",
    "def getLinks(html, max_links=10):\n",
    "    url = []  # List to store the extracted URLs\n",
    "    cursor = 0  # Cursor to track position in HTML content\n",
    "    nlinks = 0  # Counter for number of links extracted\n",
    "\n",
    "    # Loop to extract links until the maximum is reached or no more links are found\n",
    "    while (cursor >= 0 and nlinks < max_links):\n",
    "        start_link = html.find(\"a href\", cursor)  # Find the start of a link\n",
    "        if start_link == -1:  # If no more links are found, return the list of URLs\n",
    "            return url\n",
    "        start_quote = html.find('\"', start_link)  # Find the opening quote of the URL\n",
    "        end_quote = html.find('\"', start_quote + 1)  # Find the closing quote of the URL\n",
    "        url.append(html[start_quote + 1: end_quote])  # Extract and append the URL to the list\n",
    "        cursor = end_quote + 1  # Move the cursor past this URL\n",
    "        nlinks += 1  # Increment the link counter\n",
    "\n",
    "    return url  # Return the list of URLs\n",
    "\n",
    "# Example usage:\n",
    "# Suppose you have some HTML content stored in a variable `html_content`\n",
    "# You would call the function like this:\n",
    "# links = getLinks(html_content)\n",
    "# This would return a list of URLs extracted from `html_content`\n",
    "\n",
    "# Expected Output:\n",
    "# The output will be a list containing up to `max_links` number of URLs extracted from the given HTML content.\n",
    "# If the HTML content has fewer than `max_links` URLs, all found URLs will be included in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Spider class for web crawling\n",
    "class Spider:\n",
    "    # Initializer or constructor for the Spider class\n",
    "    def __init__(self, starting_url, crawl_domain, max_iter):\n",
    "        self.crawl_domain = crawl_domain  # The domain within which the spider will crawl\n",
    "        self.max_iter = max_iter  # The maximum number of pages to crawl\n",
    "        self.links_to_crawl = [starting_url]  # Queue of links to crawl\n",
    "        self.links_visited = []  # List to keep track of visited links\n",
    "        self.collection = []  # List to store the collected data\n",
    "\n",
    "    # Method to retrieve HTML content from a URL\n",
    "    def retrieveHtml(self):\n",
    "        try:\n",
    "            # Open the URL and read the response\n",
    "            socket = urlopen(self.url)\n",
    "            # Decode the response using 'latin-1' encoding\n",
    "            self.html = socket.read().decode('latin-1')\n",
    "            return 0  # Return 0 if successful\n",
    "        except HTTPError as e:\n",
    "            # If an HTTP error occurs, print the error and return -1\n",
    "            print(f\"HTTP Error encountered: {e}\")\n",
    "            return -1\n",
    "\n",
    "    # Main method to control the crawling process\n",
    "    def run(self):\n",
    "        # Continue to crawl while there are links to crawl and the max_iter is not reached\n",
    "        while self.links_to_crawl and len(self.collection) < self.max_iter:\n",
    "            # Get the next link to crawl\n",
    "            self.url = self.links_to_crawl.pop(0)\n",
    "            print(f\"Currently crawling: {self.url}\")\n",
    "            # Add the link to the list of visited links\n",
    "            self.links_visited.append(self.url)\n",
    "            # If HTML retrieval is successful, store the HTML and find new links\n",
    "            if self.retrieveHtml() >= 0:\n",
    "                self.storeHtml()\n",
    "                self.retrieveAndValidateLinks()\n",
    "\n",
    "    # Method to retrieve and validate links in the HTML content\n",
    "    def retrieveAndValidateLinks(self):\n",
    "        # Get a list of links from the current HTML content\n",
    "        items = getLinks(self.html)\n",
    "        # Temporary list to store valid links\n",
    "        tmpList = []\n",
    "\n",
    "        # Iterate over the found links\n",
    "        for item in items:\n",
    "            item = item.strip('\"')  # Remove any extra quotes\n",
    "        \n",
    "            # Check if the link is an absolute URL that contains the crawl domain\n",
    "            if self.crawl_domain in item and item.startswith('http'):\n",
    "                tmpList.append(item)\n",
    "            # Handle relative links\n",
    "            elif item.startswith('/'):\n",
    "                # Construct the full URL using the crawl domain and relative link\n",
    "                tmpList.append('https://' + self.crawl_domain + item)\n",
    "            # Handle potential relative links without a leading slash (assuming they are not absolute URLs)\n",
    "            elif not item.startswith('http'):\n",
    "                # Construct the full URL assuming it is a relative link\n",
    "                tmpList.append('https://' + self.crawl_domain + '/' + item)\n",
    "\n",
    "        # Add valid, unvisited links to the crawl queue\n",
    "        for item in tmpList:\n",
    "            if item not in self.links_visited and item not in self.links_to_crawl:\n",
    "                self.links_to_crawl.append(item)\n",
    "                print(f'Adding to crawl queue: {item}')\n",
    "\n",
    "\n",
    "    # Method to store HTML content and associated metadata\n",
    "    def storeHtml(self):\n",
    "        # Create a dictionary to represent the document\n",
    "        doc = {\n",
    "            'url': self.url,  # URL of the page\n",
    "            'date': time.strftime(\"%d/%m/%Y\"),  # Current date\n",
    "            'html': self.html  # HTML content of the page\n",
    "        }\n",
    "        # Add the document to the collection\n",
    "        self.collection.append(doc)\n",
    "        print(f\"Stored HTML from: {self.url}\")\n",
    "\n",
    "# Example usage of the Spider class:\n",
    "# Initialize the spider with the starting URL, domain to crawl, and the maximum number of iterations.\n",
    "# my_spider = Spider(\"http://www.example.com\", \"example.com\", 20)\n",
    "\n",
    "# Start the crawling process.\n",
    "# my_spider.run()\n",
    "\n",
    "# After running, my_spider.collection will contain up to 20 pages' HTML from 'example.com'.\n",
    "# Each page's data includes the URL, the date when it was scraped, and the HTML content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen de la Clase Spider para Rastreo Web\n",
    "\n",
    "La clase `Spider` está diseñada para el proceso de rastreo web, que explora sistemáticamente la web para recopilar datos. A continuación, se presenta un resumen de sus funcionalidades clave:\n",
    "\n",
    "## Inicialización\n",
    "- La clase se inicializa con un `starting_url`, el dominio dentro del cual se rastreará (`crawl_domain`), y un número máximo de páginas a rastrear (`max_iter`).\n",
    "\n",
    "## Proceso de Rastreo\n",
    "- La araña mantiene una cola de enlaces (`links_to_crawl`) para visitar y una lista de enlaces ya visitados (`links_visited`).\n",
    "- El método `run` procesa cada enlace en la cola, continuando hasta que la cola esté vacía o se alcance el límite de `max_iter`.\n",
    "\n",
    "## Recuperación de Contenido HTML\n",
    "- El método `retrieveHtml` abre cada enlace, lee su contenido y lo decodifica en un formato de cadena. Maneja casos de éxito y error durante este proceso.\n",
    "\n",
    "## Extracción y Validación de Enlaces\n",
    "- `retrieveAndValidateLinks` extrae nuevos enlaces del HTML de la página actual, los valida (asegurándose de que pertenezcan al dominio especificado) y agrega enlaces válidos y no visitados a la cola de rastreo.\n",
    "\n",
    "## Almacenamiento de Datos\n",
    "- El método `storeHtml` guarda el contenido HTML de cada página visitada, junto con la URL de la página y la fecha actual, en una colección para análisis o procesamiento posterior.\n",
    "\n",
    "Esta clase permite la recopilación automatizada de datos de una serie de páginas web dentro de un dominio específico, gestionando eficientemente el descubrimiento de nuevas páginas para visitar basado en los enlaces de cada página.\n",
    "\n",
    "Validemos el rastreador con el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the Spider class is defined as before with getLinks function properly defined\n",
    "\n",
    "# Example usage of the Spider class:\n",
    "\n",
    "# Instantiate the Spider with the starting URL, the domain to crawl within, and the maximum number of iterations.\n",
    "# The crawl domain is typically the base domain from which the crawler should not deviate.\n",
    "spider = Spider('https://books.toscrape.com/', 'books.toscrape.com', 20)\n",
    "\n",
    "# Start the crawling process.\n",
    "spider.run()\n",
    "\n",
    "# After running, `spider.collection` will contain the HTML content of up to 20 pages from 'ironhack.com'.\n",
    "# Each entry in the collection will include the URL, the date when it was scraped, and the HTML content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many elements does our colletion have?\n",
    "len(spider.collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spider.collection[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enumerate the urls retreived\n",
    "[spider.collection[i]['url'] for i in range(len(spider.collection))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafío para Pequeñas Empresas: Extracción de Datos de Productos de una Tienda en Línea Ficticia\n",
    "\n",
    "## Objetivo\n",
    "Tu tarea es realizar un análisis de productos recopilando datos de una tienda en línea simulada, como \"Fake Store API\" (https://fakestoreapi.com/). Este sitio web está diseñado para prácticas y ofrece un entorno seguro para el web scraping.\n",
    "\n",
    "## Pasos\n",
    "1. **Recopilación de Datos**:\n",
    "   - Utiliza la clase `Spider` para rastrear el sitio web de \"Fake Store API\".\n",
    "   - Recopila datos sobre productos, incluyendo nombres, categorías, precios y descripciones.\n",
    "\n",
    "2. **Almacenamiento de Datos**:\n",
    "   - Almacena los datos raspados en un formato estructurado, como un archivo CSV o una base de datos.\n",
    "\n",
    "3. **Análisis**:\n",
    "   - Analiza los datos recopilados para entender la distribución de productos a través de diferentes categorías, rangos de precios y otras métricas relevantes.\n",
    "\n",
    "4. **Informe**:\n",
    "   - Prepara un informe que resuma tus hallazgos, incluyendo perspectivas sobre tendencias de productos, estrategias de precios y popularidad de categorías.\n",
    "\n",
    "## Tareas para los Estudiantes\n",
    "- Trabajar en parejas para planificar y ejecutar el proceso de web scraping.\n",
    "- Asegurar que se sigan prácticas éticas de scraping, incluyendo el adherirse a las directrices de `robots.txt` y limitar la tasa de solicitudes.\n",
    "- Realizar un análisis exhaustivo de los datos recopilados y colaborar para crear un informe comprensivo.\n",
    "- Presentar tus hallazgos en clase, destacando las perspectivas clave y las metodologías utilizadas.\n",
    "\n",
    "## Resultado Esperado\n",
    "Obtener experiencia práctica en web scraping, análisis de datos y presentación de hallazgos en un contexto empresarial. Este proyecto también mejorará tu comprensión de la dinámica del mercado de venta al por menor en línea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu codigo aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Uso de APIs para la Recuperación de Datos\n",
    "\n",
    "### Entendiendo el Panorama General\n",
    "\n",
    "Al tratar de recuperar datos específicos de un sitio web, es crucial primero verificar si el sitio ofrece una interfaz programática para consultar datos. Estas interfaces, conocidas como Interfaces de Programación de Aplicaciones (APIs), proporcionan una forma más eficiente y estructurada de acceder a los datos en comparación con el web scraping.\n",
    "\n",
    "### La Ventaja de las APIs\n",
    "\n",
    "Las APIs, especialmente las APIs RESTful, ofrecen un método bien definido para interactuar con servicios web. Están construidas sobre un conjunto de reglas y estándares que permiten la recuperación de datos de manera predecible y sencilla. Esto es lo que caracteriza a una API RESTful:\n",
    "\n",
    "- **URI Base**: Cada API RESTful tiene un Identificador de Recursos Uniforme (URI) base, que sirve como punto de entrada para la API. Por ejemplo, `http://ejemplo.com/recursos/` podría ser un URI base.\n",
    "\n",
    "- **Tipo de Medio de Internet**: Las APIs RESTful a menudo devuelven datos en un formato específico, como JSON (Notación de Objetos de JavaScript), que se utiliza ampliamente debido a su simplicidad y legibilidad. Sin embargo, otros formatos como XML, Atom o incluso imágenes pueden ser utilizados.\n",
    "\n",
    "- **Métodos HTTP Estándar**: Estas APIs aprovechan los métodos HTTP estándar para operaciones:\n",
    "    - `GET`: Recuperar datos del servidor (por ejemplo, una lista de productos).\n",
    "    - `PUT`: Actualizar datos existentes o crear nuevos datos si no existen, y es una operación idempotente (repetir la solicitud resulta en el mismo estado).\n",
    "    - `POST`: Crear nuevos datos o actualizar datos existentes (no idempotente).\n",
    "    - `DELETE`: Eliminar datos.\n",
    "\n",
    "- **Enlaces de Hipertexto para Estado y Navegación**: Las APIs RESTful a menudo usan enlaces de hipertexto (URLs) para representar el estado actual de una aplicación o para navegar entre recursos relacionados.\n",
    "\n",
    "### Uso de APIs con Autenticación\n",
    "\n",
    "Muchas APIs RESTful requieren autenticación por razones de seguridad. Esto generalmente se hace enviando un token o clave con tu solicitud de API, lo cual verifica tu identidad y autoriza tu acceso a la API. El proceso para obtener y usar tokens de autenticación varía entre APIs, por lo que es esencial referirse a la documentación específica de la API para obtener orientación.\n",
    "\n",
    "### Resumen\n",
    "\n",
    "Aprovechar las APIs para la recuperación de datos no solo se alinea con las prácticas web éticas sino que también asegura una forma más estable y eficiente de acceder a los datos. Cuando una API está disponible, generalmente es el método preferido sobre el web scraping.\n",
    "\n",
    "## Ejemplo: Obtener Datos del Clima Usando la API de OpenWeatherMap en Python\n",
    "\n",
    "Este ejemplo demuestra cómo usar la API de OpenWeatherMap para obtener datos meteorológicos actuales para una ciudad específica usando Python.\n",
    "\n",
    "### Prerrequisitos\n",
    "- Una clave API de OpenWeatherMap.\n",
    "- La biblioteca `requests` de Python instalada. (Instalar a través de `pip install requests` si es necesario.)\n",
    "\n",
    "### Pasos a Seguir\n",
    "1. **Registrarse para la API de OpenWeatherMap**:\n",
    "   - Regístrate para obtener una cuenta en [OpenWeatherMap](https://openweathermap.org/api).\n",
    "   - Obtén tu clave API gratuita (nota que podría haber un retraso de activación).\n",
    "\n",
    "2. **Script de Python para la Recuperación de Datos Meteorológicos**:\n",
    "   - El script utiliza la biblioteca `requests` para realizar una llamada API.\n",
    "   - Reemplaza `'TU_CLAVE_API'` con tu clave API de OpenWeatherMap real.\n",
    "   - Reemplaza `'NOMBRE_CIUDAD'` con el nombre de tu ciudad deseada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_weather(api_key, city):\n",
    "    base_url = \"http://api.openweathermap.org/data/2.5/weather?\"\n",
    "    city_name = city\n",
    "    complete_url = f\"{base_url}appid={api_key}&q={city_name}\"\n",
    "    response = requests.get(complete_url)\n",
    "    return response.json()\n",
    "\n",
    "# Replace 'YOUR_API_KEY' with your actual API key and 'CITY_NAME' with your city\n",
    "api_key = 'YOUR_API_KEY'\n",
    "city_name = 'CITY_NAME'\n",
    "weather_data = get_weather(api_key, city_name)\n",
    "\n",
    "print(f\"Weather in {city_name}:\")\n",
    "print(weather_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output\n",
    "The script will output the current weather data in JSON format, which includes temperature, humidity, weather description, etc.\n",
    "\n",
    "### Note\n",
    "- Ensure you replace `'YOUR_API_KEY'` and `'CITY_NAME'` with your actual API key and desired city.\n",
    "- The OpenWeatherMap API provides data in various formats and details. You might want to explore their documentation for more specific use cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafío: Analizando Tendencias de Hashtags en Instagram con Instaloader\n",
    "\n",
    "## Objetivo\n",
    "Aprovechar `Instaloader`, una biblioteca de Python, para descargar publicaciones asociadas con un hashtag específico en Instagram. Analiza los datos recopilados para identificar tendencias, contenido popular y compromiso del usuario.\n",
    "\n",
    "## Pasos\n",
    "\n",
    "### 1. Instalar Instaloader\n",
    "- Asegúrate de que Python esté instalado en tu sistema.\n",
    "- Instala `Instaloader` usando pip: `pip install instaloader`\n",
    "\n",
    "### 2. Recolección de Datos\n",
    "- Elige un hashtag relevante para un tema de interés (por ejemplo, #naturaleza, #viaje, #comida).\n",
    "- Usa `Instaloader` para descargar publicaciones etiquetadas con el hashtag elegido. Considera limitaciones como el número de publicaciones para evitar sobrecargar la API.\n",
    "\n",
    "```python\n",
    "import instaloader\n",
    "\n",
    "L = instaloader.Instaloader()\n",
    "posts = instaloader.Hashtag.from_name(L.context, 'TU_HASHTAG').get_posts()\n",
    "\n",
    "for post in posts:\n",
    "    # Agrega código para procesar y almacenar detalles de la publicación\n",
    "```\n",
    "### 3. Análisis de Datos\n",
    "Analiza los datos descargados para:\n",
    "- Tendencias populares en el hashtag.\n",
    "- Temas o sujetos comunes en imágenes o subtítulos.\n",
    "- Niveles de compromiso del usuario (me gusta, comentarios).\n",
    "\n",
    "### 4. Reporte\n",
    "- Compila tus hallazgos en un informe.\n",
    "- Incluye representaciones visuales (gráficos, nubes de palabras) para ilustrar las tendencias clave.\n",
    "\n",
    "### Notas Importantes\n",
    "- Respeta los términos de servicio de Instagram y las directrices éticas en el scraping de datos.\n",
    "- Ten en cuenta la privacidad y el consentimiento, especialmente con el contenido generado por el usuario.\n",
    "- El alcance de la recolección de datos debe ser limitado para fines educativos.\n",
    "\n",
    "### Resultado Esperado\n",
    "Este desafío tiene como objetivo proporcionar experiencia práctica con Instaloader, desarrollar habilidades de análisis de datos y ofrecer perspectivas sobre tendencias en redes sociales y comportamiento del usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu codigo aquí"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
