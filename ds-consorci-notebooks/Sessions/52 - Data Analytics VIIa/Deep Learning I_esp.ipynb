{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning I\n",
    "\n",
    "1. Gradient Deescend\n",
    "\n",
    "2. ¿Cómo aprender de los datos?\n",
    "    * Square / Euclidean Loss\n",
    "    * Hinge / Margin Loss (es decir, Support Vector Machine)\n",
    "    * Pérdida Logística (Regresión Logística)\n",
    "    * Sigmoid Cross-Entropy Loss (clasificador Softmax)\n",
    "    * Descenso de gradiente por lotes (Batch Gradient Descend)\n",
    "    * Stochastic Gradient Descend\n",
    "3. Mini-batch Gradient Descend (Descenso del gradiente por mini-lotes)\n",
    "\n",
    "4. Diferenciación Automática"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4wmBVcRZgrT"
   },
   "source": [
    "## Previa: Optimización.\n",
    "\n",
    "Lo más sencillo que podríamos intentar para minimizar una función $f(x)$ sería muestrear dos puntos relativamente cercanos entre sí, y simplemente dar pasos repetidos alejándonos del valor más alto. Este simple algoritmo tiene una limitación severa: no puede acercarse más al mínimo verdadero que el tamaño del paso.\n",
    "\n",
    "El método **Nelder-Mead** ajusta dinámicamente el tamaño del paso en función de la pérdida del nuevo punto. Si el nuevo punto es mejor que cualquier valor visto previamente, expande el tamaño del paso para acelerar hacia el fondo. De igual manera, si el nuevo punto es peor, contrae el tamaño del paso para converger alrededor del mínimo. Los ajustes usuales son reducir a la mitad el tamaño del paso al contraer y duplicar el tamaño del paso al expandir.\n",
    "\n",
    "Este método se puede extender fácilmente a ejemplos de dimensiones superiores, todo lo que se necesita es tomar un punto más de los que hay dimensiones. Luego, el enfoque más simple es reemplazar el peor punto con un punto reflejado a través del centroide de los demás puntos n. Si este punto es mejor que el mejor punto actual, entonces podemos intentar estirar exponencialmente a lo largo de esta línea. Por otro lado, si este nuevo punto no es mucho mejor que el valor anterior, entonces estamos cruzando un valle, por lo que reducimos el paso hacia un punto mejor.\n",
    "\n",
    "> Ver [\"Un Tutorial Interactivo sobre Optimización Numérica\"](http://www.benfrederickson.com/numerical-optimization/)\n",
    "\n",
    "Preguntas:\n",
    "\n",
    "+ ¿Cuáles son las limitaciones de este método desde un punto de vista computacional?\n",
    "+ ¿En qué casos es una alternativa real?\n",
    "\n",
    "Los métodos de optimización **basados en gradientes** proporcionan una alternativa a los enfoques de muestreo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkZJe1Y-YjUL"
   },
   "source": [
    "# 1. Descenso de Gradiente (Gradient Descend)\n",
    "\n",
    "Supongamos que tenemos una función $f(w): \\mathbf{R} \\rightarrow \\mathbf{R}$ y que nuestro objetivo es encontrar el argumento $w$ que minimiza esta función (para maximización, considera $-f(w)$). Para este fin, el concepto crítico es la *derivada*.\n",
    "\n",
    "La derivada de $f$ de una variable $w$, $ f'(w)$ o $\\frac{\\delta f}{\\delta w}$, es una medida de la tasa a la que el valor de la función cambia con respecto al cambio de la variable. Se define como el siguiente límite:\n",
    "\n",
    "$$ f'(w) = \\lim_{h \\rightarrow 0} \\frac{f(w + h) - f(w)}{h} $$\n",
    "\n",
    "La derivada especifica cómo escalar un pequeño cambio en la entrada para obtener el cambio correspondiente en la salida. Conociendo el valor de $f(w)$ en un punto $w$, esto permite predecir el valor de la función en un punto vecino:\n",
    "\n",
    "$$ f(w + h) \\approx f(w) + h f'(w)$$\n",
    "\n",
    "## 1.1. Primer enfoque\n",
    "\n",
    "Entonces, siguiendo estos pasos podemos disminuir el valor de la función:\n",
    "\n",
    "+ Comenzar desde un valor $w^0$ aleatorio.\n",
    "+ Calcular la derivada $f'(w) = \\lim_{h \\rightarrow 0} \\frac{f(w + h) - f(w)}{h}$.\n",
    "+ Caminar pequeños pasos en la dirección opuesta de la derivada, $w^{i+1} = w^i - h f'(w^i)$, porque sabemos que $f(w - h f'(w))$ es menor que $f(w)$ para $h$ suficientemente pequeño, hasta que $ f'(w) \\approx 0$.\n",
    "\n",
    "La búsqueda del mínimo termina cuando la derivada es cero porque no tenemos más información sobre en qué dirección movernos. $w$ se llama un punto crítico o estacionario de $f(w)$ si $f'(w)=0$.\n",
    "\n",
    "Todos los puntos extremos (máximos/mínimos) son puntos críticos porque $f(w)$ es menor/mayor que en todos los puntos vecinos. Pero estos no son los únicos puntos críticos: hay una tercera clase de puntos críticos llamados *puntos de muertos* (saddle points). Los puntos muertos son puntos que tienen derivadas parciales igual a cero pero en los cuales la función no tiene ni un valor máximo ni mínimo.\n",
    "\n",
    "Si $f$ es una *función convexa*, cuando la derivada es cero, este debería ser el extremo de nuestra función. En otros casos podría ser un mínimo/máximo local o un punto muerto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1067,
     "status": "ok",
     "timestamp": 1647971518729,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "5r_P-G3ZbZUm",
    "outputId": "ce0ad95d-72ec-4bcb-fbf3-508bcf322645"
   },
   "outputs": [],
   "source": [
    "# numerical derivative at a point x by using finite differences\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "def fin_dif(x, \n",
    "            f, \n",
    "            h = 0.00001):\n",
    "    '''\n",
    "    This method returns the derivative of f at x\n",
    "    by using the finite difference method\n",
    "    '''\n",
    "    return (f(x+h) - f(x))/h\n",
    "\n",
    "x = 2.0\n",
    "print(\"{:2.4f}\".format(fin_dif(x,f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsBzquN2bgGn"
   },
   "source": [
    "> NOTA: Se puede demostrar que la \"fórmula de diferencia centrada\" es mejor al calcular derivadas numéricas:\n",
    "\n",
    "> $$ \\lim_{h \\rightarrow 0} \\frac{f(x + h) - f(x - h)}{2h} $$\n",
    "El error en la aproximación de \"diferencia finita\" se puede derivar del teorema de Taylor y, asumiendo que $f$ es diferenciable, es $O(h)$. En el caso de la \"diferencia centrada\", el error es $O(h^2)$.\n",
    "\n",
    "Hay dos problemas con las derivadas numéricas:\n",
    "\n",
    "+ Es aproximado.\n",
    "+ Es muy lento de evaluar (dos evaluaciones de función: $f(x + h), f(x - h)$).\n",
    "\n",
    "¡Nuestros conocimientos de Cálculo podrían ayudar!\n",
    "\n",
    "## 1.2. Segundo enfoque\n",
    "\n",
    "Para encontrar el mínimo local usando gradient descend y en el caso de conocer una expresión analítica de la derivada de la **función** que queremos minimizar, puedes comenzar en un punto aleatorio y moverte en la dirección de descenso más pronunciado en relación con la derivada:\n",
    "\n",
    "+ Comenzar desde un valor $x$ aleatorio.\n",
    "+ Calcular la derivada $f'(x)$ analíticamente.\n",
    "+ Dar un pequeño paso en la dirección opuesta de la derivada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "executionInfo": {
     "elapsed": 1142,
     "status": "ok",
     "timestamp": 1647971519842,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "147fFQFFcd5Y",
    "outputId": "6f4d7be4-3df0-4a11-a937-013f39024406"
   },
   "outputs": [],
   "source": [
    "#@title Default title text\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression \n",
    "from scipy import stats \n",
    "%matplotlib inline\n",
    "\n",
    "x = np.linspace(-10,20,100)\n",
    "y = x**2 - 6*x + 5\n",
    "start = 15\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_facecolor('#EAEAF2')\n",
    "plt.plot(x,y, 'r-')\n",
    "plt.plot([start],[start**2 - 6*start + 5],'o')\n",
    "ax.text(start,\n",
    "        start**2 - 6*start + 35,\n",
    "        'Start',\n",
    "        ha='center',\n",
    "        color=sns.xkcd_rgb['blue'],\n",
    "       )\n",
    "\n",
    "d = 2 * start - 6\n",
    "end = start - d\n",
    "\n",
    "plt.plot([end],[end**2 - 6*end + 5],'o')\n",
    "plt.ylim([-10,250])\n",
    "plt.gcf().set_size_inches((10,3))\n",
    "plt.grid(True)\n",
    "ax.text(end,\n",
    "        start**2 - 6*start + 35,\n",
    "        'End',\n",
    "        ha='center',\n",
    "        color=sns.xkcd_rgb['green'],\n",
    "       )\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnn1ivYNdIhp"
   },
   "source": [
    "¡Hay un problema! ¿Cuál?\n",
    "\n",
    "Necesitamos definir un *paso* adecuado para modular el valor del gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 332,
     "status": "ok",
     "timestamp": 1645026903828,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "3dFqF4WVcqCG",
    "outputId": "d5359296-8dfd-488d-e997-7949524387b5"
   },
   "outputs": [],
   "source": [
    "old_min = 0\n",
    "temp_min = 15\n",
    "step_size = 0.01\n",
    "precision = 0.0001\n",
    "\n",
    "def f(x):\n",
    "    return x**2 - 6*x + 5\n",
    "    \n",
    "def f_derivative(x):\n",
    "    import math\n",
    "    return 2*x -6\n",
    "\n",
    "mins = []\n",
    "cost = []\n",
    "\n",
    "while abs(temp_min - old_min) > precision:\n",
    "    old_min = temp_min \n",
    "    gradient = f_derivative(old_min) \n",
    "    move = gradient * step_size\n",
    "    temp_min = old_min - move\n",
    "    cost.append((3-temp_min)**2)\n",
    "    mins.append(temp_min)\n",
    "\n",
    "# rounding the result to 2 digits because of the step size\n",
    "print(\"Local minimum occurs at {:3.6f}.\".format(round(temp_min,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "executionInfo": {
     "elapsed": 583,
     "status": "ok",
     "timestamp": 1645026904377,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "ADdYLeGnc_Ae",
    "outputId": "9275d342-5f18-4cb5-9150-c23533e017b3"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-10,20,100)\n",
    "y = x**2 - 6*x + 5\n",
    "\n",
    "x, y = (zip(*enumerate(cost)))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_facecolor('#EAEAF2')\n",
    "plt.plot(x,y, 'r-', alpha=0.7)\n",
    "plt.ylim([-10,150])\n",
    "plt.gcf().set_size_inches((10,3))\n",
    "plt.grid(True)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "executionInfo": {
     "elapsed": 408,
     "status": "ok",
     "timestamp": 1645026904737,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "AjBimEs5dEGB",
    "outputId": "e398f7ff-ab35-47df-c1b2-09e5c00430ae"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-10,20,100)\n",
    "y = x**2 - 6*x + 5\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_facecolor('#EAEAF2')\n",
    "plt.plot(x,y, 'r-')\n",
    "plt.ylim([-10,250])\n",
    "plt.gcf().set_size_inches((10,3))\n",
    "plt.grid(True)\n",
    "plt.plot(mins,cost,'o', alpha=0.3)\n",
    "ax.text(start,\n",
    "        start**2 - 6*start + 25,\n",
    "        'Start',\n",
    "        ha='center',\n",
    "        color=sns.xkcd_rgb['blue'],\n",
    "       )\n",
    "ax.text(mins[-1],\n",
    "        cost[-1]+20,\n",
    "        'End (%s steps)' % len(mins),\n",
    "        ha='center',\n",
    "        color=sns.xkcd_rgb['blue'],\n",
    "       )\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j11Iabv98uP4"
   },
   "source": [
    "#### Alpha\n",
    "\n",
    "El tamaño del paso, **alfa**, es un concepto delicado: ya que si es demasiado pequeño convergeremos lentamente a la solución, pero si es demasiado grande podemos divergir de la solución. \n",
    "\n",
    "Hay varias políticas a seguir a la hora de seleccionar el tamaño del paso:\n",
    "\n",
    "+ Pasos de tamaño constante. En este caso, el tamaño de paso determina la precisión de la solución.\n",
    "+ Pasos de tamaño decreciente.\n",
    "+ En cada paso, seleccionar el paso óptimo.\n",
    "\n",
    "La última política es buena, pero demasiado cara."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVUkCGkqdRQP"
   },
   "source": [
    "## 1.3. De las derivadas al gradiente: minimización de funciones $n$-dimensionales.\n",
    "Consideremos una función $n$-dimensional $f: \\mathbf{R}^n \\rightarrow \\mathbf{R}$. \n",
    "Por ejemplo:\n",
    "\n",
    "$$f(\\mathbf{x}) = \\suma_{n} x_n^2$$\n",
    "Nuestro objetivo es encontrar el argumento $\\mathbf{x}$ que minimice esta función.\n",
    "\n",
    "El gradiente de $f$ es el vector cuyas componentes son las $n$ derivadas parciales de $f$. Se trata pues de una función vectorial.\n",
    "\n",
    "El gradiente apunta en la dirección de la mayor tasa de incremento de la función.\n",
    "\n",
    "$$\\nabla {f} = (\\frac{\\parcial f}{\\parcial x_1}, \\dots, \\frac{\\parcial f}{\\parcial x_n})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1645026904738,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "UTjC31WKdlC0",
    "outputId": "7b76173f-2aee-4cbb-938f-27045c77a3e5"
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return sum(x_i**2 for x_i in x)\n",
    "\n",
    "def fin_dif_partial_centered(x, \n",
    "                             f, \n",
    "                             i, \n",
    "                             h=1e-6):\n",
    "    '''\n",
    "    This method returns the partial derivative of the i-th \n",
    "    component of f at x\n",
    "    by using the centered finite difference method\n",
    "    '''\n",
    "    w1 = [x_j + (h if j==i else 0) for j, x_j in enumerate(x)]\n",
    "    w2 = [x_j - (h if j==i else 0) for j, x_j in enumerate(x)]\n",
    "    return (f(w1) - f(w2))/(2*h)\n",
    "\n",
    "def gradient_centered(x, \n",
    "                      f, \n",
    "                      h=1e-6):\n",
    "    '''\n",
    "    This method returns the gradient vector of f at x\n",
    "    by using the centered finite difference method\n",
    "    '''\n",
    "    return[round(fin_dif_partial_centered(x,f,i,h), 10) for i,_ in enumerate(x)]\n",
    "\n",
    "\n",
    "x = [1.0,1.0,1.0]\n",
    "\n",
    "print('{:.6f}'.format(f(x)), gradient_centered(x,f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SuoyRktk7f5z"
   },
   "source": [
    "La función que hemos evaluado, $f({\\mathbf x}) = x_1^2+x_2^2+x_3^2$, es $3$ en $(1,1,1)$ y el vector gradiente en este punto es $(2,2,2)$. \n",
    "\n",
    "Entonces, podemos seguir estos pasos para maximizar (o minimizar) la función:\n",
    "\n",
    "+ Partir de un vector aleatorio $\\mathbf{x}$.\n",
    "+ Calcular el vector gradiente.\n",
    "+ Dar un pequeño paso en la dirección opuesta al vector gradiente.\n",
    "\n",
    "> Es importante tener en cuenta que este cálculo del gradiente es muy costoso: si $\\mathbf{x}$ tiene dimensión $n$, tenemos que evaluar $f$ en $2*n$ puntos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQcpRA91XWim"
   },
   "source": [
    "# 2. Cómo aprender de los datos?\n",
    "\n",
    "En general, *Aprender de los Datos* es una disciplina científica que se ocupa del diseño y desarrollo de algoritmos que permiten a las computadoras inferir, a partir de los datos, un modelo que permite una representación compacta de los datos crudos y/o buenas capacidades de generalización. En el primer caso, estamos hablando de aprendizaje no supervisado. En el segundo, de aprendizaje supervisado.\n",
    "\n",
    "Hoy en día, esta es una tecnología importante porque permite a los sistemas computacionales mejorar su rendimiento con la experiencia acumulada a partir de los datos observados en escenarios del mundo real.\n",
    "\n",
    "Consideremos el problema de aprendizaje supervisado desde un punto de vista de optimización. Al aprender un modelo a partir de datos, el escenario más común está compuesto por los siguientes elementos:\n",
    "\n",
    "+ Un conjunto de datos $(\\mathbf{x},y)$ de $n$ ejemplos. Por ejemplo, $(\\mathbf{x},y)$ puede representar:\n",
    "\n",
    "  + $\\mathbf{x}$: el comportamiento de un jugador de videojuegos; $y$: pagos mensuales.\n",
    "  + $\\mathbf{x}$: datos de sensores del motor de tu coche; $y$: probabilidad de error del motor.\n",
    "  + $\\mathbf{x}$: datos financieros de un cliente de un banco; $y$: calificación del cliente.\n",
    "\n",
    "  Si $y$ es un valor real, el problema que estamos intentando resolver se llama problema de *regresión*. Si $y$ es binario o categórico, se llama problema de *clasificación*.\n",
    "\n",
    "+ Una función objetivo $f_{(\\mathbf{x},y)}(\\mathbf{w})$, que queremos minimizar, representando la discrepancia entre nuestros datos y el modelo que queremos ajustar.\n",
    "\n",
    "+ Un modelo $M$ que está representado por un conjunto de parámetros $\\mathbf{w}$.\n",
    "\n",
    "+ El gradiente de la función objetivo, $\\nabla {f_{(\\mathbf{x},y)}(\\mathbf{w})}$ con respecto a los parámetros del modelo.\n",
    "\n",
    "En el caso de la regresión $f_{(\\mathbf{x},y)}(\\mathbf{w})$ representa los errores de un modelo de representación de datos $M$. Ajustar un modelo puede definirse como encontrar los parámetros óptimos $\\mathbf{w}$ que minimicen la siguiente expresión:\n",
    "\n",
    "$f_{(\\mathbf{x},y)}(\\mathbf{w}) = \\frac{1}{n} \\sum_{i} (y_i - M(\\mathbf{x}_i,\\mathbf{w}))^2$\n",
    "\n",
    "Los problemas alternativos de regresión y clasificación se pueden definir considerando diferentes formulaciones para medir los errores de un modelo de representación de datos. Estas formulaciones se conocen como la *Función de Pérdida* del problema.\n",
    "\n",
    "\n",
    "## 2.1. Square / Euclidean Loss\n",
    "\n",
    "En problemas de regresión, la función de pérdida más común es la Square Loss:\n",
    "\n",
    "$$ L(y, f(\\mathbf{x})) = \\frac{1}{n} \\sum_i (y_i - f(\\mathbf{x}_i))^2  $$\n",
    "\n",
    "La función square loss se puede reescribir y utilizar para la clasificación:\n",
    "\n",
    "$$ L(y, f(\\mathbf{x})) = \\frac{1}{n} \\sum_i (1 - y_i f(\\mathbf{x}_i))^2  $$\n",
    "\n",
    "\n",
    "## 2.2. Hinge / Margin Loss (es decir, Support Vector Machine)\n",
    "\n",
    "La función de hinge loss se define como:\n",
    "\n",
    "$$ L(y, f(\\mathbf{x})) = \\frac{1}{n} \\sum_i \\mbox{max}(0, 1 - y_i f(\\mathbf{x}_i))  $$\n",
    "\n",
    "Hinge loss proporciona un límite superior convexo relativamente ajustado sobre la Pérdida 0–1.\n",
    "\n",
    "## 2.3. Pérdida Logística (Regresión Logística)\n",
    "\n",
    "Esta función muestra una tasa de convergencia similar a la función de hinge loss, y dado que es continua, se pueden utilizar métodos simples de gradient descend.\n",
    "\n",
    "$$ L(y, f(\\mathbf{x})) = \\frac{1}{n} log(1 + exp(-y_i f(\\mathbf{x}_i))) $$\n",
    "\n",
    "\n",
    "## 2.4. Sigmoid Cross-Entropy Loss (clasificador Softmax)\n",
    "\n",
    "La entropía cruzada es una función de pérdida muy utilizada para entrenar **problemas multiclase**. Nos centraremos en modelos que asumen que las clases son mutuamente excluyentes.\n",
    "\n",
    "En este caso, nuestras etiquetas tienen esta forma $\\mathbf{y}_i =(1.0,0.0,0.0)$. Si nuestro modelo predice una distribución diferente, digamos  $ f(\\mathbf{x}_i)=(0.4,0.1,0.5)$, entonces nos gustaría ajustar los parámetros para que $f(\\mathbf{x}_i)$ se acerque más a $\\mathbf{y}_i$.\n",
    "\n",
    "C.Shannon mostró que si quieres enviar una serie de mensajes compuestos por símbolos de un alfabeto con distribución $y$ ($y_j$ es la probabilidad del símbolo $j$-ésimo), entonces para usar el menor número de bits en promedio, deberías asignar $\\log(\\frac{1}{y_j})$ bits al símbolo $j$-ésimo.\n",
    "\n",
    "El número óptimo de bits se conoce como **entropía**:\n",
    "\n",
    "$$ H(\\mathbf{y}) = \\sum_j y_j \\log\\frac{1}{y_j} = - \\sum_j y_j \\log y_j$$\n",
    "\n",
    "**Entropía cruzada** es el número de bits que necesitaremos si codificamos símbolos utilizando una distribución errónea $\\hat y$:\n",
    "\n",
    "$$ H(y, \\hat y) =   - \\sum_j y_j \\log \\hat y_j $$ \n",
    "\n",
    "En nuestro caso, la distribución real es $\\mathbf{y}$ y la \"errónea\" es $f(\\mathbf{x}_i)$. Por lo tanto, minimizar **la entropía cruzada** con respecto a nuestros parámetros del modelo resultará en el modelo que mejor aproxime nuestras etiquetas si se consideran como una distribución probabilística.\n",
    "\n",
    "La entropía cruzada se utiliza en combinación con el clasificador **Softmax**. Para clasificar $\\mathbf{x}_i$ podríamos tomar el índice correspondiente al valor máximo de $f(\\mathbf{x}_i)$, pero Softmax ofrece una salida ligeramente más intuitiva (probabilidades de clase normalizadas) y también tiene una interpretación probabilística:\n",
    "\n",
    "$$ P(\\mathbf{y}_i = j \\mid \\mathbf{x_i}) = - log \\left( \\frac{e^{f_j(\\mathbf{x_i})}}{\\sum_k e^{f_k(\\mathbf{x_i})} } \\right) $$\n",
    "\n",
    "donde $f_k$ es un clasificador lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Llc1-3D19CjX"
   },
   "source": [
    "## 2.5. Descenso de gradiente por lotes (Batch Gradient Descend)\n",
    "\n",
    "Podemos implementar **Gradient descend** de la siguiente manera (*descenso de gradiente por lotes*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1645026904738,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "xPHgrfHE9EaE",
    "outputId": "9e84da69-ef06-4c7b-8bab-d15036c549e3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# f = 2x\n",
    "x = np.arange(10)\n",
    "y = np.array([2*i for i in x])\n",
    "\n",
    "# f_target = 1/n Sum (y - wx)**2\n",
    "def target_f(x,y,w):\n",
    "    return np.sum((y - x * w)**2.0) / x.size\n",
    "\n",
    "# gradient_f = 2/n Sum 2wx**2 - 2xy\n",
    "def gradient_f(x,y,w):\n",
    "    return 2 * np.sum(2*w*(x**2) - 2*x*y) / x.size\n",
    "\n",
    "def step(w,grad,alpha):\n",
    "    return w - alpha * grad\n",
    "\n",
    "\n",
    "\n",
    "def BGD(target_f, \n",
    "        gradient_f, \n",
    "        x, \n",
    "        y, \n",
    "        toler = 1e-6, \n",
    "        alpha=0.01):\n",
    "    '''\n",
    "    Batch gradient descend by using a given step\n",
    "    '''\n",
    "    w = random.random()\n",
    "    val = target_f(x,y,w)\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        gradient = gradient_f(x,y,w)\n",
    "        next_w = step(w, gradient, alpha)\n",
    "        next_val = target_f(x,y,next_w)    \n",
    "        if (abs(val - next_val) < toler):\n",
    "            return w\n",
    "        else:\n",
    "            w, val = next_w, next_val\n",
    "            \n",
    "print('{:.6f}'.format(BGD(target_f, gradient_f, x, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BW7el7av9ine"
   },
   "source": [
    "## 2.6. Stochastic Gradient Descend\n",
    "\n",
    "La última función evalúa todo el conjunto de datos $(\\mathbf{x}_i,y_i)$ en cada paso. \n",
    "\n",
    "Si el conjunto de datos es grande, esta estrategia es demasiado costosa. En este caso utilizaremos una estrategia llamada **SGD** (*Stochastic Gradient Descend*).\n",
    "\n",
    "Al aprender de los datos, la función de coste es aditiva: se calcula sumando los errores de reconstrucción de las muestras. \n",
    "\n",
    "Entonces, podemos calcular la estimación del gradiente (y movernos hacia el mínimo) utilizando sólo **una muestra de datos** (o una pequeña muestra de datos).\n",
    "\n",
    "Así, encontraremos el mínimo iterando esta estimación del gradiente sobre el conjunto de datos.\n",
    "\n",
    "Una iteración completa sobre el conjunto de datos se denomina **epoch**. Durante un epoch, los datos deben utilizarse en un orden aleatorio.\n",
    "\n",
    "Si aplicamos este método tenemos algunas garantías teóricas para encontrar un buen mínimo:\n",
    "+ SGD utiliza esencialmente el gradiente inexacto por iteración. Puesto que no hay comida gratis, ¿cuál es el coste de utilizar el gradiente aproximado? La respuesta es que la velocidad de convergencia es más lenta que la del algoritmo de descenso del gradiente.\n",
    "+ La convergencia de SGD se ha analizado utilizando las teorías de minimización convexa y de aproximación estocástica: converge casi con seguridad a un mínimo global cuando la función objetivo es convexa o pseudoconvexa, y en caso contrario converge casi con seguridad a un mínimo local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1645026904739,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "enqYrpSq9kDI",
    "outputId": "ebd4bd87-ffd1-4d6a-d9a5-705f09204ea7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.arange(10)\n",
    "y = np.array([2*i for i in x])\n",
    "data = list(zip(x,y))\n",
    "\n",
    "for (x_i,y_i) in data:\n",
    "    print('{:3d} {:3d}'.format(x_i,y_i))\n",
    "print(\"\")\n",
    "\n",
    "def in_random_order(data):\n",
    "    '''\n",
    "    Random data generator\n",
    "    '''\n",
    "    import random\n",
    "    indexes = [i for i,_ in enumerate(data)]\n",
    "    random.shuffle(indexes)\n",
    "    for i in indexes:\n",
    "        yield data[i]\n",
    "        \n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def SGD(target_f, \n",
    "        gradient_f, \n",
    "        x, \n",
    "        y, \n",
    "        toler = 1e-6, \n",
    "        epochs=100, \n",
    "        alpha_0=0.01):\n",
    "    '''\n",
    "    Stochastic gradient descend with automatic step adaptation (by\n",
    "    reducing the step to its 95% when there are iterations with no increase)\n",
    "    '''\n",
    "    data = list(zip(x,y))\n",
    "    w = random.random()\n",
    "    alpha = alpha_0\n",
    "    min_w, min_val = float('inf'), float('inf')\n",
    "    epoch = 0\n",
    "    iteration_no_increase = 0\n",
    "    while epoch < epochs and iteration_no_increase < 100:\n",
    "        val = target_f(x, y, w)\n",
    "        if min_val - val > toler:\n",
    "            min_w, min_val = w, val\n",
    "            alpha = alpha_0\n",
    "            iteration_no_increase = 0\n",
    "        else:\n",
    "            iteration_no_increase += 1\n",
    "            alpha *= 0.95\n",
    "        for x_i, y_i in list(in_random_order(data)):\n",
    "            gradient_i = gradient_f(x_i, y_i, w)\n",
    "            w = w - (alpha *  gradient_i)\n",
    "        epoch += 1\n",
    "    return min_w\n",
    "  \n",
    "print('w: {:.6f}'.format(SGD(target_f, gradient_f, x, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6FcMBx497LP"
   },
   "source": [
    "# 3. Mini-batch Gradient Descend (Descenso del gradiente por mini-lotes)\n",
    "\n",
    "En código, el descenso del gradiente por lotes general se ve algo así:\n",
    "\n",
    "```python\n",
    "nb_epochs = 100\n",
    "for i in range(nb_epochs):\n",
    "    grad = evaluate_gradient(target_f, data, w)\n",
    "    w = w - learning_rate * grad\n",
    "```\n",
    "\n",
    "Para un número predefinido de epochs, primero calculamos el vector gradiente de la función objetivo para todo el conjunto de datos con respecto a nuestro vector de parámetros. \n",
    "\n",
    "**Stochastic gradient descent** (SGD) en cambio, realiza una actualización de los parámetros para cada ejemplo de entrenamiento y etiqueta:\n",
    "\n",
    "```python\n",
    "nb_epochs = 100\n",
    "for i in range(nb_epochs):\n",
    "    np.random.shuffle(data)\n",
    "    for sample in data:\n",
    "        grad = evaluate_gradient(target_f, sample, w)\n",
    "        w = w - learning_rate * grad\n",
    "```\n",
    "\n",
    "El **Mini-batch gradient descent** finalmente toma lo mejor de ambos mundos y realiza una actualización para cada minilote de $n$ ejemplos de entrenamiento:\n",
    "\n",
    "```python\n",
    "nb_epochs = 100\n",
    "for i in range(nb_epochs):\n",
    "  np.random.shuffle(data)\n",
    "  for batch in get_batches(data, batch_size=50):\n",
    "    grad = evaluate_gradient(target_f, batch, w)\n",
    "    w = w - learning_rate * grad\n",
    "```\n",
    "\n",
    "El Mini-batch SGD tiene la ventaja de que trabaja con una estimación del gradiente ligeramente menos ruidosa. Sin embargo, a medida que aumenta el tamaño del minilote, disminuye el número de actualizaciones realizadas por cada cálculo efectuado (al final se vuelve muy ineficiente, como el Batch Gradient Descend). \n",
    "\n",
    "Existe un compromiso óptimo (en términos de eficiencia computacional) que puede variar en función de la distribución de los datos y de las particularidades de la clase de función considerada, así como de la forma en que se implementen los cálculos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTK8mzlM_yGN"
   },
   "source": [
    "# 4. Diferenciación Automática\n",
    "\n",
    "> El algoritmo de **backpropagation** (retropropagación) fue introducido originalmente en la década de 1970, pero su importancia no fue completamente apreciada hasta un famoso artículo de 1986 por David Rumelhart, Geoffrey Hinton y Ronald Williams. (Michael Nielsen en \"Neural Networks and Deep Learning\", http://neuralnetworksanddeeplearning.com/chap2.html).\n",
    "\n",
    "> **backpropagation** es el algoritmo clave que hace que el entrenamiento de deep models sea computacionalmente viable. Para las redes neuronales modernas, puede hacer que el entrenamiento con gradient descend sea hasta diez millones de veces más rápido, en comparación con una naive implementation. Esa es la diferencia entre un modelo que tarda una semana en entrenarse y otro que tardaría 200,000 años. (Christopher Olah, 2016)\n",
    "\n",
    "Hemos visto que, para optimizar nuestros modelos, necesitamos calcular la derivada de la función de pérdida respecto a todos los parámetros del modelo.\n",
    "\n",
    "El cálculo de derivadas en modelos computacionales se aborda mediante cuatro métodos principales:\n",
    "\n",
    "+ trabajando manualmente las derivadas y codificando el resultado (como en el artículo original que describe la retropropagación);\n",
    "+ diferenciación numérica (usando aproximaciones de diferencia finita);\n",
    "+ diferenciación simbólica (usando manipulación de expresiones en software, como Sympy);\n",
    "+ y diferenciación automática (AD).\n",
    "\n",
    "La **diferenciación automática** (AD) funciona aplicando sistemáticamente la **regla de la cadena** del cálculo diferencial a nivel del operador elemental.\n",
    "\n",
    "Supongamos $ y = f(g(x)) $ nuestra función objetivo. En su forma básica, la regla de la cadena establece:\n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial g} \\frac{\\partial g}{\\partial x} $$\n",
    "\n",
    "o, si hay más de una variable $g_i$ entre $y$ y $x$ (por ejemplo, si $f$ es una función bidimensional como $f(g_1(x), g_2(x))$), entonces:\n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial x} = \\sum_i \\frac{\\partial y}{\\partial g_i} \\frac{\\partial g_i}{\\partial x} $$\n",
    "\n",
    "> Ver https://www.math.hmc.edu/calculus/tutorials/multichainrule/\n",
    "\n",
    "Ahora, veamos cómo la AD permite la evaluación precisa de derivadas con machine precision, con solo un pequeño factor constante de sobrecarga.\n",
    "\n",
    "En su descripción más básica, AD se basa en el hecho de que todos los cálculos numéricos\n",
    "son en última instancia composiciones de un conjunto finito de operaciones elementales para las cuales se conocen las derivadas.\n",
    "\n",
    "Por ejemplo, consideremos el cálculo de la derivada de esta función, que representa un modelo de red neuronal de 1 capa:\n",
    "\n",
    "$$\n",
    "    f(x) = \\frac{1}{1 + e^{- ({w}^T \\cdot  x + b)}} \n",
    "$$\n",
    "\n",
    "Primero, escribamos cómo evaluar $f(x)$ a través de una secuencia de operaciones primitivas:\n",
    "\n",
    "\n",
    "```python\n",
    "x = ?\n",
    "f1 = w * x\n",
    "f2 = f1 + b\n",
    "f3 = -f2\n",
    "f4 = 2.718281828459 ** f3\n",
    "f5 = 1.0 + f4\n",
    "f = 1.0/f5\n",
    "```\n",
    "\n",
    "El signo de interrogación indica que $x$ es un valor que debe proporcionarse. \n",
    "\n",
    "Este *programa* puede calcular el valor de $x$ y también **poblar variables del programa**. \n",
    "\n",
    "Podemos evaluar $\\frac{\\parcial f}{\\parcial x}$ en algún $x$ utilizando la regla de la cadena. Esto se llama *forward-mode differentiation*. \n",
    "\n",
    "En nuestro caso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1645026904739,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "KflHgUbk_992",
    "outputId": "c90b6bb1-63ab-4a58-b1cb-1c4c045c7288"
   },
   "outputs": [],
   "source": [
    "def f(x,w,b):\n",
    "    f1 = w * x\n",
    "    f2 = f1 + b\n",
    "    f3 = -f2\n",
    "    f4 = 2.718281828459 ** f3\n",
    "    f5 = 1.0 + f4\n",
    "    return 1.0/f5\n",
    "\n",
    "def dfdx_forward(x, w, b):\n",
    "    f1 = w * x\n",
    "    p1 = w                            # p1 = df1/dx\n",
    "    f2 = f1 + b\n",
    "    p2 = p1 * 1.0                     # p2 = p1 * df2/df1 \n",
    "    f3 = -f2\n",
    "    p3 = p2 * -1.0                    # p3 = p2 * df3/df2\n",
    "    f4 = 2.718281828459 ** f3\n",
    "    p4 = p3 * 2.718281828459 ** f3    # p4 = p3 * df4/df3\n",
    "    f5 = 1.0 + f4\n",
    "    p5 = p4 * 1.0                     # p5 = p4 * df5/df4\n",
    "    f = 1.0/f5\n",
    "    df = p5 * -1.0 / f5 ** 2.0        # df/dx = p5 * df/df5\n",
    "    return f, df\n",
    "\n",
    "der = (f(3+0.00001, 2, 1) - f(3, 2, 1))/0.00001\n",
    "\n",
    "print(\"Value of the function at (3, 2, 1): \",f(3, 2, 1))\n",
    "print(\"df/dx Derivative (fin diff) at (3, 2, 1): \",der)\n",
    "print(\"df/dx Derivative (aut diff) at (3, 2, 1): \",dfdx_forward(3, 2, 1)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O71IkVfjAWs0"
   },
   "source": [
    "Es interesante observar que este *programa* puede derivarse automáticamente si tenemos acceso a **subrutinas que implementan las derivadas de funciones primitivas** (como $\\exp{(x)}$ o $1/x$) y todas las variables intermedias se calculan en el orden correcto. \n",
    "\n",
    "También es interesante señalar que AD permite la evaluación exacta de las derivadas a **machine precision**, con sólo un pequeño factor constante de sobrecarga."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_IbCnklAdG1"
   },
   "source": [
    "Forward differentiation es eficiente para funciones $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ con $n << m$ (sólo se necesitan $O(n)$ barridos). \n",
    "\n",
    "Para los casos $n >> m$ se necesita una técnica diferente. Para ello, reescribiremos la regla de la cadena como:\n",
    "\n",
    "$$\n",
    "\\frac{parcial f}{parcial x} = \\frac{parcial g}{parcial x} \\frac{parcial f}{parcial g}\n",
    "$$\n",
    "\n",
    "para propagar derivadas hacia atrás desde una salida dada. Esto se denomina *diferenciación en modo inverso*. El paso inverso comienza en el final (es decir, $\\frac{\\partial f}{\\partial f} = 1$) y se propaga hacia atrás a todas las dependencias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1645026904739,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "dpaRdXQsARE2",
    "outputId": "de8b5d9e-2d29-4840-b161-f4b471d870e9"
   },
   "outputs": [],
   "source": [
    "def dfdx_backward(x, w, b):\n",
    "    import numpy as np\n",
    "    f1 = w * x\n",
    "    f2 = f1 + b\n",
    "    f3 = -f2\n",
    "    f4 = 2.718281828459 ** f3\n",
    "    f5 = 1.0 + f4\n",
    "    f = 1.0/f5\n",
    "    \n",
    "    pf = 1.0                           # pf = df/df\n",
    "    p5 = 1.0 * -1.0 / (f5 * f5) * pf   # p5 = pf * df/df5 \n",
    "    p4 = p5 * 1.0                      # p4 = p5 * df5/df4\n",
    "    p3 = p4 * np.log(2.718281828459) \\\n",
    "          * 2.718281828459 ** f3       # p3 = p4 * df4/df3\n",
    "    p2 = p3 * -1.0                     # p2 = p3 * df3/df2\n",
    "    p1 = p2 * 1.0                      # p1 = p2 * df2/df1\n",
    "    dfx = p1 * w                       # dfx = p1 * df1/dx \n",
    "    return f, dfx\n",
    "\n",
    "print(\"df/dx Derivative (aut diff) at (3, 2, 1): \",\n",
    "      dfdx_backward(3, 2, 1)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r88BQg4yA44Q"
   },
   "source": [
    "Cualquier función compleja que pueda descomponerse en un conjunto de funciones elementales puede derivarse de forma automática, con machine precision, mediante este algoritmo.\n",
    "\n",
    "**¡Ya no necesitamos codificar derivadas complejas para aplicar SGD!**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Learning from data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
