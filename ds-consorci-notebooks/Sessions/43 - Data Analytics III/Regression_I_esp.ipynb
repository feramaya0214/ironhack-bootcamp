{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de Regresión (I)\n",
    "\n",
    "En este cuaderno, vemos cómo el análisis de regresión puede ayudar a **entender el comportamiento de los datos**, a **predecir valores de datos** (continuos o dicotómicos) y a **encontrar predictores importantes** (modelos dispersos).\n",
    "Presentamos diferentes modelos de regresión: regresión lineal simple, regresión lineal múltiple y regresión polinómica.\n",
    "Evaluamos los resultados cualitativamente mediante herramientas de visualización de Seaborn y cuantitativamente mediante la biblioteca Scikit-learn, así como otras cajas de herramientas.\n",
    "\n",
    "Usamos diferentes conjuntos de datos reales:\n",
    "* Predicción del precio de un nuevo mercado de viviendas\n",
    "* Extensión del hielo marino y cambio climático\n",
    "* Conjunto de datos de diabetes de Scikit-learn\n",
    "* Conjunto de datos macroeconómicos de EE. UU. de Longley\n",
    "* Conjunto de datos de publicidad\n",
    "\n",
    "### Contenidos del cuaderno:\n",
    "\n",
    "- Regresión\n",
    "    - Regresión Lineal Simple\n",
    "    - Regresión Lineal Múltiple\n",
    "    - Regresión Polinómica\n",
    "- OLS (Mínimos Cuadrados Ordinarios)\n",
    "- Evaluación del ajuste (MSE, R^2)\n",
    "- Predicción (Scikit-learn)\n",
    "- Visualización (Seaborn lmplot)\n",
    "\n",
    "## Cómo hacer predicciones sobre cantidades del mundo real.\n",
    "\n",
    "+ ¿Cómo cambia el volumen de ventas con los cambios en los precios? ¿Cómo se ve afectado por el clima?\n",
    "+ ¿Cómo varía la cantidad de un medicamento absorbido con el peso corporal del paciente? ¿Depende de la presión arterial?\n",
    "+ ¿Cuántos clientes puedo esperar hoy?\n",
    "+ ¿A qué hora debo irme a casa para evitar el atasco?\n",
    "+ ¿Cuál es la probabilidad de lluvia para los próximos dos lunes? ¿Cuál es la temperatura esperada?\n",
    "\n",
    "### Ejemplo:\n",
    "    \n",
    "<center><img src=\"files/images/life-expectancy-vs-gdp-per-capita.png\"></center>\n",
    "\n",
    "Puedes encontrar otro ejemplo interesante [aquí](https://ourworldindata.org/grapher/life-expectancy-of-women-vs-life-expectancy-of-women?tab=chart&country=&region=World)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de comenzar, podemos definir algunas configuraciones para el cuaderno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for the visualizations\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline \n",
    "plt.rc('font', size=12) \n",
    "plt.rc('figure', figsize = (12, 5))\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2,'font.family': [u'times']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12 # Let's make our predictions deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notación\n",
    "\n",
    "$x_i$ elemento de un vector, $\\textbf{x}$ vector columna, $\\textbf{x'}$ (transpuesta de $\\textbf{x}$) vector fila, $X$ matriz.\n",
    "\n",
    "### De los Datos a los Modelos\n",
    "\n",
    "Todas estas preguntas tienen una estructura común: preguntamos sobre una variable $\\textbf{y}$ (*respuesta*) que puede expresarse como una combinación de una o más variables (independientes) $\\textbf{x}_i$ (comúnmente llamadas *covariables* o *predictores* o *regresores*).\n",
    "\n",
    "El papel de la regresión es construir un modelo (fórmula) para predecir la respuesta a partir de las covariables.\n",
    "\n",
    "# Modelo de Regresión Lineal\n",
    "\n",
    "El modelo más simple que podemos pensar es el **modelo lineal**, donde la respuesta $\\textbf{y}$ depende linealmente de los predictores $\\textbf{x}_i$:\n",
    "\n",
    "$$ \\textbf{y}  =  a_1 \\textbf{x}_1  + \\dots + a_m \\textbf{x}_{m} + \\epsilon $$ \n",
    "\n",
    "Los $a_i$ se denominan *parámetros* del modelo o *coeficientes* y $\\epsilon$ se llama *término de error*, *término de perturbación* o *ruido* (en contraste con la \"señal\" proporcionada por el resto del modelo). Esta variable captura todos los demás factores que influyen en la variable dependiente $\\textbf{y}$ aparte de los predictores $\\textbf{x}$. \n",
    "\n",
    "Esta ecuación puede reescribirse en una forma más compacta (matricial) como\n",
    "\n",
    "$$ \\textbf{y}  = X \\textbf{w} + \\epsilon $$\n",
    "\n",
    "donde $$ \\textbf{y} = \\left( \\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array} \\right), \n",
    " X = \\left( \\begin{array}{c} x_{11}  \\dots x_{1m} \\\\ x_{21}  \\dots x_{2m}\\\\ \\vdots \\\\ x_{n1}  \\dots x_{nm} \\end{array} \\right),  \\textbf{w} = \\left( \\begin{array}{c} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_m \\end{array} \\right) \n",
    " \\epsilon = \\left( \\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_m \\end{array} \\right) $$\n",
    " \n",
    " **La regresión lineal** es la técnica para crear modelos lineales.\n",
    " \n",
    "### Regresión Lineal Simple\n",
    "\n",
    "En la **regresión lineal simple**, con una sola variable, describimos la relación entre el predictor y la respuesta con una línea recta. \n",
    "\n",
    "El modelo es:\n",
    "$$ \\textbf{y}  =  a_0+ a_1 \\textbf{x}_1 + \\epsilon$$\n",
    "\n",
    "El parámetro $a_0$ se llama *término constante* o *intercepto*.\n",
    "\n",
    "En la forma matricial, agregamos un término constante al cambiar a la matriz: $(\\textbf{1},X).$\n",
    "\n",
    "#### Ejemplo: \n",
    "\n",
    "¿Depende el precio del seguro de la experiencia al conducir?\n",
    "\n",
    "Dada la siguiente información, los precios mensuales de seguros de automóviles ($\\textbf{y}$) y la experiencia de conducción en años ($\\textbf{x}_{1}$) de un conjunto de n=8 sujetos, podemos construir un modelo lineal para responder a esta pregunta.\n",
    "\n",
    "\n",
    "<center><img src=\"files/images/data-insurance.png\" width=\"500\"></center>\n",
    "\n",
    "\n",
    "<center><img src=\"files/images/price-insurance.png\" width=\"500\"></center>\n",
    "\n",
    "\n",
    "\n",
    "También podemos predecir el precio mensual del seguro de automóvil para un conductor con 20 años de experiencia en conducción.\n",
    "\n",
    "### Interpolación vs. extrapolación\n",
    "\n",
    "En la práctica, cuando hacemos una predicción para algún valor de x que no hemos visto antes, necesitamos ser muy cuidadosos. Predecir $y$ para un valor de $x$ que está dentro del intervalo de puntos que vimos en los datos originales (los datos con los que ajustamos nuestro modelo) se llama **interpolación**. Predecir y para un valor de x que está fuera del rango de valores que realmente vimos para x en los datos originales se llama **extrapolación**.\n",
    "Para conjuntos de datos reales, incluso si un ajuste lineal parece apropiado, necesitamos ser extremadamente cuidadosos con la extrapolación, que a menudo puede llevar a predicciones falsas!\n",
    "\n",
    "\n",
    "<center><img src=\"files/images/extrapolation.png\"  width=\"500\"></center>\n",
    "\n",
    "\n",
    "### Regresión Múltiple\n",
    "En la Regresión de Mínimos Cuadrados Ordinarios con una sola variable describimos la relación entre el predictor y la respuesta con una línea recta. Este caso se llama regresión lineal *simple*. \n",
    "\n",
    "La regresión lineal simple se puede extender a un mayor número de variables. Teniendo m variables predictoras, ajustaremos un hiperplano m-dimensional a nuestros m predictores.\n",
    "\n",
    "$$ \\textbf{y} = a_1 \\textbf{x}_1 + \\dots + a_m \\textbf{x}_m = X \\textbf{w} $$\n",
    "\n",
    "### Regresión Polinómica\n",
    "\n",
    "A pesar de su nombre, la regresión lineal se puede usar para ajustar funciones no lineales. Un modelo de regresión lineal es lineal en los parámetros del modelo, no necesariamente en los predictores. Si agregas transformaciones no lineales de tus predictores al modelo de regresión lineal, el modelo será no lineal en los predictores.\n",
    "\n",
    "$$ \\textbf{y} = a_1 \\phi(\\textbf{x}_1) + \\dots + a_m \\phi(\\textbf{x}_m) $$\n",
    "\n",
    "Esta técnica de regresión no lineal muy popular es la *Regresión Polinómica*, una técnica que modela la relación entre la respuesta y los predictores como un polinomio de orden n. Cuanto mayor sea el orden del polinomio, más funciones \"onduladas\" podrás ajustar.\n",
    "\n",
    "Podemos representar una relación curva entre nuestras variables introduciendo términos **polinómicos**, como por ejemplo, un modelo cúbico:\n",
    "\n",
    "\n",
    "$$y_i \\approx a_0 + a_1 x_i + a_2 x_i^2 + a_3 x_i^3$$\n",
    "\n",
    "\n",
    "\n",
    "Usar un polinomio de orden superior tiene un precio: **complejidad computacional** y **sobreajuste**. El sobreajuste se refiere a una situación en la que el modelo se ajusta a las idiosincrasias de los datos de entrenamiento y pierde la capacidad de generalizar de lo visto a predecir lo no visto.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "<center><img src=\"files/images/overfitting.png\" width = '700'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimadores\n",
    "\n",
    "Generemos un conjunto de datos para ilustrar la regresión lineal simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X1 = np.random.randn(300, 2)  # Random floats sampled from a univariate “normal” (Gaussian) distribution\n",
    "A = np.array([[0.6, .4], [.4, 0.6]]) # Transformation matrix\n",
    "X2 = np.dot(X1, A)\n",
    "\n",
    "X = X2[:, 0]\n",
    "y = X2[:, 1]\n",
    "\n",
    "plt.plot(X, y, \"o\", alpha=0.3) # alpha, transparency value, between 0 (transparent) and 1 (opaque)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos crear un modelo lineal para explicar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model=[0+1*x for x in np.arange(-2,3)] # np.arange returns evenly spaced values within a given interval.\n",
    "\n",
    "plt.plot(X, y, \"o\", alpha=0.3);\n",
    "plt.plot(np.arange(-2,3), model,'r'); \n",
    "plt.show()\n",
    "# The red line gives the predicted values of this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero hay otros modelos lineales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(X, y, \"o\", alpha=0.3);\n",
    "# We can use several parameters and we do not know which is the best model\n",
    "model1=[0+1*x for x in np.arange(-2,3)]\n",
    "model2=[0.3+0.9*x for x in np.arange(-2,3)]\n",
    "model3=[0-0.1*x for x in np.arange(-2,3)]\n",
    "plt.plot(np.arange(-2,3), model1,'r')\n",
    "plt.plot(np.arange(-2,3), model2,'g')\n",
    "plt.plot(np.arange(-2,3), model3,'y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Cuál es el mejor modelo para un conjunto de muestras?\n",
    "\n",
    "### MCO (Mínimos Cuadrados Ordinarios)\n",
    "\n",
    "Considera el sistema sobredeterminado\n",
    "\n",
    "$$\\textbf{y} = a_0+a_1 \\textbf{x} $$\n",
    "\n",
    "Los Mínimos Cuadrados Ordinarios (MCO) es el **estimador** más simple y común en el cual los dos valores de $a$ se eligen para minimizar la suma de las distancias al cuadrado entre los valores predichos y los valores reales.\n",
    "\n",
    "Dado el conjunto de muestras $(\\textbf{x},\\textbf{y})$, el objetivo es minimizar:\n",
    "\n",
    "$$ ||a_0 + a_1 \\textbf{x} -  \\textbf{y} ||^2_2 = \\sum_{j=1}^n (a_0+a_1 x_{j} -  y_j )^2,$$ con respecto a $a_0, a_1$.\n",
    "\n",
    "Esta expresión se conoce a menudo como **suma de errores cuadrados de predicción (SSE)**.\n",
    "\n",
    "#### Cómo calcular el MCO: Scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin\n",
    "\n",
    "x = np.array([2.2, 4.3, 5.1, 5.8, 6.4, 8.0])\n",
    "y = np.array([0.4, 10.1, 14.0, 10.9, 15.4, 18.5])\n",
    " \n",
    "# Minimize the sum of squares using a lambda function\n",
    "\n",
    "sse = lambda a, x, y: np.sum((a[0] + a[1]*x - y) ** 2) # Store the sum of squared differences function\n",
    "# Lambda function is a small anonymous function. \n",
    "# It can take any number of arguments, but can only have one expression. \n",
    "# Syntax \"lambda arguments : expression\"\n",
    "\n",
    "a0,a1 = fmin(sse, [0,1], args=(x,y)); # Minimize the sum of squared differences\n",
    "# [0,1] is the initial guess for a[0] and a[1] in function sse.\n",
    "\n",
    "plt.plot(x, y, 'ro')\n",
    "plt.plot([0,10], [a0, a0+a1*10], alpha=0.8) # Add the regression line, colored in blue\n",
    "for xi, yi in zip(x,y):\n",
    "    plt.plot([xi]*2, [yi, a0+a1*xi], \"k:\") # Add pointed black line to illustrate the errors\n",
    "plt.xlim(2, 9); plt.ylim(0, 20) # Restrict the domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las líneas negras señaladas ilustran los errores verticales que se minimizan.\n",
    "\n",
    "**Nota**: Existen alternativas a los modelos de regresión con errores en las variables como los **mínimos cuadrados totales**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otros estimadores\n",
    "\n",
    "Podemos minimizar otros criterios, como la suma de las diferencias absolutas entre los valores predichos y los valores reales (**suma de errores absolutos (SAE)**).\n",
    "\n",
    "$$  \\sum_{j=1}^n |a_0+a_1 x_{j} -  y_j|,$$ con respecto a $a_0, a_1$.\n",
    "\n",
    "**Pregunta**\n",
    "Intenta ajustar un modelo lineal minimizando el SAE y haz el mismo gráfico para visualizar los resultados. ¿Qué diferencias observas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "\n",
    "from scipy.optimize import fmin\n",
    "\n",
    "x = np.array([2.2, 4.3, 5.1, 5.8, 6.4, 8.0])\n",
    "y = np.array([0.4, 10.1, 14.0, 10.9, 15.4, 18.5])\n",
    " \n",
    "# Minimize the sum of squares using a lambda function\n",
    "\n",
    "sse = lambda a, x, y: np.sum((a[0] + a[1]*x - y) ** 2) # Store the sum of squared differences function\n",
    "sae = lambda a, x, y: np.sum(abs(a[0] + a[1]*x - y))\n",
    "# Lambda function is a small anonymous function. \n",
    "# It can take any number of arguments, but can only have one expression. \n",
    "# Syntax \"lambda arguments : expression\"\n",
    "\n",
    "b0,b1 = fmin(sae, [0,1], args=(x,y)); # Minimize the sum of squared differences\n",
    "# [0,1] is the initial guess for b[0] and b[1] in function sse.\n",
    "\n",
    "plt.plot(x, y, 'ro')\n",
    "plt.plot([0,10], [a0, a0+a1*10], alpha=0.8) # Add the regression line, colored in blue\n",
    "plt.plot([0,10], [b0, b0+b1*10], alpha=0.8) # Add the regression line, colored in orange\n",
    "for xi, yi in zip(x,y):\n",
    "    plt.plot([xi]*2, [yi, a0+a1*xi], \"k:\") # Add pointed black line to illustrate the errors\n",
    "plt.xlim(2, 9); plt.ylim(0, 20) # Restrict the domain\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puedes ver, en este caso, los valores lejanos son menos penalizados.\n",
    "\n",
    "**OLS es un enfoque popular por varias razones**\n",
    "\n",
    "+ Es computacionalmente económico calcular los coeficientes.\n",
    "+ Es más fácil de interpretar que modelos más sofisticados. En situaciones donde el objetivo es entender un modelo simple en detalle, en lugar de estimar la respuesta con precisión, pueden proporcionar una visión de lo que el modelo captura.\n",
    "+ Finalmente, en situaciones donde hay mucho ruido, puede ser difícil encontrar la forma funcional verdadera, por lo que un modelo restringido puede funcionar bastante bien en comparación con un modelo complejo que es más afectado por el ruido.\n",
    "\n",
    "El modelo resultante se representa de la siguiente manera:\n",
    "\n",
    "$$\\widehat{\\textbf{y}} = \\widehat{a}_0+\\widehat{a}_1 \\textbf{x}$$\n",
    "\n",
    "Aquí, los sombreros sobre las variables indican que se estiman a partir de los datos que tenemos disponibles.\n",
    "\n",
    "# Implementación en Python con Scikit-learn\n",
    "\n",
    "### División de Entrenamiento y Prueba\n",
    "\n",
    "Uno de los aspectos clave del aprendizaje automático supervisado es la evaluación y validación del modelo. Cuando evalúas el rendimiento predictivo de tu modelo, es esencial que el proceso sea imparcial.\n",
    "\n",
    "No puedes evaluar el rendimiento predictivo de un modelo con los mismos datos que usaste para entrenar. Necesitas evaluar el modelo con datos nuevos que no hayan sido vistos por el modelo antes. Puedes lograrlo dividiendo tu conjunto de datos antes de usarlo.\n",
    "\n",
    "Usando train_test_split() de la biblioteca de ciencia de datos scikit-learn, puedes dividir tu conjunto de datos en subconjuntos que minimicen el potencial de sesgo en tu proceso de evaluación y validación.\n",
    "\n",
    "#### Implementación de la división de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some dummy data\n",
    "X1 = np.random.randn(12, 2)  # Random floats sampled from a univariate “normal” (Gaussian) distribution\n",
    "A = np.array([[0.6, .4], [.4, 0.6]]) # Transformation matrix\n",
    "X2 = np.dot(X1, A)\n",
    "\n",
    "X = X2[:, 0].reshape(-1,1) # X needs to be a 2D array as the input format for the model\n",
    "y = X2[:, 1]\n",
    "\n",
    "print('Size of X and y: {} {}'.format(X.shape, y.shape))\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize our data\n",
    "plt.plot(X, y, \"o\", alpha=0.5) # alpha, transparency value, between 0 (transparent) and 1 (opaque).\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Para completar**\n",
    "Divide en conjuntos de entrenamiento y prueba y verifica los tamaños."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split in train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=seed)\n",
    "\n",
    "# Check train and test sizes\n",
    "print(X.shape, X_train.shape, X_test.shape)\n",
    "print(y.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción\n",
    "\n",
    "Hagamos predicciones usando **Scikit-learn**.\n",
    "\n",
    "Scikit-learn es una biblioteca que ofrece una variedad de técnicas de aprendizaje automático supervisadas y no supervisadas. Scikit-learn proporciona una interfaz orientada a objetos centrada en el concepto de un Estimador.\n",
    "\n",
    "El método <code>Estimator.fit</code> establece el estado del estimador basado en los *datos de entrenamiento*. Por lo general, los datos están compuestos por un arreglo de numpy bidimensional $X$ de forma <code>(n_samples, n_predictors)</code> que contiene la llamada *matriz de características* y un arreglo de numpy unidimensional $\\textbf{y}$ que contiene las *respuestas*. Algunos estimadores permiten al usuario controlar el comportamiento de ajuste.\n",
    "Los estimadores que pueden generar predicciones proporcionan un método ``Estimator.predict``.\n",
    "En el caso de la regresión, ``Estimator.predict`` devolverá los valores de regresión predichos, $\\widehat{\\textbf{y}}$.\n",
    "\n",
    "Durante el proceso de ajuste, el estado del estimador se almacena en los atributos de instancia que tienen un guion bajo al final (``'_'``). Por ejemplo, los coeficientes de un estimador de ``LinearRegression`` se almacenan en el atributo ``coef_``. Veamos un ejemplo práctico:\n",
    "\n",
    "**Para completar**\n",
    "Crea el estimador de Regresión Lineal y realiza el ajuste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create the Linear Regression estimator\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Perform the fitting\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "# Print coefs\n",
    "print(lm.intercept_, lm.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define xmin xmax for the model:\n",
    "xmin, xmax = round(X.min()), round(X.max())\n",
    "\n",
    "x_model = np.arange(xmin, xmax+1)\n",
    "y_model=[lm.intercept_+lm.coef_*x for x in x_model] # np.arange returns evenly spaced values within a given interval.\n",
    "\n",
    "plt.plot(X_train, y_train, \"o\", alpha=0.5, label=\"train\") # alpha, transparency value, between 0 (transparent) and 1 (opaque).\n",
    "plt.plot(X_test, y_test, \"o\", alpha=0.5, label=\"test\") # alpha, transparency value, between 0 (transparent) and 1 (opaque).\n",
    "plt.plot(x_model, y_model,'r'); \n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# The red line gives the predicted values of this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo 1: Datos de Vivienda en Boston\n",
    "\n",
    "El conjunto de datos de vivienda de Boston proporciona registros de mediciones de 13 atributos de los mercados de vivienda alrededor de Boston, así como el precio medio. Queremos predecir el precio de un mercado dado un conjunto de atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "boston = datasets.load_boston() # Dictionary-like object that exposes its keys as attributes.\n",
    "X_boston,y_boston = boston.data, boston.target # Create X matrix and y vector from the dataset.\n",
    "print('Shape of data: {} {}'.format(X_boston.shape, y_boston.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos de Boston tiene 506 instancias y 13 atributos.\n",
    "Veamos el contenido del conjunto de datos, que son las claves de los atributos o características y la descripción general:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('keys: {}'.format(boston.keys()))\n",
    "print('feature names: {}'.format(boston.feature_names))\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the Boston dataset\n",
    "df_boston = pd.DataFrame(boston.data, columns=boston.feature_names) \n",
    "df_boston['PRICE'] = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero consideramos la tarea de predecir los **valores medianos de las casas** en el área de Boston utilizando como predictor uno de los atributos, por ejemplo, **LSTAT**, definido como \"la proporción de estatus bajo de la población\".\n",
    "\n",
    "**Para completar**\n",
    "- Visualiza los datos que queremos ajustar\n",
    "- Divide tus datos en un conjunto de entrenamiento y otro de prueba\n",
    "- Ajusta una regresión lineal simple usando la variable LSTAT para predecir el Precio\n",
    "- ¿Qué tan buena es la regresión?\n",
    "\n",
    "Primero, visualicemos los datos que queremos ajustar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "X = df_boston.LSTAT.values.reshape(-1, 1)\n",
    "y = df_boston.PRICE.values\n",
    "\n",
    "plt.plot(X, y, \"o\", alpha=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide tus datos en conjuntos de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajusta una regresión lineal simple: **LSTAT vs Precio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Linear Regression estimator\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Perform the fitting\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "# Print coeficient and intercept of the regression\n",
    "print(lm.intercept_, lm.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualicemos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "y_train_pred = lm.predict(X_train)\n",
    "\n",
    "plt.plot(X_train, y_train, \"o\", alpha=.5)\n",
    "plt.plot(X_test, y_test, \"o\", alpha=.5)\n",
    "plt.plot(X_train, y_train_pred, \"r\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluación\n",
    "\n",
    "Podemos evaluar el ajuste del modelo calculando el error cuadrático medio ($MSE$) y el coeficiente de determinación ($R^2$) del modelo.\n",
    "\n",
    "El coeficiente $R^2$ se define como:\n",
    "$$R^2 = (1 - \\textbf{u}/\\textbf{v})$$\n",
    "donde $\\textbf{u}$ es la suma residual de cuadrados: $$\\textbf{u}=\\sum (\\textbf{y} - \\widehat{\\textbf{y}} )^2$$ donde ${\\textbf{y}}$ es la respuesta observada y $\\widehat{\\textbf{y}}$ es la respuesta predicha.\n",
    "\n",
    "Y $\\textbf{v}$ es la suma total de cuadrados: $$\\textbf{v}=\\sum (\\textbf{y} - \\bar{\\textbf{y}})^2,$$ donde $\\bar{\\textbf{y}}$ es la media de los datos observados.\n",
    "\n",
    "El mejor puntaje posible para $R^2$ es 1.0 (cuando $\\textbf{u}=0$): valores más bajos son peores. $R^2$ es 0.0 cuando $\\textbf{u}=\\textbf{v}$.\n",
    "\n",
    "**Para completar**\n",
    "\n",
    "Realiza la evaluación:\n",
    "\n",
    "- Usando la puntuación del modelo\n",
    "- Usando métricas de sklearn\n",
    "\n",
    "#### Usando la puntuación del modelo\n",
    "\n",
    "El método score devuelve el coeficiente de determinación R^2 de la predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Score for training and testing sets \n",
    "# Best possible score is 1.0, lower values are worse.\n",
    "print('Score:')\n",
    "print(lm.score(X_train, y_train), lm.score(X_test, y_test))\n",
    "\n",
    "# Compute MSE for training and testing sets \n",
    "y_train_pred = lm.predict(X_train)\n",
    "y_test_pred = lm.predict(X_test)\n",
    "\n",
    "mse_train = np.mean((y_train_pred - y_train)**2)\n",
    "mse_test = np.mean((y_test_pred - y_test)**2)\n",
    "print('MSE:')\n",
    "print(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usando métricas de sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Make prediction\n",
    "y_train_pred = lm.predict(X_train)\n",
    "y_test_pred = lm.predict(X_test)\n",
    "\n",
    "# Compute Score\n",
    "print('Score:')\n",
    "print(r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred))\n",
    "\n",
    "# Compute MSE for training and testing sets \n",
    "print('MSE:')\n",
    "print(mean_squared_error(y_train_pred, y_train), mean_squared_error(y_test_pred, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluación estadística con modelos estadísticos\n",
    "\n",
    "El paquete ``statsmodels`` ofrece varias clases diferentes que proporcionan distintas opciones para la regresión lineal. Comenzar con la regresión lineal es bastante sencillo con el módulo OLS.\n",
    "\n",
    "Podemos realizar la regresión del predictor sobre la respuesta, utilizando la clase ``sm.OLS`` y su método de inicialización ``OLS(y, X)``. Este método toma como entrada dos objetos similares a arrays: $X$ y $\\textbf{y}$. En general, $X$ será un arreglo de numpy o un marco de datos de pandas con forma ``(n, p)`` donde $n$ es el número de puntos de datos y $p$ es el número de predictores. $\\textbf{y}$ es un arreglo de numpy unidimensional o una serie de pandas de longitud $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "est = sm.OLS(y_train, X_train) # Creates an object OLS estimator\n",
    "est = est.fit()\n",
    "est.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compara el rendimiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_boston,y_boston = boston.data, boston.target # Create X matrix and y vector from the dataset.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_boston, y_boston, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Score using a single feature\n",
    "regr_feat1 = LinearRegression()\n",
    "scores = []\n",
    "n_features = boston.feature_names.shape[0]\n",
    "\n",
    "for i in np.arange(n_features):\n",
    "    # Select feature i\n",
    "    feat_name = boston.feature_names[i]\n",
    "    feat1_train=X_train[:,i:i+1]\n",
    "    feat1_test=X_test[:,i:i+1]\n",
    "    \n",
    "    # Train model\n",
    "    regr_feat1.fit(feat1_train, y_train)   \n",
    "    \n",
    "    # Predict\n",
    "    y_train_pred = regr_feat1.predict(feat1_train)\n",
    "    y_test_pred = regr_feat1.predict(feat1_test)\n",
    "    \n",
    "    # Evaluate model\n",
    "    train_score = regr_feat1.score(feat1_train, y_train)\n",
    "    test_score = regr_feat1.score(feat1_test, y_test)\n",
    "    mse_train = np.mean((y_train_pred - y_train)**2)\n",
    "    mse_test = np.mean((y_test_pred - y_test)**2)\n",
    "    \n",
    "    scores.append([train_score, test_score, mse_train, mse_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame(scores, columns=[\"train_score\", \"test_score\",\"train_mse\", \"test_mse\"], index=boston.feature_names)\n",
    "df_scores.sort_values(by=\"test_score\", ascending=False, inplace=True)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise train and test scores\n",
    "f = plt.figure(figsize=(15,5))\n",
    "ax1 = f.add_subplot(121)\n",
    "ax2 = f.add_subplot(122)\n",
    "\n",
    "df_scores[[\"train_score\", \"test_score\"]].plot(kind=\"bar\", ax=ax1, title='Score')\n",
    "df_scores[[\"train_mse\", \"test_mse\"]].plot(kind=\"bar\", ax=ax2, title='MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajustando un modelo lineal múltiple\n",
    "También es posible ajustar un modelo lineal múltiple, utilizando todas las variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Linear Regression estimator\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Perform the fitting\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "# Print coefs\n",
    "print(lm.intercept_, lm.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Score for training and testing sets \n",
    "train_score = lm.score(X_train, y_train)\n",
    "test_score = lm.score(X_test, y_test)\n",
    "\n",
    "print(train_score, test_score)\n",
    "\n",
    "# Compute MSE for training and testing sets \n",
    "y_train_pred = lm.predict(X_train)\n",
    "y_test_pred = lm.predict(X_test)\n",
    "\n",
    "mse_train = np.mean((y_train_pred - y_train)**2)\n",
    "mse_test = np.mean((y_test_pred - y_test)**2)\n",
    "print(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EJERCICIO 1: Cambio Climático y Extensión del Hielo Marino**<p>\n",
    "\n",
    "Queremos responder a la pregunta: ¿Ha habido una disminución en la cantidad de hielo en los últimos años?\n",
    "\n",
    "Para ello utilizaremos las mediciones de la extensión del hielo marino del [Centro Nacional de Datos de Nieve y Hielo](https://nsidc.org).\n",
    "\n",
    "Realizaremos los siguientes pasos de procesamiento:\n",
    "\n",
    "- Debemos leer y limpiar los datos.\n",
    "- Para calcular la tendencia en un intervalo de tiempo dado (mes), necesitamos normalizar los datos.\n",
    "- Estos valores pueden ser graficados para toda la serie temporal o para meses específicos.\n",
    "- Podemos calcular la tendencia como una regresión lineal simple (OLS) y evaluarla cuantitativamente.\n",
    "- También podemos estimar el valor de la extensión para 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and show the info and contents:\n",
    "import pandas as pd\n",
    "ice = pd.read_csv('files/ch06/SeaIce.txt',delim_whitespace=True)\n",
    "print('shape: {}'.format(ice.shape))\n",
    "ice.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular la anomalía en un intervalo de tiempo dado, podemos calcular la media para ese intervalo de tiempo (usando el período de 1981 a 2010 para la extensión media), antes de la limpieza de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización\n",
    "\n",
    "### La importancia de graficar\n",
    "\n",
    "El cuarteto de Anscombe comprende cuatro conjuntos de datos que tienen estadísticas descriptivas simples casi idénticas, pero tienen distribuciones muy diferentes y parecen muy diferentes cuando se grafican. Cada conjunto de datos consiste en once puntos (x, y). Fueron construidos en 1973 por el estadístico Francis Anscombe para demostrar tanto la importancia de graficar los datos antes de analizarlos como el efecto de los valores atípicos y otras observaciones influyentes en las propiedades estadísticas.\n",
    "\n",
    "<center><img src=\"files/images/Anscombe's_quartet_3.svg\"  width=\"600\"></center>\n",
    "\n",
    "Fuente [Wikipedia](https://en.wikipedia.org/wiki/Anscombe%27s_quartet).\n",
    "\n",
    "### Ejemplo 1: Datos de Vivienda\n",
    "\n",
    "Continuaremos con nuestro conjunto de datos de vivienda de Boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datayou\n",
    "from sklearn import datasets\n",
    "boston = datasets.load_boston() # Dictionary-like object that exposes its keys as attributes.\n",
    "df_boston = pd.DataFrame(boston.data, columns=boston.feature_names) # Create a DataFrame from the Boston dataset\n",
    "df_boston['PRICE'] = boston.target\n",
    "print('Shape of data: {}'.format(df_boston.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of prices:\n",
    "plt.hist(df_boston.PRICE) \n",
    "plt.xlabel('price ($1000s)')\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de Regresión Lineal: Seaborn lmplot\n",
    "\n",
    "La función ``lmplot()`` del módulo Seaborn está diseñada para explorar relaciones lineales de diferentes formas en conjuntos de datos multidimensionales. Los datos de entrada deben estar en un ``DataFrame`` de Pandas. Para graficar, proporciona los nombres de las variables predictoras y de respuesta junto con el conjunto de datos.\n",
    "\n",
    "Primero consideramos la tarea de predecir los valores medianos de las casas en el área de Boston usando como predictor uno de los atributos, por ejemplo, LSTAT, definido como \"la proporción de estatus más bajo de la población\".\n",
    "La visualización de Seaborn se puede usar para mostrar estas relaciones lineales fácilmente:\n",
    "\n",
    "Usaremos la variable precio como nuestra respuesta $\\textbf{y}$ y LSTAT como nuestro predictor $\\textbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the relations between price and LSTAT\n",
    "sns.lmplot(x=\"LSTAT\", y=\"PRICE\", data=df_boston, aspect=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lmplot tiene dos componentes principales.\n",
    "\n",
    "+ El primero es un diagrama de dispersión, que muestra los puntos de datos observados.\n",
    "+ El segundo es una línea de regresión, que muestra el modelo lineal estimado que relaciona las dos variables. Debido a que la línea de regresión es solo una estimación, se traza con una banda de confianza del 95% para dar una impresión de la certeza en el modelo (usando Bootstraping).\n",
    "\n",
    "¿Es la relación entre el precio de la casa y ``lstat`` no lineal? ¿Es la línea recta un ajuste pobre? Quizás se pueda obtener un mejor ajuste incluyendo términos de orden superior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate a polynomial regression of order 2\n",
    "sns.lmplot(x=\"LSTAT\", y=\"PRICE\", data=df_boston, aspect=2, order=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Estimate a polynomial regression of order 3\n",
    "sns.lmplot(x=\"LSTAT\", y=\"PRICE\", data=df_boston, aspect=2, order=3);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué pasaría si consideramos la variable RM o AGE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualization of the relations between price and \"average number of rooms per dwelling\"\n",
    "sns.lmplot(x=\"RM\", y=\"PRICE\", data=df_boston, aspect=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualization of the relations between price and \"proportion of owner-occupied units built prior to 1940\"\n",
    "sns.lmplot(x=\"AGE\", y=\"PRICE\", data=df_boston, aspect=2, order=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the relations between price and \"proportion of owner-occupied units built prior to 1940\"\n",
    "sns.lmplot(x=\"AGE\", y=\"PRICE\", data=df_boston, aspect=2, order=3);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
