{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0df9d3d-63be-4d3b-bf13-d38fbf754b50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Introducción a Big Data y PySpark\n",
    "\n",
    "## 1.1 Antecedentes de Big Data\n",
    "\n",
    "En los últimos años, el término \"Big Data\" ha evolucionado para representar un concepto fundamental en el ámbito de la tecnología y los negocios. Esencialmente, Big Data se refiere al enorme volumen de datos estructurados y no estructurados que se generan cada segundo en el mundo digitalizado de hoy. Esta sección profundizará en qué consiste Big Data y por qué es crucial en el contexto contemporáneo, destacando especialmente su relación con Apache Spark.\n",
    "\n",
    "### 1.1.1 Comprensión de las Tres V's de Big Data\n",
    "\n",
    "Antes de adentrarnos más en el ámbito de Big Data, es esencial familiarizarnos con los conceptos fundamentales que forman su base: las Tres V's: Volumen, Velocidad y Variedad. Estos términos nos ayudan a comprender la magnitud y complejidad de Big Data, proporcionándonos las herramientas para navegar y gestionar este vasto océano digital de manera más efectiva. Tomemos un momento para entender cada uno de estos aspectos en puntos:\n",
    "\n",
    "- **Volumen**: En el mundo de Big Data, 'Volumen' se refiere a la inmensa cantidad de datos que se generan cada momento de cada día. No se trata solo de la información almacenada en bases de datos; incluye datos de redes sociales, sitios web, smartphones y muchas otras fuentes. Para darles una idea, es como intentar llenar un cubo con un flujo interminable de agua, donde el cubo representa nuestra capacidad de almacenamiento y el agua representa los datos.\n",
    "\n",
    "- **Velocidad**: 'Velocidad' señala la impresionante rapidez con la que se generan y recopilan estos datos. No es un río tranquilo, sino un torrente de información que fluye cada segundo desde diversas fuentes. En este entorno de ritmo acelerado, poder procesar y analizar datos rápidamente es crucial para mantenerse al día con el panorama en constante cambio y tomar decisiones oportunas.\n",
    "\n",
    "- **Variedad**: Por último, pero no menos importante, 'Variedad' enfatiza los diferentes tipos de datos que encontramos en el universo de Big Data. Los datos pueden ser estructurados, como las filas y columnas ordenadas en una hoja de cálculo, o no estructurados, como el contenido de un correo electrónico o una publicación en redes sociales. Poder manejar esta diversa gama de datos, entender y extraer valiosas perspectivas de ellos es una habilidad muy buscada en el mundo de Big Data.\n",
    "\n",
    "### 1.1.2 Tecnologías de Big Data\n",
    "\n",
    "La llegada de las tecnologías de Big Data ha revolucionado la forma en que manejamos enormes volúmenes de datos, transformando tareas de gestión de datos abrumadoras en procesos manejables y eficientes. Las tecnologías clave en este panorama incluyen Hadoop, Spark y otras, cada una desempeñando un papel vital en el almacenamiento, procesamiento y análisis de Big Data.\n",
    "\n",
    "#### Hadoop: La base del procesamiento de Big Data\n",
    "Hadoop ha sido una plataforma fundamental en el procesamiento de Big Data, permitiendo el almacenamiento y procesamiento distribuido de grandes conjuntos de datos a través de clusters de computadoras. Sus componentes clave incluyen:\n",
    "- **HDFS (Sistema de Archivos Distribuidos de Hadoop)**: Divide archivos en grandes bloques y los distribuye a través de nodos en un clúster, asegurando alta disponibilidad de datos y tolerancia a fallos. [Más información sobre HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html).\n",
    "- **MapReduce**: Un modelo de programación que procesa grandes conjuntos de datos en paralelo a través de un clúster de Hadoop. [Más información sobre MapReduce](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html).\n",
    "- **YARN (Yet Another Resource Negotiator)**: Gestiona recursos y programa aplicaciones en clústers. [Más información sobre YARN](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html).\n",
    "- **Hadoop Common**: Proporciona utilidades comunes y bibliotecas que apoyan otros módulos de Hadoop. [Más información sobre Hadoop Common](https://hadoop.apache.org/docs/current/).\n",
    "- **HBase**: Una base de datos NoSQL que funciona sobre HDFS, ofreciendo acceso de lectura/escritura en tiempo real. [Más información sobre HBase](https://hbase.apache.org/).\n",
    "\n",
    "Consulta: [video de Hadoop](https://www.youtube.com/watch?v=aReuLtY0YMI)\n",
    "\n",
    "#### Spark: Avanzando en el procesamiento de Big Data\n",
    "Siguiendo el ejemplo de Hadoop, Spark introdujo avances significativos, particularmente en la computación en memoria, mejorando las velocidades de procesamiento de datos. Sus características clave incluyen:\n",
    "- **Computación en Memoria**: Almacena datos en memoria, reduciendo el tiempo de procesamiento en comparación con el enfoque basado en disco de Hadoop. [Más información sobre la Computación en Memoria](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence).\n",
    "- **RDD (Conjuntos de Datos Distribuidos Resilientes)**: Colecciones de elementos tolerantes a fallos procesados en paralelo. [Más información sobre RDD](https://spark.apache.org/docs/latest/rdd-programming-guide.html).\n",
    "- **DataFrame y Dataset**: APIs para operaciones de datos estructurados. [Más información sobre DataFrame y Dataset](https://spark.apache.org/docs/latest/sql-programming-guide.html).\n",
    "- **MLlib**: Una biblioteca de aprendizaje automático para ciencia de datos escalable. [Más información sobre MLlib](https://spark.apache.org/docs/latest/ml-guide.html).\n",
    "- **GraphX**: Permite el procesamiento de datos de grafos. [Más información sobre GraphX](https://spark.apache.org/docs/latest/graphx-programming-guide.html).\n",
    "- **Spark Streaming**: Facilita el procesamiento de flujos de datos en vivo tolerante a fallos. [Más información sobre Spark Streaming](https://spark.apache.org/docs/latest/streaming-programming-guide.html).\n",
    "\n",
    "Consulta: [video de Spark](https://www.youtube.com/watch?v=VZ7EHLdrVo0)\n",
    "\n",
    "#### Expandiendo el Ecosistema de Big Data\n",
    "Más allá de Hadoop y Spark, el ecosistema de Big Data abarca otras tecnologías clave:\n",
    "- **Apache Kafka**: Una plataforma para manejar flujos de datos en tiempo real. Esencial para el streaming de alta capacidad y tolerante a fallos.\n",
    "- **Apache Flink**: Conocido por sus capacidades de procesamiento de flujos y análisis de datos en tiempo real.\n",
    "- **Bases de Datos NoSQL**: Como Cassandra y MongoDB, estas bases de datos apoyan el almacenamiento y gestión de datos distribuidos a gran escala.\n",
    "\n",
    "#### Perspectivas Prácticas y Tendencias Futuras\n",
    "- **Casos de Uso**: Hadoop sobresale en el procesamiento por lotes, mientras que Spark es preferido para análisis en tiempo real y algoritmos iterativos.\n",
    "- **Herramientas del Ecosistema**: Hive y Pig mejoran las capacidades de Hadoop y Spark proporcionando consultas similares a SQL y scripting de flujo de datos, respectivamente.\n",
    "- **Tendencias y Desafíos**: El cambio hacia soluciones basadas en la nube, la integración de aprendizaje automático, y abordar desafíos como la seguridad de datos, la complejidad de proyectos, y las curvas de aprendizaje están dando forma al futuro de Big Data.\n",
    "\n",
    "#### Mejorando el Aprendizaje con Aplicaciones Prácticas\n",
    "Ejemplos prácticos, estudios de caso y conjuntos de datos de muestra fomentan la comprensión práctica y vinculan la teoría con aplicaciones del mundo real.\n",
    "\n",
    "Al aprovechar estas tecnologías, las organizaciones pueden almacenar, procesar y analizar grandes conjuntos de datos de manera eficiente, lo que lleva a un procesamiento de datos más rápido, percepciones profundas y toma de decisiones informadas, fomentando así la innovación en varios campos.\n",
    "\n",
    "\n",
    "## 1.2 Introducción a Spark\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg\" alt=\"Alt text\" width=\"500\" height=\"300\">\n",
    "\n",
    "En el dinámico dominio de Big Data, [Apache Spark](https://spark.apache.org/) ha surgido como un motor de cómputo preeminente que está a la vanguardia del procesamiento y análisis de Big Data. Diseñado para ser tanto rápido como de propósito general, facilita la extracción sin problemas de insights de conjuntos de datos sustanciales, jugando un papel fundamental en el proceso de toma de decisiones impulsado por datos moderno. Aprovechar plataformas como [Databricks](https://databricks.com/), amplifica sus capacidades, ofreciendo un entorno colaborativo e interactivo que se integra sin esfuerzo con Spark.\n",
    "\n",
    "### 1.2.1 Visión General de Spark\n",
    "\n",
    "[Apache Spark](https://spark.apache.org/docs/latest/), renombrado por su motor de cómputo unificado, ha traído un cambio de paradigma en el procesamiento y análisis de Big Data. Superando las capacidades de las tecnologías de Big Data más antiguas, brilla cuando se trata de manejar grandes conjuntos de datos, ofreciendo un marco escalable, tolerante a fallos y adaptativo. Su capacidad de cómputo en memoria permite un procesamiento de datos relámpago rápido, lo que lo convierte en una herramienta invaluable en el kit de herramientas de analistas de datos y científicos que buscan obtener insights accionables de Big Data. Su compatibilidad con diversas fuentes de datos y operación sin problemas tanto en local como en la nube lo posiciona como una solución versátil para los intrincados desafíos planteados por Big Data.\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/cluster-overview.png\" alt=\"Alt text\" width=\"500\" height=\"300\">\n",
    "\n",
    "En el ecosistema de Spark, el flujo de trabajo operativo está coordinado por tres componentes principales: el Programa Conductor, el Administrador del Clúster y los Nodos Trabajadores.\n",
    "\n",
    "- **Programa Conductor**: El componente central que gobierna la ejecución general de la aplicación Spark. Traduce las tareas del programa del usuario en unidades de trabajo que pueden distribuirse entre los nodos trabajadores. El programa conductor también recopila los resultados de los nodos trabajadores y entrega el resultado final.\n",
    "\n",
    "- **Administrador del Clúster**: Esta entidad externa supervisa la asignación de recursos dentro del clúster de Spark, gestionando esencialmente la distribución de tareas. El administrador del clúster podría ser autónomo o integrado con otras plataformas de gestión de clústeres como Mesos o YARN, asegurando que los recursos se utilicen de manera óptima y que las tareas se asignen adecuadamente para fomentar una ejecución rápida.\n",
    "\n",
    "- **Nodos Trabajadores**: Estos son los ejecutores reales de las tareas asignadas por el programa conductor. Cada nodo trabajador mantiene un proceso ejecutor que es responsable de ejecutar las tareas individuales. Después de ejecutar las tareas, los nodos devuelven los resultados al programa conductor. Su papel es crucial para asegurar el procesamiento paralelo, acelerando así significativamente el procesamiento y análisis de datos.\n",
    "\n",
    "Esta trinidad forma la columna vertebral de una aplicación Spark, asegurando fluidez y eficiencia en el procesamiento y análisis de Big Data.\n",
    "\n",
    "#### 1.2.2 Integración con Databricks\n",
    "\n",
    "[Databricks](https://databricks.com/product/unified-data-analytics-platform), fundado por los creadores originales de Apache Spark, sirve como una plataforma unificada de análisis de datos que mejora las capacidades de Spark al proporcionar un entorno basado en la nube que fomenta la colaboración y la innovación. Databricks facilita la integración fluida de la ciencia de datos, la ingeniería de datos y el análisis de datos en una sola plataforma, permitiendo a las organizaciones acelerar la innovación y mejorar la eficiencia. Su espacio de trabajo interactivo empodera a los equipos para colaborar y compartir insights, fomentando una cultura de toma de decisiones basada en datos. Además, su integración nativa con Spark asegura que puedas aprovechar al máximo el poder de Spark con seguridad mejorada, flujos de trabajo optimizados y análisis avanzados, convirtiéndolo en una piedra angular en el ecosistema de Big Data.\n",
    "\n",
    "### 1.2.3 Características y Beneficios de Usar PySpark (Elección SCRM)\n",
    "\n",
    "[PySpark](https://spark.apache.org/docs/latest/api/python/) es la API de Python para Apache Spark, combinando la potencia de procesamiento de datos de Spark con la versatilidad y facilidad de uso de Python. Aquí están las características y beneficios destacados de usar PySpark:\n",
    "\n",
    "1. **Velocidad**: PySpark aprovecha el cómputo en memoria de Spark, lo que le permite ejecutar cargas de trabajo hasta 100 veces más rápido que las herramientas tradicionales de procesamiento de Big Data. Utiliza técnicas avanzadas de optimización para ofrecer análisis extremadamente rápidos.\n",
    "\n",
    "2. **Facilidad de Uso**: Equipado con más de 100 operadores de alto nivel, PySpark facilita la construcción fácil de aplicaciones paralelas, reduciendo el tiempo y esfuerzo necesario para desarrollar tuberías de procesamiento de datos. Su integración con Python, un lenguaje conocido por su simplicidad, añade aún más a la facilidad de uso.\n",
    "\n",
    "3. **Generalidad**: PySpark ofrece una solución unificada que combina sin problemas consultas SQL, análisis de streaming y análisis complejos bajo una sola plataforma. Esta generalidad significa que los profesionales de datos pueden usar una sola herramienta para una gama diversa de tareas de datos, mejorando la eficiencia y productividad.\n",
    "\n",
    "4. **Funciona en Todas Partes**: Con su arquitectura flexible, Spark puede operar en diversos entornos, incluyendo Hadoop, Apache Mesos, Kubernetes, clústeres autónomos o en la nube, asegurando que puedas usarlo de la manera que mejor se adapte a las necesidades de tu organización.\n",
    "\n",
    "### 1.2.4 PySpark vs. Otras Herramientas de Big Data\n",
    "\n",
    "PySpark ha tallado un lugar distinto para sí mismo en el kit de herramientas de Big Data, ofreciendo capacidades no encontradas o limitadas en otras herramientas de Big Data. Así es como se distingue:\n",
    "\n",
    "- **Procesamiento por Lotes y Transmisión de Datos en Tiempo Real**: Mientras que otras herramientas pueden sobresalir en el procesamiento por lotes o la transmisión, PySpark maneja proficientemente ambos, permitiendo el procesamiento de datos por lotes y flujos de datos en tiempo real dentro del mismo marco.\n",
    "\n",
    "- **Aprendizaje Automático y Procesamiento de Grafos**: PySpark va más allá del simple procesamiento de datos para ofrecer un rico conjunto de bibliotecas para el aprendizaje automático y el procesamiento de grafos. Esto significa que puedes construir modelos predictivos y analizar redes complejas directamente dentro de tu entorno PySpark, sin necesidad de herramientas adicionales.\n",
    "\n",
    "- **Integración con Python**: Al aprovechar el lenguaje de programación Python, PySpark abre un rico ecosistema de bibliotecas y herramientas que pueden usarse en conjunto con las capacidades de procesamiento de datos de Spark, ofreciendo una solución holística para el análisis de datos.\n",
    "\n",
    "A través de su combinación de velocidad, facilidad de uso y amplias capacidades, PySpark se establece como una herramienta versátil y poderosa en el mundo de Big Data, empoderando a los profesionales para obtener insights más profundos y agregar más valor a sus esfuerzos de análisis de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4e65cd2-4851-4c4b-89fc-3be181a6cab4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[1]: </div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[1]: </div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=4834245828650669#setting/sparkui/0614-152608-357o4auh/driver-8304742766061018811\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.38.88.11:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=4834245828650669#setting/sparkui/0614-152608-357o4auh/driver-8304742766061018811\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://10.38.88.11:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inicialización de una SparkSession en Databricks\n",
    "\n",
    "# En Spark, la SparkSession es el punto de entrada a cualquier funcionalidad de Spark. Cuando trabajas con la API de DataFrame y Dataset, SparkSession es la referencia que usas para iniciar cualquier tipo de transformación o acción sobre los datos.\n",
    "\n",
    "# En Databricks, no necesitas crear una SparkSession manualmente, ya que se inicializa automáticamente y está disponible a través de la variable `spark`. Esta variable contiene la SparkSession y puede ser utilizada para acceder a una gran cantidad de funcionalidades ofrecidas por PySpark.\n",
    "\n",
    "# El siguiente comando te ayudará a recuperar detalles sobre la SparkSession creada automáticamente en Databricks. Incluye información como el nombre de la aplicación, la URL del Spark master y la versión de Spark que se está utilizando.\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "297bd594-053b-4495-b17e-73b8cbc08780",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.3 Configuración de PySpark\n",
    "\n",
    "Antes de sumergirse en las funcionalidades de PySpark, es esencial configurar adecuadamente tu entorno de PySpark. En Databricks, este proceso de configuración está optimizado para facilitar la facilidad de uso y los ciclos de desarrollo rápidos. Aquí tienes una guía detallada para ayudarte a configurar PySpark en Databricks:\n",
    "\n",
    "### 1.3.1 Instalación y configuración de PySpark\n",
    "\n",
    "En Databricks, el proceso de configuración para PySpark es prácticamente inexistente, ahorrándote las molestias de instalaciones y configuraciones. PySpark viene preinstalado y configurado, permitiéndote iniciar tus proyectos de análisis de datos sin demoras. Esto es particularmente beneficioso para los recién llegados que pueden sumergirse directamente en el aprendizaje y uso de las funcionalidades de PySpark sin preocuparse por las complejidades de la configuración.\n",
    "\n",
    "### 1.3.2 Configuración de un entorno de desarrollo\n",
    "\n",
    "Databricks brilla como un espacio de trabajo colaborativo, ofreciendo una plataforma interactiva donde puedes crear scripts de PySpark con facilidad. El espacio de trabajo facilita no solo la creación de scripts sino también el desarrollo de visualizaciones vívidas, mejorando la representación de datos y los insights. Además, puedes compartir tu trabajo de manera fluida con otros, fomentando la colaboración y el aprendizaje. Este espacio de trabajo interactivo y colaborativo se demuestra ser una piedra angular para los equipos que apuntan a desarrollar soluciones basadas en datos de manera eficiente.\n",
    "\n",
    "### 1.3.3 (Avanzado) Integración con Airflow para ejecutar scripts de Python en Databricks\n",
    "\n",
    "(Esta es una introducción, no profundizaremos más) La integración con Apache Airflow ofrece una forma eficiente de orquestar y automatizar la ejecución de scripts de Python en los clústeres de Databricks. Esta configuración te permite programar y monitorear flujos de trabajo e integrar Databricks de manera fluida en tu pipeline de datos. Aquí tienes una guía paso a paso sobre cómo configurar esta integración:\n",
    "\n",
    "1. **Configura Airflow**: Asegúrate de tener Apache Airflow instalado y configurado en tu entorno de trabajo (hay un Airflow común para Omnichannel, no es necesario configurarlo). Puedes encontrar instrucciones detalladas de instalación en la [documentación oficial de Airflow](https://airflow.apache.org/docs/apache-airflow/stable/installation.html).\n",
    "\n",
    "2. **Plugin de Databricks**: Instala el plugin de Databricks para Airflow. Este plugin facilita la interacción entre la instancia de Airflow y los clústeres de Databricks. Puedes aprender más sobre este plugin en la [página de documentación de Airflow-Databricks](https://airflow.apache.org/docs/apache-airflow-providers-databricks/stable/index.html).\n",
    "\n",
    "3. **Crea una conexión de Databricks en Airflow**: Establece una conexión en Airflow, proporcionando los detalles necesarios como el Host de Databricks y el Token de Databricks. La [documentación oficial de Databricks](https://docs.databricks.com/dev-tools/data-pipelines.html) proporciona orientación detallada sobre cómo configurar conexiones entre Databricks y Airflow.\n",
    "\n",
    "4. **Desarrolla scripts de Python**: Desarrolla tus scripts de Python o paquetes (que pueden ser empaquetados como ruedas de Python) que pretendes ejecutar en los clústeres de Databricks.\n",
    "\n",
    "5. **Crea un DAG de Airflow**: Desarrolla un DAG de Airflow (Grafo Acíclico Dirigido) para orquestar la ejecución de tus scripts de Python en Databricks. Dentro del DAG, puedes usar `DatabricksSubmitRunOperator` para especificar los detalles del trabajo de Databricks, incluyendo las especificaciones del clúster y la ubicación del script de Python. Puedes encontrar más información sobre cómo crear DAGs en la [documentación de Airflow](https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html).\n",
    "\n",
    "6. **Ejecuta y monitorea el flujo de trabajo**: Una vez que la configuración esté completa, puedes ejecutar el flujo de trabajo desde la interfaz de usuario de Airflow y monitorear el progreso y los registros del trabajo. Consulta la [documentación de la interfaz de usuario de Airflow](https://airflow.apache.org/docs/apache-airflow/stable/ui.html) para detalles sobre cómo usar la interfaz para gestionar y monitorear flujos de trabajo.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1288/1*wJDEU3iMu9vxHweIyDczig.png\" alt=\"Alt text\" width=\"500\" height=\"300\">\n",
    "\n",
    "Al aprovechar la integración entre Airflow y Databricks, puedes automatizar la ejecución de scripts de Python en Databricks, aprovechando al máximo las capacidades computacionales de los clústeres de Databricks y las funcionalidades de automatización de Airflow para construir pipelines de datos robustos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a543f3c0-74f8-4f80-8d6d-264a07196cb6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.4 Conceptos Básicos de PySpark\n",
    "\n",
    "### 1.4.1 Entendiendo los RDDs\n",
    "\n",
    "Un RDD, o Resilient Distributed Dataset, es una estructura de datos fundamental en Spark que permite el procesamiento paralelo tolerante a fallos. Representa una colección inmutable y particionada de elementos que pueden ser procesados en paralelo a través de una red distribuida de nodos. Aquí, expandimos sus características y funcionalidades:\n",
    "\n",
    "Mira este video: [Fundamentos de RDDs](https://www.youtube.com/watch?v=nH6C9vqtyYU)\n",
    "\n",
    "1. **Inmutabilidad**: Los RDDs son inmutables, lo que significa que una vez creados, sus elementos no pueden ser alterados. Esta propiedad asegura la consistencia y fiabilidad de los datos durante los cálculos. Cuando se aplica una transformación a un RDD, resulta en un nuevo RDD, dejando el original sin cambios. (Recuerda, computamos en paralelo y distribuimos los datos en los esclavos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "416149ac-e0cb-4940-a78c-6d8538d910f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import spark as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20666532-5465-48da-a14a-eea81c99dabb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Message: 'RDD' object does not support item assignment\n"
     ]
    }
   ],
   "source": [
    "# Initializing an RDD with a list of integers\n",
    "rdd1 = sc.parallelize([1, 2, 3])\n",
    "\n",
    "# Applying a transformation to create a new RDD; rdd1 remains unchanged\n",
    "rdd2 = rdd1.map(lambda x: x * 2)\n",
    "\n",
    "# Trying to change a value in the original RDD (this will cause an error, demonstrating immutability)\n",
    "try:\n",
    "    rdd1[0] = 10\n",
    "except TypeError as e:\n",
    "    error_message = str(e)\n",
    "\n",
    "print(f\"Error Message: {error_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "330dbbdf-d1ff-4a00-b987-40d7bea91b6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original RDD: [1, 2, 3], Transformed RDD: [2, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "# Collecting the values from rdd1 to show it remains unchanged\n",
    "rdd1_collect = rdd1.collect()  # Output: [1, 2, 3]\n",
    "\n",
    "# Collecting the values from rdd2 to show it contains the transformed data\n",
    "rdd2_collect = rdd2.collect()  # Output: [2, 4, 6]\n",
    "\n",
    "# Showing both outputs to clearly illustrate the concept of immutability\n",
    "print(f\"Original RDD: {rdd1_collect}, Transformed RDD: {rdd2_collect}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a1c9481-8320-496c-b50d-233e08324c63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. **Resiliencia**: Los RDDs son resilientes, lo que significa que pueden recuperarse automáticamente de fallos. Los datos en los RDDs están distribuidos a través de múltiples nodos en un clúster, y Spark lleva un registro del linaje de cada RDD para que pueda volver a calcular los datos perdidos si es necesario. Este grafo de linaje de datos ayuda a recomputar tareas en caso de fallos de nodos, asegurando la tolerancia a fallos sin pérdida de datos.\n",
    "\n",
    "   **Ejemplo**:\n",
    "   \n",
    "   En este ejemplo, simulamos un fallo de nodo eliminando manualmente una partición del RDD. Luego realizamos una acción que obliga a Spark a recomputar los datos perdidos utilizando la información de linaje almacenada, ilustrando el concepto de resiliencia en los RDDs de Spark. Aprende más sobre la resiliencia de los RDD en la [documentación oficial](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a39ad37-2c25-412b-828f-ab9afd539049",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD Lineage Information Before Failure: b'(2) PythonRDD[5] at RDD at PythonRDD.scala:58 [Memory Serialized 1x Replicated]\\n |  ParallelCollectionRDD[4] at readRDDFromInputStream at PythonRDD.scala:435 [Memory Serialized 1x Replicated]'\n",
      "Collected Data Before Failure: [2, 4, 6, 8, 10]\n",
      "\n",
      "RDD Lineage Information After Failure and Recomputation: b'(2) PythonRDD[5] at RDD at PythonRDD.scala:58 []\\n |  ParallelCollectionRDD[4] at readRDDFromInputStream at PythonRDD.scala:435 []'\n",
      "Collected Data After Recomputation: [2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "# Creating an RDD\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5], 2)\n",
    "\n",
    "# Applying a transformation to create a new RDD\n",
    "rdd_transformed = rdd.map(lambda x: x * 2)\n",
    "\n",
    "# Cache the RDD to illustrate RDD resiliency (Spark would store the RDD across worker nodes)\n",
    "rdd_transformed.cache()\n",
    "\n",
    "# Get the debug string which contains the lineage information before simulating failure\n",
    "debug_string_before = rdd_transformed.toDebugString()\n",
    "\n",
    "# Collect data before simulating failure\n",
    "collected_data_before = rdd_transformed.collect()\n",
    "\n",
    "# Simulating a node failure by unpersisting the RDD (removing it from memory and disk)\n",
    "rdd_transformed.unpersist()\n",
    "\n",
    "# Get the debug string which contains the lineage information after simulating failure\n",
    "debug_string_after = rdd_transformed.toDebugString()\n",
    "\n",
    "# Force Spark to recompute the lost data (due to unpersist) based on the lineage information by performing an action\n",
    "collected_data_after = rdd_transformed.collect()\n",
    "\n",
    "# Printing the debug string (lineage information) and the collected data\n",
    "print(f\"RDD Lineage Information Before Failure: {debug_string_before}\")\n",
    "print(f\"Collected Data Before Failure: {collected_data_before}\")\n",
    "print(f\"\\nRDD Lineage Information After Failure and Recomputation: {debug_string_after}\")\n",
    "print(f\"Collected Data After Recomputation: {collected_data_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4efe11a-114c-48b2-91de-f380593d464e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3. **Evaluaciones Perezosas**: Los RDDs emplean evaluaciones perezosas para optimizar la eficiencia computacional. Bajo este esquema, las transformaciones no se ejecutan inmediatamente; en cambio, Spark registra las transformaciones y solo las realiza cuando se invoca una acción (como 'collect' o 'save'). Este proceso permite a Spark optimizar el plan de ejecución y realizar las optimizaciones necesarias, como el predicate pushdown. Al posponer la transformación de datos real hasta que sea necesario, se ahorran recursos computacionales considerables.\n",
    "\n",
    "   **Ejemplo**:\n",
    "   \n",
    "   En este ejemplo, demostraremos el concepto de evaluación perezosa. Crearemos un RDD y aplicaremos una serie de transformaciones. Sin embargo, observarás que estas transformaciones no se ejecutan hasta que llamemos a una acción (como `collect`). Esto puede confirmarse mirando la interfaz de usuario de Spark o examinando la visualización del DAG, donde verás que las tareas no se lanzan hasta que se llama a una acción, mostrando el aspecto de evaluación perezosa de Spark. [Lee más](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations) sobre las operaciones de RDD para profundizar tu entendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80efacbc-5c42-482d-972d-88e7fab6266e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD Transformation Lineage Before Action: b'(32) PythonRDD[13] at RDD at PythonRDD.scala:58 []\\n |   ParallelCollectionRDD[12] at readRDDFromInputStream at PythonRDD.scala:435 []'\n",
      "\n",
      "Collected Data (After Action is invoked): [6, 8, 10]\n",
      "\n",
      "RDD Transformation Lineage After Action: b'(32) PythonRDD[13] at RDD at PythonRDD.scala:58 []\\n |   ParallelCollectionRDD[12] at readRDDFromInputStream at PythonRDD.scala:435 []'\n"
     ]
    }
   ],
   "source": [
    "# Creating an RDD\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# Applying a series of transformations (map and filter)\n",
    "# At this stage, no computation happens, Spark just records these transformations (Lazy Evaluation)\n",
    "rdd_transformed = rdd.map(lambda x: x * 2)\n",
    "rdd_filtered = rdd_transformed.filter(lambda x: x > 4)\n",
    "\n",
    "# Get the debug string to illustrate the transformations recorded by Spark\n",
    "debug_string_before_action_1 = rdd_filtered.toDebugString()\n",
    "\n",
    "# Now we perform an action (collect) which triggers the actual computation\n",
    "collected_data = rdd_filtered.collect()\n",
    "\n",
    "# Get the debug string after performing the action to see the transformations applied\n",
    "debug_string_after_action_2 = rdd_filtered.toDebugString()\n",
    "\n",
    "# Printing the debug string (to show the recorded transformations) and the collected data\n",
    "print(f\"RDD Transformation Lineage Before Action: {debug_string_before_action_1}\")\n",
    "print(f\"\\nCollected Data (After Action is invoked): {collected_data}\")\n",
    "print(f\"\\nRDD Transformation Lineage After Action: {debug_string_after_action_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f20eceb2-38a9-4cf9-ba62-0ee522955d8f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "4. **Cálculos en Memoria**: Los RDDs tienen la capacidad de almacenar cálculos intermedios en memoria (RAM), lo cual puede acelerar significativamente los algoritmos iterativos y los cálculos complejos al evitar lecturas de disco después de cada operación. Sin embargo, esta característica también requiere un uso cauteloso, especialmente al tratar con grandes volúmenes de datos.\n",
    "\n",
    "   Aunque es tentador cachear datos para acelerar tus aplicaciones en Spark, hacerlo imprudentemente puede llevar a problemas. Aquí hay algunas consideraciones a tener en cuenta:\n",
    "\n",
    "   - **Consumo de Memoria**: Cachear grandes conjuntos de datos puede consumir una cantidad considerable de memoria, lo que potencialmente puede llevar a errores de OutOfMemory. Siempre monitorea el uso de memoria de tu trabajo para prevenir esto.\n",
    "   \n",
    "   - **Sobrecarga de GC**: Un caché excesivo puede causar altas sobrecargas de recolección de basura (GC), reduciendo los beneficios de rendimiento del caché. Es un equilibrio delicado entre cachear para acelerar y evitar sobrecargas de GC.\n",
    "   \n",
    "   - **Serialización de Datos**: Dependiendo del nivel de almacenamiento elegido, los datos podrían necesitar ser serializados antes de cachear, lo que puede introducir sobrecargas computacionales adicionales.\n",
    "   \n",
    "   - **Elegir el Nivel de Almacenamiento Correcto**: PySpark ofrece varios niveles de almacenamiento (como MEMORY_ONLY, MEMORY_AND_DISK, etc.) para permitirte equilibrar el uso de memoria y la eficiencia de la CPU. Elegir el nivel adecuado basado en el tamaño de tus datos y el tipo de carga de trabajo es crucial.\n",
    "   \n",
    "   **Ejemplo**:\n",
    "   \n",
    "   En esta demostración, destacaremos la característica de cálculo en memoria de PySpark que optimiza los algoritmos iterativos. Crearemos un RDD más grande y realizaremos transformaciones más complejas. Después de cachear el RDD utilizando el método `cache()` (sugiriendo a Spark que almacene el RDD en memoria tanto como sea posible), ejecutaremos una acción para activar el proceso de caché. A continuación, realizaremos otra acción para observar cómo el caché acelera el cálculo. Puedes comparar el tiempo de ejecución de cada acción para ver la diferencia en velocidad. Para profundizar tu entendimiento de la persistencia de RDD, considera leer [esta sección](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence) de la documentación oficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "630b37fd-d1e2-4954-8370-13b15bfa7084",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for the first action (without using cache): 2.81 seconds\n",
      "Time taken for the second action (using cached data): 0.29 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Creating a larger RDD\n",
    "rdd = sc.parallelize(range(1, 100000000))\n",
    "\n",
    "# Applying more complex transformations\n",
    "rdd_transformed = rdd.map(lambda x: x * 2).filter(lambda x: x % 3 == 0)\n",
    "\n",
    "# Cache the RDD\n",
    "rdd_transformed.cache()\n",
    "\n",
    "# Perform an action to populate the cache (1st action)\n",
    "start_time_cache_population = time.time()\n",
    "rdd_transformed.count()\n",
    "time_cache_population = time.time() - start_time_cache_population\n",
    "\n",
    "# Perform another action to observe the benefit of caching (2nd action)\n",
    "start_time_cache_utilization = time.time()\n",
    "rdd_transformed.count()\n",
    "time_cache_utilization = time.time() - start_time_cache_utilization\n",
    "\n",
    "# Printing the time taken for each action to show the benefit of caching\n",
    "print(f\"Time taken for the first action (without using cache): {time_cache_population:.2f} seconds\")\n",
    "print(f\"Time taken for the second action (using cached data): {time_cache_utilization:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36001249-a71b-4d75-a875-cf966ef0af76",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**5. Operaciones:**\n",
    "\n",
    "En PySpark, las operaciones que puedes realizar en los RDDs se clasifican ampliamente en dos categorías: *Transformaciones* y *Acciones*. Comprender estos dos tipos de operaciones es crucial para trabajar eficientemente con Spark, ya que dictan fundamentalmente cómo se manipulan y recuperan los datos en una aplicación Spark. A continuación, profundizaremos en cada categoría, explorando sus características y utilidad con ejemplos:\n",
    "\n",
    "**Transformaciones:**\n",
    "\n",
    "Piensa en las transformaciones como tus herramientas para esculpir datos. Las usas para dar forma, tallar y moldear tus datos en la forma deseada. Son algo así como las instrucciones en una receta de cocina, donde se te indica que piques las cebollas, marines la carne, etc., preparando el escenario para la cocina final (¡o acción en el lenguaje de PySpark!). Aquí tienes algunas transformaciones comunes y cómo las usarías:\n",
    "\n",
    "   - **map**: Aplica una función a cada elemento en el RDD, produciendo un nuevo RDD. Imagina que tienes una lista de precios para diferentes artículos, y de repente te enteras de que todos los precios deben incluir un impuesto del 10%. La transformación de map es tu herramienta ideal para ajustar todos estos precios de una vez. Te permite aplicar una función (como agregar un impuesto del 10%) a cada elemento en tu conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8196543e-1d14-4642-830a-75cb32666d6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[32]: [110.00000000000001, 220.00000000000003, 330.0, 440.00000000000006]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[32]: [110.00000000000001, 220.00000000000003, 330.0, 440.00000000000006]</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Python example for 'map'\n",
    "prices_rdd = sc.parallelize([100, 200, 300, 400])\n",
    "\n",
    "# Operation each value is multiplied by 1.1 -> 100 * 1.1, then 200 * 1.1 ...\n",
    "prices_with_tax_rdd = prices_rdd.map(lambda x: x * 1.1)\n",
    "\n",
    "# Final Output: [110.0, 220.0, 330.0, 440.0]\n",
    "prices_with_tax_rdd.collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96e33bfd-9a63-4006-a514-a5606baf4419",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "   - **filter**: Conserva los elementos que cumplen con criterios específicos, creando un RDD más pequeño. Supón que tienes una gran lista de clientes, pero solo te interesan aquellos que están ubicados en una ciudad específica. La transformación de filtro te ayuda a filtrar tu lista para conservar solo los clientes de esa ciudad, haciendo tu lista mucho más manejable y relevante para tu análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77f3249c-8f09-4699-a660-131ae8ef14d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[33]: [(&#39;Alice&#39;, &#39;NY&#39;), (&#39;Charlie&#39;, &#39;NY&#39;)]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[33]: [(&#39;Alice&#39;, &#39;NY&#39;), (&#39;Charlie&#39;, &#39;NY&#39;)]</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Python example for 'filter'\n",
    "customers_rdd = sc.parallelize([(\"Alice\", \"NY\"), (\"Bob\", \"LA\"), (\"Charlie\", \"NY\"), (\"Dave\", \"SF\")])\n",
    "\n",
    "# Filter the data by NY\n",
    "ny_customers_rdd = customers_rdd.filter(lambda x: x[1] == \"NY\")\n",
    "\n",
    "# Final Output: [('Alice', 'NY'), ('Charlie', 'NY')]\n",
    "ny_customers_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0adf068-1297-405e-aa34-e035b7c795be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "   - **flatMap**: Similar a map, pero cada elemento de entrada puede mapearse a 0 o más elementos de salida. La transformación flatMap es similar a map, pero con una pequeña variación. Puede \"aplanar\" los resultados. Entonces, si cada elemento de tu RDD es una lista de elementos, flatMap creará un nuevo RDD donde todas estas listas se fusionan en una sola lista. Puedes pensar en ello como una forma de 'desencadenar' o 'deslistar' tus listas, por así decirlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cb40095-6636-4363-bc58-b953b0378473",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: ['Hello', 'world', 'PySpark', 'is', 'fun', 'Learn', 'big', 'data']"
     ]
    }
   ],
   "source": [
    "# 3. flatMap: Python example for 'flatMap'\n",
    "sentences_rdd = sc.parallelize([\"Hello world\", \"PySpark is fun\", \"Learn big data\"])\n",
    "\n",
    "words_rdd = sentences_rdd.flatMap(lambda x: x.split(\" \"))\n",
    "\n",
    "# Output: ['Hello', 'world', 'PySpark', 'is', 'fun', 'Learn', 'big', 'data']\n",
    "words_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "259cad2b-f527-431f-95a7-03f8a564f41e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "   - **distinct**: Devuelve un nuevo RDD que contiene elementos distintos del RDD original. Si tu RDD contiene elementos duplicados y quieres deshacerte de ellos, la transformación `distinct` es tu amiga. Crea un nuevo RDD con solo elementos únicos del RDD original, eliminando esencialmente todos los duplicados. Es como una varita mágica que puede hacer desaparecer todas las entradas duplicadas, dejando solo las entradas únicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96f556b3-82cc-42b3-b3c4-72abd01a1ef6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[34]: [1, 2, 3, 4, 5]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[34]: [1, 2, 3, 4, 5]</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Python example for 'distinct'\n",
    "numbers_rdd = sc.parallelize([1, 2, 3, 3, 4, 4, 5])\n",
    "\n",
    "unique_numbers_rdd = numbers_rdd.distinct()\n",
    "\n",
    "# Output: [1, 2, 3, 4, 5]\n",
    "unique_numbers_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0459f0d7-9be0-438c-955d-45009ae5556a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "   - **Union**: La transformación de unión se utiliza para combinar dos RDDs en un único RDD. No elimina duplicados. Si deseas eliminar duplicados, puedes seguir la transformación de unión con una transformación de `distinct`. Es una manera sencilla de combinar conjuntos de datos en uno solo para análisis más complejos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ac75915-bfe4-4d9b-bc64-dddf582f309f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[1, 2, 3, 4, 5, 3, 4, 5, 6, 7]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">[1, 2, 3, 4, 5, 3, 4, 5, 6, 7]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Python example for 'Union'\n",
    "rdd1 = sc.parallelize([1, 2, 3, 4, 5])\n",
    "rdd2 = sc.parallelize([3, 4, 5, 6, 7])\n",
    "\n",
    "rdd_union = rdd1.union(rdd2)\n",
    "\n",
    "# Output: [1, 2, 3, 4, 5, 3, 4, 5, 6, 7]\n",
    "print(rdd_union.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b82feec-1dc6-4108-af66-be31f8496adf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "   - **ReduceByKey**: La transformación reduceByKey se utiliza para combinar valores con la misma clave en un RDD de pares clave-valor. Proporcionas una función que especifica cómo combinar los valores, y reduceByKey aplicará esa función a todos los valores con la misma clave. Es una manera eficiente de agregar datos en un RDD, especialmente cuando se trabaja con datos agrupados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ab22fa5-42dc-48e5-91f7-cfa61283d656",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating an RDD with pairs representing Product ID and Sales amount\n",
    "rdd = sc.parallelize([(\"Product1\", 100), (\"Product2\", 200), (\"Product1\", 150), (\"Product2\", 100), (\"Product1\", 200), (\"Product2\", 300)])\n",
    "\n",
    "# Applying the reduceByKey transformation to sum up sales amounts for each product ID\n",
    "rdd_reduce_by_key = rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Collecting and printing the results to see the total sales amount for each product ID\n",
    "# Final Output: [('Product1', 450), ('Product2', 600)]\n",
    "print(rdd_reduce_by_key.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3082296-0bf5-4c62-b55d-7d5749787fc6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Para una comprensión más profunda y para explorar más operaciones de transformación, puedes consultar la [documentación de Spark sobre transformaciones](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations).\n",
    "\n",
    "#### Acciones:\n",
    "Las acciones son operaciones que desencadenan la ejecución del grafo construido durante la fase de transformación. Esencialmente, nada se calcula en tu RDD hasta que se llama a una acción. Una vez que se llama a una acción, los datos se calculan en paralelo en diferentes nodos de tu clúster, y los resultados se devuelven al driver de Spark. Vamos a explorar algunas acciones comunes que usarías con frecuencia:\n",
    "\n",
    "   - **reduce**: Esta acción agrega todos los elementos en un RDD utilizando una función especificada que toma dos entradas y devuelve una salida única. Opera secuencialmente, aplicando la función a los primeros dos elementos, luego aplicándola nuevamente al resultado y al siguiente elemento, y así sucesivamente. Es una acción poderosa para realizar operaciones como encontrar la suma, el máximo o el mínimo de elementos en el RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "730cfa11-3e32-48d2-89c0-fbdf166d3960",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">15\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">15\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First, we initialize an RDD with a list of numbers\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# Next, we use the reduce action to find the sum of all the elements in the RDD.\n",
    "# The lambda function takes two arguments (x and y) and returns their sum.\n",
    "# This lambda function will be applied across all elements in the RDD to find the total sum.\n",
    "sum_of_elements = rdd.reduce(lambda x, y: x + y)\n",
    "\n",
    "# Let's print the result to verify\n",
    "# Final output: 15\n",
    "print(sum_of_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72e0d92c-e4cd-4063-b29e-647b223ee7c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">5\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">5\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Similarly, you can use reduce to find the maximum or minimum element in the RDD.\n",
    "# Here, we find the maximum element in the RDD.\n",
    "max_element = rdd.reduce(lambda x, y: x if x > y else y)\n",
    "\n",
    "# Printing the maximum element\n",
    "# Final Output: 5\n",
    "print(max_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68c9e630-63bb-4d47-8ccd-db7acf1b1a9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "   - **Collect**: Recupera todos los elementos del RDD al nodo controlador - esto debe usarse con cautela en conjuntos de datos grandes para evitar problemas de memoria. Esta acción recupera todos los elementos del RDD al nodo controlador (tu máquina local o el lugar donde se inicia el SparkContext). A menudo se utiliza para recuperar los resultados finales de un cálculo o para depurar durante el desarrollo. Sin embargo, es importante usar esta acción con prudencia, especialmente con grandes conjuntos de datos, ya que traer demasiados datos de una vez al controlador puede causar desbordamiento de memoria y ralentizar todo el proceso. A menudo es mejor usar acciones como take(n) o first() para recuperar un número limitado de resultados si solo estás buscando previsualizar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a52cd6c2-5efe-4856-9096-e24e0928824e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[&#39;Spark&#39;, &#39;is&#39;, &#39;powerful&#39;, &#39;big&#39;, &#39;data&#39;, &#39;processing&#39;, &#39;tool&#39;]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">[&#39;Spark&#39;, &#39;is&#39;, &#39;powerful&#39;, &#39;big&#39;, &#39;data&#39;, &#39;processing&#39;, &#39;tool&#39;]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initializing an RDD with a list of words\n",
    "rdd = sc.parallelize([\"Spark\", \"is\", \"a\", \"powerful\", \"big\", \"data\", \"processing\", \"tool\"])\n",
    "\n",
    "# Applying a transformation: we will filter the words that have more than one letter\n",
    "filtered_rdd = rdd.filter(lambda x: len(x) > 1)\n",
    "\n",
    "# Now, we will use the collect action to retrieve all the elements from the filtered RDD to the driver node\n",
    "collected_data = filtered_rdd.collect()\n",
    "\n",
    "# Let's print the collected data\n",
    "# Final output: ['Spark', 'is', 'powerful', 'big', 'data', 'processing', 'tool']\n",
    "print(collected_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3553c49-c9cb-4964-a5c2-9926f5a0ca6a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "   - **Take**: Esta acción se utiliza para recuperar los primeros 'n' elementos de un RDD, donde 'n' es un parámetro que especificas. Es una herramienta útil cuando quieres inspeccionar rápidamente algunos elementos del RDD sin recoger todos los datos (que podrían ser grandes) de vuelta al nodo controlador. Ayuda a prevenir problemas de desbordamiento de memoria que podrían ocurrir al usar collect() en un conjunto de datos grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e95bf8c-21aa-4f4d-8fe0-aa028db63d6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initializing an RDD with a range of numbers\n",
    "rdd = sc.parallelize(range(100))\n",
    "\n",
    "# Using the take action to get the first 10 elements of the RDD\n",
    "first_10_elements = rdd.take(10)\n",
    "\n",
    "# Printing the first 10 elements\n",
    "# Final output: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "print(first_10_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aedecfa3-4519-4c33-9ccf-c742932d256b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "   - **First**: La acción first(), como sugiere el nombre, se utiliza para recuperar el primer elemento de un RDD. Esta acción puede ser particularmente útil cuando estás interesado en inspeccionar rápidamente el primer registro del conjunto de datos para entender su estructura o formato sin cargar todo el conjunto de datos al nodo controlador, ahorrando así recursos computacionales y tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65246729-70c0-4826-91d8-8c648a202bbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">0\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">0\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initializing an RDD with a series of numbers\n",
    "rdd = sc.parallelize(range(100))\n",
    "\n",
    "# Using the first action to get the first element of the RDD\n",
    "first_element = rdd.first()\n",
    "\n",
    "# Printing the first element\n",
    "# Final output: 0\n",
    "print(first_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49435775-d2ec-4c90-ae87-e3c845049d8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Para explicaciones detalladas y una lista más extensa de acciones, consulta la [documentación de Spark sobre acciones](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions).\n",
    "\n",
    "**6. Particionado**\n",
    "\n",
    "En el contexto de Spark, el particionado es un medio para distribuir la carga de trabajo entre múltiples nodos en un clúster, un método que mejora el paralelismo y, por ende, la velocidad de las tareas de procesamiento de datos. Cada partición contiene una porción de los datos y opera de manera independiente, permitiendo que los cálculos se lleven a cabo simultáneamente, lo cual es una ventaja significativa especialmente cuando se trabaja con big data.\n",
    "\n",
    "**Optimización a través del Particionado**\n",
    "\n",
    "Optimizar el rendimiento de las aplicaciones de Spark a menudo implica ajustar la estrategia de particionado. Aquí hay algunos aspectos a considerar:\n",
    "\n",
    "1. **Número de Particiones:** Encontrar el número adecuado de particiones es crucial. Demasiado pocas particiones pueden no utilizar completamente los recursos disponibles, mientras que demasiadas particiones podrían aumentar la sobrecarga debido a la gestión de tareas.\n",
    "   \n",
    "2. **Sesgo de Datos:** A veces los datos pueden estar distribuidos de manera desigual entre las particiones, una situación conocida como sesgo de datos. Es esencial tener una distribución equilibrada para asegurar que todos los nodos en el clúster trabajen aproximadamente de manera igual.\n",
    "   \n",
    "3. **Ajuste para Operaciones Específicas:** Ciertas operaciones podrían beneficiarse de un tipo específico de particionado. Por ejemplo, operaciones como 'join' pueden optimizarse utilizando el particionado para minimizar el reordenamiento de datos.\n",
    "\n",
    "Vamos a profundizar en un ejemplo para entender el particionado y cómo optimizarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "852f6c11-4c6f-406e-a61f-646a1ea01a5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">The RDD is divided into 4 partitions.\n",
       "Partition: 0 | Elements: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
       "Partition: 1 | Elements: 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49\n",
       "Partition: 2 | Elements: 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74\n",
       "Partition: 3 | Elements: 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">The RDD is divided into 4 partitions.\nPartition: 0 | Elements: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\nPartition: 1 | Elements: 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49\nPartition: 2 | Elements: 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74\nPartition: 3 | Elements: 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initializing an RDD with a range of numbers and specifying the number of partitions\n",
    "rdd = sc.parallelize(range(100), 4)\n",
    "\n",
    "# Getting the number of partitions\n",
    "num_partitions = rdd.getNumPartitions()\n",
    "\n",
    "# Printing the number of partitions\n",
    "print(f\"The RDD is divided into {num_partitions} partitions.\")\n",
    "\n",
    "# Function to print the index and elements of each partition\n",
    "def show_partitions(index, iterator): yield f\"Partition: {index} | Elements: {' '.join(map(str, iterator))}\"\n",
    "\n",
    "# Using the mapPartitionsWithIndex method to apply the function to each partition\n",
    "partitions = rdd.mapPartitionsWithIndex(show_partitions).collect()\n",
    "\n",
    "# Printing the details of each partition\n",
    "for partition in partitions:\n",
    "    print(partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8143a32-49de-41b7-acfa-bb230f7d5eea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element counts in each partition: [50, 50]\n"
     ]
    }
   ],
   "source": [
    "# Apache Spark Example: Demonstrating Optimization by Reducing Data Skewness\n",
    "\n",
    "# Initialize an RDD with a skewed list of numbers\n",
    "# This creates a dataset where the number 1 appears 30 times, 2 appears 40 times, and 3 appears 30 times\n",
    "# This simulates a skewed dataset where certain values are overrepresented\n",
    "rdd_skewed = sc.parallelize([1]*30 + [2]*40 + [3]*30, 2)\n",
    "\n",
    "# Define a function to calculate the count of elements in each partition\n",
    "# The function iterates over the elements of a partition and counts them\n",
    "def count_elements(iterator): \n",
    "    yield sum(1 for _ in iterator)  # Count the number of elements in the iterator\n",
    "\n",
    "# Apply the function to each partition of the RDD using mapPartitions\n",
    "# mapPartitions applies a function to each partition of the RDD\n",
    "# The result is a new RDD where each element represents the count of items in a partition\n",
    "skewed_partitions_count = rdd_skewed.mapPartitions(count_elements).collect()\n",
    "\n",
    "# Print the count of elements in each partition\n",
    "# This shows the distribution of data across the partitions\n",
    "# In a skewed dataset, some partitions may have significantly more data than others\n",
    "print(f\"Element counts in each partition: {skewed_partitions_count}\")\n",
    "\n",
    "# The output helps in understanding the skewness in data distribution across partitions\n",
    "# Based on this information, further steps can be taken to optimize and balance the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09eb5d0f-be56-4330-9013-a05c190b105d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "7. **Persistencia**: La persistencia, o almacenamiento en caché, es una característica en Spark que permite a los usuarios controlar el nivel de almacenamiento de los RDDs, facilitando la optimización de los cálculos especialmente cuando un RDD se reutiliza varias veces dentro de una aplicación. Puedes decidir si almacenar el RDD en memoria (RAM), lo que permite tiempos de acceso más rápidos a costa de mayores requisitos de almacenamiento, o almacenarlo en disco, que es más lento pero menos intensivo en memoria.\n",
    "Al elegir sabiamente el nivel de persistencia, puedes acelerar enormemente los cálculos que acceden al RDD varias veces, ya que los datos no tienen que ser recalculados desde cero con cada acción. Es una característica útil a tener en cuenta, especialmente cuando se trabaja en algoritmos iterativos o tareas de análisis de datos interactivos.\n",
    "En la siguiente sección, veremos un ejemplo en Python que demuestra cómo establecer diferentes niveles de persistencia y cómo puede impactar en el rendimiento de tu aplicación Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3353a7b-3a62-40a3-bedf-10694530cdd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Serialized 1x Replicated\n",
       "Disk Serialized 1x Replicated\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Serialized 1x Replicated\nDisk Serialized 1x Replicated\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "# Creating an RDD\n",
    "rdd = sc.parallelize([1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "# Persisting RDD in Memory\n",
    "rdd.persist()\n",
    "\n",
    "# Performing some transformations and actions\n",
    "rdd1 = rdd.map(lambda x: x * 2)\n",
    "rdd1.collect()\n",
    "\n",
    "# Checking the persistence level\n",
    "print(rdd1.getStorageLevel())\n",
    "\n",
    "# Unpersisting the RDD from Memory\n",
    "rdd1.unpersist()\n",
    "\n",
    "# Persisting RDD on Disk\n",
    "rdd1 = rdd.map(lambda x: x * 2)\n",
    "rdd1.persist(StorageLevel.DISK_ONLY)\n",
    "rdd1.collect()\n",
    "\n",
    "# Checking the persistence level\n",
    "print(rdd1.getStorageLevel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2059eb93-47da-47e9-a708-59a827b438c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Apache Spark: `.persist()` vs `.cache()`\n",
    "\n",
    "**Método `.cache()`:**\n",
    "- `.cache()` es una abreviatura para usar `.persist()` con el nivel de almacenamiento predeterminado.\n",
    "- El nivel de almacenamiento predeterminado para `.cache()` es `MEMORY_ONLY`, lo que significa que almacena el RDD o DataFrame en memoria.\n",
    "- Si no hay suficiente memoria, algunas particiones no se almacenarán en caché y se volverán a calcular según sea necesario.\n",
    "- `.cache()` se usa comúnmente para mantener datos en memoria cuando el mismo RDD necesita ser accedido múltiples veces.\n",
    "\n",
    "**Método `.persist()`:**\n",
    "- `.persist()` ofrece más flexibilidad al permitirte especificar el nivel de almacenamiento.\n",
    "- Los niveles de almacenamiento varían e incluyen `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`, `MEMORY_AND_DISK_SER`, `DISK_ONLY`, etc.\n",
    "- Con `.persist()`, puedes controlar si Spark almacena el RDD:\n",
    "    - En memoria\n",
    "    - En disco\n",
    "    - Tanto en memoria como en disco\n",
    "    - En formato serializado o deserializado\n",
    "- `.persist()` es particularmente útil para gestionar grandes conjuntos de datos que pueden no caber completamente en memoria, o para optimizar el rendimiento eligiendo una estrategia de almacenamiento adecuada para casos de uso específicos.\n",
    "\n",
    "En resumen, mientras que `.cache()` es una forma simple y conveniente de almacenar datos en memoria, `.persist()` ofrece más control sobre el almacenamiento y la serialización de RDDs o DataFrames en Apache Spark.\n",
    "\n",
    "8. **Integración con Otros Tipos de Datos**: \n",
    "\n",
    "En PySpark, los RDDs son una parte de un ecosistema más amplio, permitiendo un manejo y análisis de datos cohesivo y flexible. Se integran sin problemas con otras estructuras de datos prominentes en Spark, a saber, DataFrames y Datasets, para facilitar flujos de trabajo de análisis de datos y aprendizaje automático más ágiles y diversificados. Así es como interactúan entre ellos:\n",
    "\n",
    " - **DataFrames**: Un DataFrame es una colección distribuida de datos organizada en columnas nombradas, similar a una tabla en una base de datos relacional. Puedes convertir fácilmente un RDD a un DataFrame y viceversa, lo que proporciona más opciones para la manipulación y análisis de datos. Permite realizar operaciones como consultas SQL, aprovechando el motor de optimización de SparkSQL. (profundizaremos en DataFrames en sesiones posteriores de la capacitación)\n",
    "\n",
    " - **Datasets**: Los Datasets son una versión segura de tipo de DataFrames, disponibles en la API de Spark para Scala y Java. Combinan los beneficios de los RDDs (seguridad de tipo, funciones del usuario) y los DataFrames (planes de ejecución optimizados). Aunque no están disponibles en PySpark, entender cómo funcionan los Datasets puede ser beneficioso cuando se trabaja con Spark en otros lenguajes de programación. Sin embargo, no trabajamos con datasets en Omnichannel.\n",
    "\n",
    "Veamos cómo convertir RDDs a DataFrames y aprovechar las funcionalidades adicionales ofrecidas por DataFrames en PySpark a través de un ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1969456-d2e8-4d8b-a604-a54c8ef0d12c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ID</th><th>Name</th></tr></thead><tbody><tr><td>1</td><td>Alice</td></tr><tr><td>2</td><td>Bob</td></tr><tr><td>3</td><td>Charlie</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Alice"
        ],
        [
         2,
         "Bob"
        ],
        [
         3,
         "Charlie"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Create an RDD\n",
    "rdd = spark.sparkContext.parallelize([(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")])\n",
    "\n",
    "# Define a schema\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Convert RDD to DataFrame using the schema\n",
    "df = spark.createDataFrame(rdd, schema=schema)\n",
    "\n",
    "# Show DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c36637f9-cdbd-43e5-bbf4-41d232d1477b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1.5 Entendiendo los DataFrames\n",
    "\n",
    "Los DataFrames son una parte vital de Spark, introducidos para superar algunas de las limitaciones asociadas con los RDDs y para proporcionar una forma más estructurada y optimizada de manejar datos. Desvelemos el concepto de DataFrames en Spark:\n",
    "\n",
    "1. **Definición y Estructura**: Un DataFrame en Spark es una colección distribuida de datos que está organizada en columnas nombradas. Es conceptualmente equivalente a una tabla en una base de datos relacional o un data frame en la biblioteca pandas de Python, pero con más optimización y funcionalidad bajo el capó.\n",
    "\n",
    "2. **Ventajas sobre los RDDs**:\n",
    "    - **Optimización**: Los DataFrames están construidos sobre los RDDs y optimizados usando Catalyst Optimizer, que genera un plan de ejecución optimizado, haciendo el procesamiento de datos más rápido y eficiente.\n",
    "    - **Facilidad de Uso**: Con su formato estructurado y la capacidad de usar consultas SQL directamente, los DataFrames son más fáciles e intuitivos de usar en comparación con los RDDs.\n",
    "    - **Integración con Varios Formatos de Datos**: Los DataFrames pueden integrarse sin problemas con varios formatos de datos (como JSON, CSV, Parquet) y bases de datos, proporcionando más flexibilidad en el manejo y análisis de datos.\n",
    "\n",
    "3. **Similitudes con SQL y Pandas**:\n",
    "    - **SQL**: Los DataFrames pueden ser consultados usando consultas SQL directamente en Spark, lo que los hace una herramienta útil para personas con experiencia en bases de datos relacionales.\n",
    "    - **Pandas**: Para aquellos familiarizados con la biblioteca Pandas de Python, la transición a usar DataFrames en Spark es relativamente sencilla debido a las similitudes en su estructura y funcionalidades.\n",
    "\n",
    "Pasemos a través de un ejemplo básico donde creamos un DataFrame y realizamos algunas operaciones, similares a las operaciones de SQL/Pandas, para darte una idea de cómo funcionan los DataFrames en Spark.\n",
    "\n",
    "Lectura Adicional:\n",
    "- [Introducción a los DataFrames - Apache Spark](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- [API de DataFrames en Python - Databricks](https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7ef9077-4b9f-4779-8f5e-5703595c42f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th></tr></thead><tbody><tr><td>Alice</td><td>25</td></tr><tr><td>Bob</td><td>30</td></tr><tr><td>Charlie</td><td>35</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Alice",
         25
        ],
        [
         "Bob",
         30
        ],
        [
         "Charlie",
         35
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating a DataFrame and performing basic operations\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Create a list of Row objects\n",
    "row_list = [Row(name=\"Alice\", age=25), Row(name=\"Bob\", age=30), Row(name=\"Charlie\", age=35)]\n",
    "\n",
    "# Create a DataFrame from the list of Row objects\n",
    "df = spark.createDataFrame(row_list)\n",
    "\n",
    "# Show the DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82740a61-f90f-455f-baf1-bd8bd578aea6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Ejercicio 1: Análisis de Datos de Ventas\n",
    "\n",
    "#### Contexto\n",
    "\n",
    "Se te ha proporcionado un conjunto de datos que contiene información de ventas de diferentes regiones. Tu tarea es analizar estos datos utilizando PySpark para obtener información sobre el rendimiento de ventas de diferentes productos y regiones.\n",
    "\n",
    "#### Tarea 1: Preparación de Datos\n",
    "- Carga los datos proporcionados en un RDD.\n",
    "- Inspecciona las primeras entradas del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d2ca11b-d181-477e-871e-59dfa6385ab7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data to use\n",
    "data = [\n",
    "    ('North', 'Product1', 100),\n",
    "    ('South', 'Product1', 200),\n",
    "    ('East', 'Product1', 300),\n",
    "    ('West', 'Product1', 400),\n",
    "    ('North', 'Product2', 150),\n",
    "    ('South', 'Product2', 250),\n",
    "    ('East', 'Product2', 350),\n",
    "    ('West', 'Product2', 450),\n",
    "    ('North', 'Product3', 200),\n",
    "    ('South', 'Product3', 300),\n",
    "    ('East', 'Product3', 400),\n",
    "    ('West', 'Product3', 500),\n",
    "    ('North', 'Product4', 250),\n",
    "    ('South', 'Product4', 350),\n",
    "    ('East', 'Product4', 450),\n",
    "    ('West', 'Product4', 550),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "937a51cc-5200-4833-a0a6-ef2e7caadc80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here your code\n",
    "# (add appropriate PySpark operations to achieve each of the subtasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "069487ae-f4c6-4fad-a360-404bf31c7942",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Tarea 2: Transformación y Análisis de Datos\n",
    "- Calcula las ventas totales por región.\n",
    "- Calcula las ventas totales por producto.\n",
    "- Encuentra la región con las ventas más altas.\n",
    "- Encuentra el producto con las ventas más altas en cada región."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb55616c-a127-46da-9683-9a61f8d258b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Here your code\n",
    "# (add appropriate PySpark operations to achieve each of the subtasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36d859e9-ac63-4276-96b5-6b8631f89bae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Tarea 3: Optimización y Persistencia de Datos\n",
    "- Optimiza la partición de datos para mejorar el rendimiento de tu análisis.\n",
    "- Persiste los RDDs intermedios que se reutilizan varias veces en la aplicación para optimizar el tiempo de cálculo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e01c178b-c38d-4c82-ac80-5177d737cd55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Here your code\n",
    "# (add appropriate PySpark operations to achieve each of the subtasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d25e7441-23cf-47c5-be46-34991a190232",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Tarea 4: Integración con Otros Tipos de Datos\n",
    "- Convierte el RDD a un DataFrame y realiza una consulta SQL simple para encontrar las ventas totales por región."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdf850fb-ce42-4033-a6e1-8e9fd7d9da1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Here your code\n",
    "# (add appropriate PySpark operations to achieve each of the subtasks)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) 1. Introduction to Big Data & PySpark",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
