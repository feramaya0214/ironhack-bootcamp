{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0df9d3d-63be-4d3b-bf13-d38fbf754b50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Introduction to Big Data & PySpark\n",
    "\n",
    "## 1.1 Background of Big Data\n",
    "\n",
    "In recent years, the term \"Big Data\" has evolved to represent a pivotal concept in the realm of technology and business. Essentially, big data refers to the enormous volume of structured and unstructured data generated every second in today's digitized world. This section will delve deeper into what Big Data entails and why it's crucial in the contemporary setting, particularly highlighting its relationship with Apache Spark.\n",
    "\n",
    "### 1.1.1 Understanding the Three V's of Big Data\n",
    "\n",
    "Before we venture deeper into the realm of big data, it's essential to get acquainted with the core concepts that form its foundation - the Three V's: Volume, Velocity, and Variety. These terms help us grasp the magnitude and complexity of big data, giving us the tools to navigate and manage this vast digital ocean more effectively. Let's take a moment to understand each of these aspects in bullet points:\n",
    "\n",
    "- **Volume**: In the world of big data, 'Volume' refers to the immense amount of data that is generated every moment of every day. It's not just about the information stored in databases; it includes data from social media, websites, smartphones, and many other sources. To give you an idea, it's like trying to fill a bucket with a never-ending stream of water, where the bucket represents our storage capacity, and the water represents the data.\n",
    "\n",
    "- **Velocity**: 'Velocity' points to the breathtaking speed at which this data is generated and collected. It's not a calm river, but a torrent of information flowing in every second from various sources. In this fast-paced environment, being able to process and analyze data quickly is crucial to keep up with the ever-changing landscape and to make timely decisions.\n",
    "\n",
    "- **Variety**: Last but not least, 'Variety' emphasizes the different types of data we encounter in the big data universe. Data can be structured, like the neat rows and columns in a spreadsheet, or unstructured, like the content of an email or a social media post. Being able to handle this diverse range of data, understanding, and extracting valuable insights from it is a skill that is highly sought after in the big data world.\n",
    "his diverse range of data, understanding, and extracting valuable insights from it is a skill that is highly sought after in the big data world.\n",
    "\n",
    "<img src=\"https://cdn.ttgtmedia.com/rms/onlineimages/3_vs_of_big_data-f.png\" alt=\"Alt text\" width=\"700\" height=\"400\">\n",
    "\n",
    "### 1.1.2 Big Data Technologies\n",
    "\n",
    "The advent of big data technologies has revolutionized the way we handle enormous volumes of data, transforming daunting data management tasks into manageable and efficient processes. Key technologies in this landscape include Hadoop, Spark, and others, each playing a vital role in the storage, processing, and analysis of big data.\n",
    "\n",
    "#### Hadoop: The Foundation of Big Data Processing\n",
    "Hadoop has been a foundational framework in big data processing, enabling distributed storage and processing of large datasets across computer clusters. Its core components include:\n",
    "- **HDFS (Hadoop Distributed File System)**: Splits files into large blocks and distributes them across nodes in a cluster, ensuring high data availability and fault tolerance. [Learn more about HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html).\n",
    "- **MapReduce**: A programming model that processes large datasets in parallel across a Hadoop cluster. [Learn more about MapReduce](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html).\n",
    "- **YARN (Yet Another Resource Negotiator)**: Manages resources and schedules applications in clusters. [Learn more about YARN](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html).\n",
    "- **Hadoop Common**: Provides common utilities and libraries supporting other Hadoop modules. [Learn more about Hadoop Common](https://hadoop.apache.org/docs/current/).\n",
    "- **HBase**: A NoSQL database running on top of HDFS, offering real-time read/write access. [Learn more about HBase](https://hbase.apache.org/).\n",
    "\n",
    "Check: [hadoop video](https://www.youtube.com/watch?v=aReuLtY0YMI)\n",
    "\n",
    "#### Spark: Advancing Big Data Processing\n",
    "Following Hadoop's lead, Spark introduced significant advancements, particularly in in-memory computing, enhancing data processing speeds. Its key features include:\n",
    "- **In-Memory Computing**: Stores data in memory, reducing processing time compared to Hadoop's disk-based approach. [Learn more about In-Memory Computing](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence).\n",
    "- **RDD (Resilient Distributed Datasets)**: Fault-tolerant collections of elements processed in parallel. [Learn more about RDD](https://spark.apache.org/docs/latest/rdd-programming-guide.html).\n",
    "- **DataFrame and Dataset**: APIs for structured data operations. [Learn more about DataFrame and Dataset](https://spark.apache.org/docs/latest/sql-programming-guide.html).\n",
    "- **MLlib**: A machine learning library for scalable data science. [Learn more about MLlib](https://spark.apache.org/docs/latest/ml-guide.html).\n",
    "- **GraphX**: Enables graph data processing. [Learn more about GraphX](https://spark.apache.org/docs/latest/graphx-programming-guide.html).\n",
    "- **Spark Streaming**: Facilitates fault-tolerant stream processing of live data. [Learn more about Spark Streaming](https://spark.apache.org/docs/latest/streaming-programming-guide.html).\n",
    "\n",
    "Check: [Spark video](https://www.youtube.com/watch?v=VZ7EHLdrVo0)\n",
    "\n",
    "#### Expanding the Big Data Ecosystem\n",
    "Beyond Hadoop and Spark, the big data ecosystem encompasses other key technologies:\n",
    "- **Apache Kafka**: A platform for handling real-time data feeds. Essential for high-throughput, fault-tolerant streaming.\n",
    "- **Apache Flink**: Known for its stream processing capabilities and real-time data analysis.\n",
    "- **NoSQL Databases**: Like Cassandra and MongoDB, these databases support large-scale, distributed data storage and management.\n",
    "\n",
    "#### Practical Insights and Future Trends\n",
    "- **Use Cases**: Hadoop excels in batch processing, while Spark is preferred for real-time analytics and iterative algorithms.\n",
    "- **Ecosystem Tools**: Hive and Pig enhance Hadoop and Spark's capabilities by providing SQL-like querying and data flow scripting, respectively.\n",
    "- **Trends and Challenges**: The shift towards cloud-based solutions, integration of machine learning, and addressing challenges like data security, project complexity, and learning curves are shaping the future of big data.\n",
    "\n",
    "#### Enhancing Learning with Practical Applications\n",
    "Hands-on examples, case studies, and sample datasets encourage practical understanding and link theory with real-world applications.\n",
    "\n",
    "By harnessing these technologies, organizations can now efficiently store, process, and analyze large datasets, leading to quicker data processing, deep insights, and informed decision-making, thus fostering innovation in various fields.\n",
    "\n",
    "\n",
    "## 1.2 Introduction to Spark\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg\" alt=\"Alt text\" width=\"500\" height=\"300\">\n",
    "\n",
    "In the dynamic domain of big data, [Apache Spark](https://spark.apache.org/) has emerged as a preeminent computing engine that stands at the forefront of big data processing and analytics. Designed to be both speedy and general-purpose, it facilitates the seamless extraction of insights from substantial datasets, playing a pivotal role in the modern data-driven decision-making process. Leveraging platforms like [Databricks](https://databricks.com/), amplifies its capabilities, offering a collaborative and interactive environment that integrates effortlessly with Spark.\n",
    "\n",
    "### 1.2.1 Overview of Spark\n",
    "\n",
    "[Apache Spark](https://spark.apache.org/docs/latest/), renowned for its unified computing engine, has brought a paradigm shift in big data processing and analytics. Superseding the capabilities of older big data technologies, it shines when it comes to handling large datasets, offering a scalable, fault-tolerant, and adaptive framework. Its in-memory computing prowess enables lightning-fast data processing, making it an invaluable tool in the toolkit of data analysts and scientists aiming to glean actionable insights from big data. Its compatibility with various data sources and seamless operation both on-premise and in the cloud position it as a versatile solution to the intricate challenges posed by big data.\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/cluster-overview.png\" alt=\"Alt text\" width=\"500\" height=\"300\">\n",
    "\n",
    "In the Spark ecosystem, the operational workflow is coordinated by three primary components: the Driver Program, the Cluster Manager, and the Worker Nodes. \n",
    "\n",
    "- **Driver Program**: The central component that governs the overall execution of the Spark application. It translates the tasks of the user program into units of work that can be distributed over the worker nodes. The driver program also collates the results from the worker nodes and delivers the final outcome.\n",
    "\n",
    "- **Cluster Manager**: This external entity oversees resource allocation within the Spark cluster, essentially managing the distribution of tasks. The cluster manager could be standalone or integrated with other cluster management platforms like Mesos or YARN, ensuring that resources are utilized optimally and tasks are allocated appropriately to foster speedy execution.\n",
    "\n",
    "- **Worker Nodes**: These are the actual executors of the tasks assigned by the driver program. Each worker node maintains an executor process which is responsible for running the individual tasks. After executing the tasks, the nodes return the results to the driver program. Their role is crucial in ensuring parallel processing, thus significantly speeding up data processing and analysis.\n",
    "\n",
    "This trinity forms the backbone of a Spark application, ensuring fluidity and efficiency in big data processing and analytics.\n",
    "\n",
    "#### 1.2.2 Integration with Databricks\n",
    "\n",
    "[Databricks](https://databricks.com/product/unified-data-analytics-platform), founded by the original creators of Apache Spark, serves as a unified data analytics platform that enhances the capabilities of Spark by providing a cloud-based environment that fosters collaboration and innovation. Databricks facilitates the smooth integration of data science, data engineering, and data analytics on a single platform, enabling organizations to accelerate innovation and improve efficiency. Its interactive workspace empowers teams to collaborate and share insights, fostering a culture of data-driven decision-making. Moreover, its native integration with Spark ensures you can harness the full power of Spark with enhanced security, streamlined workflows, and advanced analytics, making it a cornerstone in the big data ecosystem.\n",
    "\n",
    "### 1.2.3 Features and Benefits of Using PySpark (SCRM Choice)\n",
    "\n",
    "[PySpark](https://spark.apache.org/docs/latest/api/python/) is the Python API for Apache Spark, blending the data processing muscle of Spark with the versatility and ease of use of Python. Here are the standout features and benefits of using PySpark:\n",
    "\n",
    "1. **Speed**: PySpark takes advantage of Spark's in-memory computing, allowing it to run workloads up to 100 times faster than traditional big data processing tools. It leverages advanced optimization techniques to offer blazing fast analytics.\n",
    "   \n",
    "2. **Ease of Use**: Equipped with over 100 high-level operators, PySpark facilitates the easy construction of parallel applications, reducing the time and effort needed to develop data processing pipelines. Its integration with Python, a language known for its simplicity, further adds to the ease of use.\n",
    "   \n",
    "3. **Generality**: PySpark offers a unified solution that seamlessly combines SQL queries, streaming analytics, and complex analytics under a single platform. This generality means that data professionals can use a single tool for a diverse range of data tasks, enhancing efficiency and productivity.\n",
    "   \n",
    "4. **Runs Everywhere**: With its flexible architecture, Spark can operate in various environments including Hadoop, Apache Mesos, Kubernetes, standalone clusters, or in the cloud, ensuring that you can use it in the way that best suits your organization's needs.\n",
    "\n",
    "### 1.2.4 PySpark vs. Other Big Data Tools\n",
    "\n",
    "PySpark has carved out a distinct place for itself in the big data toolkit, offering capabilities not found or limited in other big data tools. Here’s how it stands apart:\n",
    "\n",
    "- **Batch Processing and Real-Time Data Streaming**: While other tools may excel in batch processing or streaming, PySpark proficiently handles both, allowing for the processing of batch data and real-time data streams within the same framework.\n",
    "   \n",
    "- **Machine Learning and Graph Processing**: PySpark goes beyond just data processing to offer a rich set of libraries for machine learning and graph processing. This means that you can build predictive models and analyze complex networks directly within your PySpark environment, without the need for additional tools.\n",
    "   \n",
    "- **Integration with Python**: By leveraging the Python programming language, PySpark opens up a rich ecosystem of libraries and tools that can be used in tandem with Spark's data processing capabilities, offering a holistic solution for data analytics.\n",
    "\n",
    "Through its combination of speed, ease of use, and broad capabilities, PySpark stands as a versatile and powerful tool in the world of big data, empowering professionals to derive deeper insights and add more value to their data analytics efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4e65cd2-4851-4c4b-89fc-3be181a6cab4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[1]: </div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[1]: </div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=4834245828650669#setting/sparkui/0614-152608-357o4auh/driver-8304742766061018811\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.38.88.11:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=4834245828650669#setting/sparkui/0614-152608-357o4auh/driver-8304742766061018811\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://10.38.88.11:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initializing a SparkSession in Databricks\n",
    "\n",
    "# In Spark, the SparkSession is the entry point to any spark functionality. When you are working with the DataFrame and Dataset API, SparkSession is the reference you use to start any kind of data transformation or action. \n",
    "\n",
    "# In Databricks, you don't have to create a SparkSession manually, as it is automatically initialized and readily available via the `spark` variable. This variable contains the SparkSession and can be used to access a plethora of functionalities offered by PySpark.\n",
    "\n",
    "# The following command will help you retrieve details about the automatically created SparkSession in Databricks. It includes information like the application name, Spark master URL, and the Spark version being used.\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "297bd594-053b-4495-b17e-73b8cbc08780",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.3 Setting Up PySpark\n",
    "\n",
    "Before diving into PySpark functionalities, it's essential to set up your PySpark environment appropriately. In Databricks, this setup process is streamlined to facilitate ease of use and rapid development cycles. Here’s a detailed guide to help you set up PySpark on Databricks:\n",
    "\n",
    "### 1.3.1 Installing and Configuring PySpark\n",
    "\n",
    "In Databricks, the setup process for PySpark is virtually non-existent, saving you from the hassles of installations and configurations. PySpark comes pre-installed and configured, enabling you to kickstart your data analytics projects without any delays. This is particularly beneficial for newcomers who can directly immerse themselves in learning and utilizing PySpark functionalities without worrying about the setup intricacies.\n",
    "\n",
    "### 1.3.2 Setting Up a Development Environment\n",
    "\n",
    "Databricks shines as a collaborative workspace, offering an interactive platform where you can craft PySpark scripts with ease. The workspace facilitates not only script creation but also the development of vivid visualizations, enhancing data representation and insights. Additionally, you can share your work seamlessly with others, fostering collaboration and learning. This interactive and collaborative workspace proves to be a cornerstone for teams aiming to develop data-driven solutions efficiently.\n",
    "\n",
    "### 1.3.3 (Advanced) Integrating with Airflow to Run Python Scripts on Databricks\n",
    "\n",
    "(This is a introduction, we won't go deeper) Integration with Apache Airflow offers an efficient way to orchestrate and automate the execution of Python scripts on Databricks clusters. This setup permits you to schedule and monitor workflows and seamlessly integrate Databricks into your data pipeline. Here is a step-by-step guide on how to set up this integration:\n",
    "\n",
    "1. **Setup Airflow**: Ensure that you have Apache Airflow installed and configured in your working environment (there is a common Airflow for Omnichannel, there is no need to set it up). You can find detailed installation instructions in the [official Airflow documentation](https://airflow.apache.org/docs/apache-airflow/stable/installation.html).\n",
    "\n",
    "2. **Databricks Plugin**: Install the Databricks plugin for Airflow. This plugin facilitates the interaction between the Airflow instance and the Databricks clusters. You can learn more about this plugin on the [Airflow-Databricks documentation page](https://airflow.apache.org/docs/apache-airflow-providers-databricks/stable/index.html).\n",
    "\n",
    "3. **Create a Databricks Connection in Airflow**: Establish a connection in Airflow, providing the necessary details such as Databricks Host and Databricks Token. The official [Databricks documentation](https://docs.databricks.com/dev-tools/data-pipelines.html) provides detailed guidance on setting up connections between Databricks and Airflow.\n",
    "\n",
    "4. **Develop Python Scripts**: Develop your Python scripts or packages (which can be packaged as Python wheels) that you intend to run on the Databricks clusters.\n",
    "\n",
    "5. **Create an Airflow DAG**: Develop an Airflow DAG (Directed Acyclic Graph) to orchestrate the execution of your Python scripts on Databricks. Within the DAG, you can use `DatabricksSubmitRunOperator` to specify the details of the Databricks job, including the cluster specifications and the location of the Python script. You can find more information on creating DAGs in the [Airflow documentation](https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html).\n",
    "\n",
    "6. **Run and Monitor the Workflow**: Once the setup is complete, you can run the workflow from the Airflow UI and monitor the job's progress and logs. Refer to the [Airflow UI documentation](https://airflow.apache.org/docs/apache-airflow/stable/ui.html) for details on how to use the UI to manage and monitor workflows.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1288/1*wJDEU3iMu9vxHweIyDczig.png\" alt=\"Alt text\" width=\"500\" height=\"300\">\n",
    "\n",
    "By leveraging the integration between Airflow and Databricks, you can automate the execution of Python scripts on Databricks, taking full advantage of the computational capabilities of Databricks clusters and the automation functionalities of Airflow to build robust data pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a543f3c0-74f8-4f80-8d6d-264a07196cb6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.4 PySpark Basics\n",
    "\n",
    "### 1.4.1 Understanding RDDs\n",
    "\n",
    "An RDD, or Resilient Distributed Dataset, is a fundamental data structure in Spark that allows for fault-tolerant parallel processing. It represents an immutable, partitioned collection of elements that can be processed in parallel across a distributed network of nodes. Here, we expand on its characteristics and functionalities:\n",
    "\n",
    "Check this video: [RDDs fundamentals](https://www.youtube.com/watch?v=nH6C9vqtyYU)\n",
    "\n",
    "1. **Immutability**: RDDs are immutable, meaning once created, its elements cannot be altered. This property ensures data consistency and reliability during computations. When a transformation is applied to an RDD, it results in a new RDD, leaving the original unchanged. (Remember, we compute in parallel an distribute the data on the slaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "416149ac-e0cb-4940-a78c-6d8538d910f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import spark as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20666532-5465-48da-a14a-eea81c99dabb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Message: 'RDD' object does not support item assignment\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initializing an RDD with a list of integers\n",
    "rdd1 = sc.parallelize([1, 2, 3])\n",
    "\n",
    "# Applying a transformation to create a new RDD; rdd1 remains unchanged\n",
    "rdd2 = rdd1.map(lambda x: x * 2)\n",
    "\n",
    "# Trying to change a value in the original RDD (this will cause an error, demonstrating immutability)\n",
    "try:\n",
    "    rdd1[0] = 10\n",
    "except TypeError as e:\n",
    "    error_message = str(e)\n",
    "\n",
    "print(f\"Error Message: {error_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "330dbbdf-d1ff-4a00-b987-40d7bea91b6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original RDD: [1, 2, 3], Transformed RDD: [2, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "# Collecting the values from rdd1 to show it remains unchanged\n",
    "rdd1_collect = rdd1.collect()  # Output: [1, 2, 3]\n",
    "\n",
    "# Collecting the values from rdd2 to show it contains the transformed data\n",
    "rdd2_collect = rdd2.collect()  # Output: [2, 4, 6]\n",
    "\n",
    "# Showing both outputs to clearly illustrate the concept of immutability\n",
    "print(f\"Original RDD: {rdd1_collect}, Transformed RDD: {rdd2_collect}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a1c9481-8320-496c-b50d-233e08324c63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. **Resiliency**: RDDs are resilient, which means they can automatically recover from failures. The data in RDDs is distributed across multiple nodes in a cluster, and Spark keeps track of the lineage of each RDD so that it can recompute lost data if necessary. This data lineage graph aids in recomputing tasks in case of node failures, ensuring fault tolerance without data loss.\n",
    "\n",
    "   **Example**:\n",
    "   \n",
    "   In this example, we simulate a node failure by manually deleting a partition of the RDD. We then perform an action that forces Spark to recompute the lost data using the stored lineage information, illustrating the concept of resiliency in Spark RDDs. Learn more about RDD resilience in the [official documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a39ad37-2c25-412b-828f-ab9afd539049",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD Lineage Information Before Failure: b'(2) PythonRDD[5] at RDD at PythonRDD.scala:58 [Memory Serialized 1x Replicated]\\n |  ParallelCollectionRDD[4] at readRDDFromInputStream at PythonRDD.scala:435 [Memory Serialized 1x Replicated]'\nCollected Data Before Failure: [2, 4, 6, 8, 10]\n\nRDD Lineage Information After Failure and Recomputation: b'(2) PythonRDD[5] at RDD at PythonRDD.scala:58 []\\n |  ParallelCollectionRDD[4] at readRDDFromInputStream at PythonRDD.scala:435 []'\nCollected Data After Recomputation: [2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "# Creating an RDD\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5], 2)\n",
    "\n",
    "# Applying a transformation to create a new RDD\n",
    "rdd_transformed = rdd.map(lambda x: x * 2)\n",
    "\n",
    "# Cache the RDD to illustrate RDD resiliency (Spark would store the RDD across worker nodes)\n",
    "rdd_transformed.cache()\n",
    "\n",
    "# Get the debug string which contains the lineage information before simulating failure\n",
    "debug_string_before = rdd_transformed.toDebugString()\n",
    "\n",
    "# Collect data before simulating failure\n",
    "collected_data_before = rdd_transformed.collect()\n",
    "\n",
    "# Simulating a node failure by unpersisting the RDD (removing it from memory and disk)\n",
    "rdd_transformed.unpersist()\n",
    "\n",
    "# Get the debug string which contains the lineage information after simulating failure\n",
    "debug_string_after = rdd_transformed.toDebugString()\n",
    "\n",
    "# Force Spark to recompute the lost data (due to unpersist) based on the lineage information by performing an action\n",
    "collected_data_after = rdd_transformed.collect()\n",
    "\n",
    "# Printing the debug string (lineage information) and the collected data\n",
    "print(f\"RDD Lineage Information Before Failure: {debug_string_before}\")\n",
    "print(f\"Collected Data Before Failure: {collected_data_before}\")\n",
    "print(f\"\\nRDD Lineage Information After Failure and Recomputation: {debug_string_after}\")\n",
    "print(f\"Collected Data After Recomputation: {collected_data_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4efe11a-114c-48b2-91de-f380593d464e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3. **Lazy Evaluations**: RDDs employ lazy evaluations to optimize computational efficiency. Under this scheme, transformations are not immediately executed; instead, Spark records the transformations and only performs them when an action (like 'collect' or 'save') is invoked. This process allows Spark to optimize the execution plan and perform necessary optimizations, like predicate pushdown. By postponing the actual data transformation until necessary, it saves considerable computational resources.\n",
    "\n",
    "   **Example**:\n",
    "   \n",
    "   In this example, we will demonstrate the concept of lazy evaluation. We will create an RDD and apply a series of transformations. However, you will observe that these transformations are not executed until we call an action (like `collect`). This can be confirmed by looking at the Spark UI or examining the DAG visualization, where you'll see that tasks are not launched until an action is called, showcasing the lazy evaluation aspect of Spark. [Read more](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations) about RDD operations to deepen your understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80efacbc-5c42-482d-972d-88e7fab6266e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD Transformation Lineage Before Action: b'(32) PythonRDD[13] at RDD at PythonRDD.scala:58 []\\n |   ParallelCollectionRDD[12] at readRDDFromInputStream at PythonRDD.scala:435 []'\n\nCollected Data (After Action is invoked): [6, 8, 10]\n\nRDD Transformation Lineage After Action: b'(32) PythonRDD[13] at RDD at PythonRDD.scala:58 []\\n |   ParallelCollectionRDD[12] at readRDDFromInputStream at PythonRDD.scala:435 []'\n"
     ]
    }
   ],
   "source": [
    "# Creating an RDD\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# Applying a series of transformations (map and filter)\n",
    "# At this stage, no computation happens, Spark just records these transformations (Lazy Evaluation)\n",
    "rdd_transformed = rdd.map(lambda x: x * 2)\n",
    "rdd_filtered = rdd_transformed.filter(lambda x: x > 4)\n",
    "\n",
    "# Get the debug string to illustrate the transformations recorded by Spark\n",
    "debug_string_before_action_1 = rdd_filtered.toDebugString()\n",
    "\n",
    "# Now we perform an action (collect) which triggers the actual computation\n",
    "collected_data = rdd_filtered.collect()\n",
    "\n",
    "# Get the debug string after performing the action to see the transformations applied\n",
    "debug_string_after_action_2 = rdd_filtered.toDebugString()\n",
    "\n",
    "# Printing the debug string (to show the recorded transformations) and the collected data\n",
    "print(f\"RDD Transformation Lineage Before Action: {debug_string_before_action_1}\")\n",
    "print(f\"\\nCollected Data (After Action is invoked): {collected_data}\")\n",
    "print(f\"\\nRDD Transformation Lineage After Action: {debug_string_after_action_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f20eceb2-38a9-4cf9-ba62-0ee522955d8f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "4. **In-Memory Computations**: RDDs have the ability to store intermediate computations in memory (RAM), which can significantly speed up iterative algorithms and complex computations by avoiding disk reads after each operation. However, this feature also necessitates cautious usage, especially when dealing with large data sizes. \n",
    "\n",
    "   While it's tempting to cache data to speed up your Spark applications, doing so imprudently can lead to issues. Here are a few considerations to keep in mind:\n",
    "\n",
    "   - **Memory Consumption**: Caching large datasets can consume a considerable amount of memory, potentially leading to OutOfMemory errors. Always monitor your job's memory usage to prevent this.\n",
    "   \n",
    "   - **GC Overhead**: Excessive caching can cause high garbage collection (GC) overheads, reducing the performance benefits of caching. It's a delicate balance between caching for speedup and avoiding GC overheads.\n",
    "   \n",
    "   - **Data Serialization**: Depending on the storage level chosen, the data might need to be serialized before caching, which can introduce additional computational overheads.\n",
    "   \n",
    "   - **Choosing the Right Storage Level**: PySpark offers several storage levels (like MEMORY_ONLY, MEMORY_AND_DISK, etc.) to let you balance between memory usage and CPU efficiency. Choosing the right level based on your data size and workload type is crucial.\n",
    "   \n",
    "   **Example**:\n",
    "   \n",
    "   In this demonstration, we will highlight PySpark's in-memory computation feature which optimizes iterative algorithms. We will create a larger RDD and perform more complex transformations. After caching the RDD using the `cache()` method (suggesting Spark to store the RDD in memory as much as possible), we will execute an action to trigger the caching process. Next, we will perform another action to observe how caching speeds up the computation. You can compare the runtime for each action to see the difference in speed. To deepen your understanding of RDD persistence, consider reading [this section](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence) of the official documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "630b37fd-d1e2-4954-8370-13b15bfa7084",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for the first action (without using cache): 2.81 seconds\nTime taken for the second action (using cached data): 0.29 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Creating a larger RDD\n",
    "rdd = sc.parallelize(range(1, 100000000))\n",
    "\n",
    "# Applying more complex transformations\n",
    "rdd_transformed = rdd.map(lambda x: x * 2).filter(lambda x: x % 3 == 0)\n",
    "\n",
    "# Cache the RDD\n",
    "rdd_transformed.cache()\n",
    "\n",
    "# Perform an action to populate the cache (1st action)\n",
    "start_time_cache_population = time.time()\n",
    "rdd_transformed.count()\n",
    "time_cache_population = time.time() - start_time_cache_population\n",
    "\n",
    "# Perform another action to observe the benefit of caching (2nd action)\n",
    "start_time_cache_utilization = time.time()\n",
    "rdd_transformed.count()\n",
    "time_cache_utilization = time.time() - start_time_cache_utilization\n",
    "\n",
    "# Printing the time taken for each action to show the benefit of caching\n",
    "print(f\"Time taken for the first action (without using cache): {time_cache_population:.2f} seconds\")\n",
    "print(f\"Time taken for the second action (using cached data): {time_cache_utilization:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "297b6631-b4d4-403b-b715-db5d0af3af07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**5. Operations:**\n",
    "\n",
    "In PySpark, the operations you can perform on RDDs are broadly classified into two categories: *Transformations* and *Actions*. Understanding these two types of operations is crucial in working efficiently with Spark as they fundamentally dictate how data is manipulated and retrieved in a Spark application. Below we will delve deeper into each category, exploring their characteristics and utility with examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36001249-a71b-4d75-a875-cf966ef0af76",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Transformations:**\n",
    "\n",
    "Think of transformations as your data sculpting tools. You use them to shape, carve, and mold your data into the desired form. They're kind of like the instructions on a cooking recipe, where you're told to chop the onions, marinate the meat, etc., setting the stage for the final cooking (or action in PySpark's language!). Here are some common transformations and how you'd use them:\n",
    "\n",
    "   - **map**: Applies a function to each item in the RDD, yielding a new RDD. Imagine you have a list of prices for different items, and you suddenly found out that all prices are supposed to have a 10% tax included. The map transformation is your go-to tool to adjust all these prices in one go. It allows you to apply a function (like adding a 10% tax) to each item in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8196543e-1d14-4642-830a-75cb32666d6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[32]: [110.00000000000001, 220.00000000000003, 330.0, 440.00000000000006]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[32]: [110.00000000000001, 220.00000000000003, 330.0, 440.00000000000006]</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Python example for 'map'\n",
    "prices_rdd = sc.parallelize([100, 200, 300, 400])\n",
    "\n",
    "# Operation each value is multiplied by 1.1 -> 100 * 1.1, then 200 * 1.1 ...\n",
    "prices_with_tax_rdd = prices_rdd.map(lambda x: x * 1.1)\n",
    "\n",
    "# Final Output: [110.0, 220.0, 330.0, 440.0]\n",
    "prices_with_tax_rdd.collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96e33bfd-9a63-4006-a514-a5606baf4419",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "   - **filter**: Retains elements that meet specific criteria, creating a smaller RDD.  Suppose you have a big list of customers, but you're only interested in those who are located in a specific city. The filter transformation helps you sift through your list to retain only the customers from that city, making your list much more manageable and relevant to your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77f3249c-8f09-4699-a660-131ae8ef14d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[33]: [(&#39;Alice&#39;, &#39;NY&#39;), (&#39;Charlie&#39;, &#39;NY&#39;)]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[33]: [(&#39;Alice&#39;, &#39;NY&#39;), (&#39;Charlie&#39;, &#39;NY&#39;)]</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Python example for 'filter'\n",
    "customers_rdd = sc.parallelize([(\"Alice\", \"NY\"), (\"Bob\", \"LA\"), (\"Charlie\", \"NY\"), (\"Dave\", \"SF\")])\n",
    "\n",
    "# Filter the data by NY\n",
    "ny_customers_rdd = customers_rdd.filter(lambda x: x[1] == \"NY\")\n",
    "\n",
    "# Final Output: [('Alice', 'NY'), ('Charlie', 'NY')]\n",
    "ny_customers_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0adf068-1297-405e-aa34-e035b7c795be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "   - **flatMap**: Similar to map, but each input item can be mapped to 0 or more output items.  The flatMap transformation is similar to map, but with a slight twist. It can \"flatten\" the results. So, if each element of your RDD is a list of elements, flatMap will make a new RDD where all these lists are merged into a single list. You can think of it as a way to 'unchain' or 'unlist' your lists, so to speak.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cb40095-6636-4363-bc58-b953b0378473",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: ['Hello', 'world', 'PySpark', 'is', 'fun', 'Learn', 'big', 'data']"
     ]
    }
   ],
   "source": [
    "# 3. flatMap: Python example for 'flatMap'\n",
    "sentences_rdd = sc.parallelize([\"Hello world\", \"PySpark is fun\", \"Learn big data\"])\n",
    "\n",
    "words_rdd = sentences_rdd.flatMap(lambda x: x.split(\" \"))\n",
    "\n",
    "# Output: ['Hello', 'world', 'PySpark', 'is', 'fun', 'Learn', 'big', 'data']\n",
    "words_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "259cad2b-f527-431f-95a7-03f8a564f41e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "   - **distinct**: Returns a new RDD containing distinct items from the original RDD. If your RDD contains duplicate elements and you want to get rid of them, the `distinct` transformation is your friend. It creates a new RDD with only unique elements from the original RDD, essentially removing all the duplicates. It's like a magic wand that can make all duplicate entries disappear, leaving only unique entries behind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96f556b3-82cc-42b3-b3c4-72abd01a1ef6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[34]: [1, 2, 3, 4, 5]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[34]: [1, 2, 3, 4, 5]</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Python example for 'distinct'\n",
    "numbers_rdd = sc.parallelize([1, 2, 3, 3, 4, 4, 5])\n",
    "\n",
    "unique_numbers_rdd = numbers_rdd.distinct()\n",
    "\n",
    "# Output: [1, 2, 3, 4, 5]\n",
    "unique_numbers_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0459f0d7-9be0-438c-955d-45009ae5556a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "   - **Union**: The union transformation is used to combine two RDDs into one RDD. It doesn't remove duplicates. If you want to remove duplicates, you can follow up the union transformation with a distinct transformation. It's a straightforward way to combine datasets into a single one for more complex analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ac75915-bfe4-4d9b-bc64-dddf582f309f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[1, 2, 3, 4, 5, 3, 4, 5, 6, 7]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">[1, 2, 3, 4, 5, 3, 4, 5, 6, 7]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Python example for 'Union'\n",
    "rdd1 = sc.parallelize([1, 2, 3, 4, 5])\n",
    "rdd2 = sc.parallelize([3, 4, 5, 6, 7])\n",
    "\n",
    "rdd_union = rdd1.union(rdd2)\n",
    "\n",
    "# Output: [1, 2, 3, 4, 5, 3, 4, 5, 6, 7]\n",
    "print(rdd_union.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b82feec-1dc6-4108-af66-be31f8496adf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "   - **ReduceByKey**: the reduceByKey transformation is used to combine values with the same key in an RDD of key-value pairs. You provide a function that specifies how to combine the values, and reduceByKey will apply that function to all values with the same key. It's an efficient way to aggregate data in an RDD, particularly when working with grouped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ab22fa5-42dc-48e5-91f7-cfa61283d656",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating an RDD with pairs representing Product ID and Sales amount\n",
    "rdd = sc.parallelize([(\"Product1\", 100), (\"Product2\", 200), (\"Product1\", 150), (\"Product2\", 100), (\"Product1\", 200), (\"Product2\", 300)])\n",
    "\n",
    "# Applying the reduceByKey transformation to sum up sales amounts for each product ID\n",
    "rdd_reduce_by_key = rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Collecting and printing the results to see the total sales amount for each product ID\n",
    "# Final Output: [('Product1', 450), ('Product2', 600)]\n",
    "print(rdd_reduce_by_key.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3082296-0bf5-4c62-b55d-7d5749787fc6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For a deeper understanding and to explore more transformation operations, you can refer to the [Spark documentation on transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations).\n",
    "\n",
    "#### Actions:\n",
    "Actions are operations that trigger the execution of the graph built during the transformation phase. Essentially, nothing is computed in your RDD until an action is called. Once an action is called, the data are computed in parallel across different nodes in your cluster, and the results are returned to the Spark driver.Let's explore some common actions that you would use frequently:\n",
    "\n",
    "\n",
    "   - **reduce**: This action aggregates all the elements in an RDD using a specified function which takes two inputs and returns a single output. It operates sequentially, applying the function to the first two elements, then applying it again to the result and the next element, and so on. It's a powerful action to perform operations like finding the sum, maximum, or minimum of elements in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "730cfa11-3e32-48d2-89c0-fbdf166d3960",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">15\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">15\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First, we initialize an RDD with a list of numbers\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# Next, we use the reduce action to find the sum of all the elements in the RDD.\n",
    "# The lambda function takes two arguments (x and y) and returns their sum.\n",
    "# This lambda function will be applied across all elements in the RDD to find the total sum.\n",
    "sum_of_elements = rdd.reduce(lambda x, y: x + y)\n",
    "\n",
    "# Let's print the result to verify\n",
    "# Final output: 15\n",
    "print(sum_of_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72e0d92c-e4cd-4063-b29e-647b223ee7c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">5\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">5\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Similarly, you can use reduce to find the maximum or minimum element in the RDD.\n",
    "# Here, we find the maximum element in the RDD.\n",
    "max_element = rdd.reduce(lambda x, y: x if x > y else y)\n",
    "\n",
    "# Printing the maximum element\n",
    "# Final Output: 5\n",
    "print(max_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68c9e630-63bb-4d47-8ccd-db7acf1b1a9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "   - **Collect**: Retrieves all the elements of the RDD to the driver node - this should be used cautiously with large datasets to avoid memory issues.This action retrieves all the elements from the RDD to the driver node (your local machine or the place where the SparkContext is initiated). It's often used for retrieving the final results of a computation or for debugging during development. However, it's important to use this action judiciously, especially with large datasets, as bringing too much data at once to the driver can cause memory overflow and slow down the entire process. It's often better to use actions like take(n) or first() to retrieve a limited number of results if you are just looking to preview the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a52cd6c2-5efe-4856-9096-e24e0928824e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[&#39;Spark&#39;, &#39;is&#39;, &#39;powerful&#39;, &#39;big&#39;, &#39;data&#39;, &#39;processing&#39;, &#39;tool&#39;]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">[&#39;Spark&#39;, &#39;is&#39;, &#39;powerful&#39;, &#39;big&#39;, &#39;data&#39;, &#39;processing&#39;, &#39;tool&#39;]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initializing an RDD with a list of words\n",
    "rdd = sc.parallelize([\"Spark\", \"is\", \"a\", \"powerful\", \"big\", \"data\", \"processing\", \"tool\"])\n",
    "\n",
    "# Applying a transformation: we will filter the words that have more than one letter\n",
    "filtered_rdd = rdd.filter(lambda x: len(x) > 1)\n",
    "\n",
    "# Now, we will use the collect action to retrieve all the elements from the filtered RDD to the driver node\n",
    "collected_data = filtered_rdd.collect()\n",
    "\n",
    "# Let's print the collected data\n",
    "# Final output: ['Spark', 'is', 'powerful', 'big', 'data', 'processing', 'tool']\n",
    "print(collected_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3553c49-c9cb-4964-a5c2-9926f5a0ca6a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "   - **Take**: This action is used to retrieve the first 'n' elements from an RDD, where 'n' is a parameter that you specify. It is a handy tool when you want to quickly inspect a few elements of the RDD without collecting all data (which might be large) back to the driver node. It helps in preventing memory overflow issues that might occur when using collect() on a large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e95bf8c-21aa-4f4d-8fe0-aa028db63d6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initializing an RDD with a range of numbers\n",
    "rdd = sc.parallelize(range(100))\n",
    "\n",
    "# Using the take action to get the first 10 elements of the RDD\n",
    "first_10_elements = rdd.take(10)\n",
    "\n",
    "# Printing the first 10 elements\n",
    "# Final output: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "print(first_10_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aedecfa3-4519-4c33-9ccf-c742932d256b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "   - **First**: The first() action, as the name suggests, is utilized to retrieve the very first element from an RDD. This action can be particularly useful when you are interested in quickly inspecting the first record of the dataset to understand its structure or format without loading the entire set of data to the driver node, thereby saving computational resources and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65246729-70c0-4826-91d8-8c648a202bbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">0\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">0\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initializing an RDD with a series of numbers\n",
    "rdd = sc.parallelize(range(100))\n",
    "\n",
    "# Using the first action to get the first element of the RDD\n",
    "first_element = rdd.first()\n",
    "\n",
    "# Printing the first element\n",
    "# Final output: 0\n",
    "print(first_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d13db2a-31d8-4f28-9669-c36e24f81b66",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For detailed explanations and a more extensive list of actions, refer to the [Spark documentation on actions](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49435775-d2ec-4c90-ae87-e3c845049d8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**6. Partitioning**\n",
    "\n",
    "In the context of Spark, partitioning is a means to distribute the workload across multiple nodes in a cluster, a method that enhances parallelism and hence the speed of data processing tasks. Each partition holds a portion of the data and operates independently, allowing computations to be carried out simultaneously, which is a significant advantage especially when working with big data.\n",
    "\n",
    "**Optimization Through Partitioning**\n",
    "\n",
    "Optimizing the performance of Spark applications often involves fine-tuning the partitioning strategy. Here are a few aspects to consider:\n",
    "\n",
    "1. **Number of Partitions:** Finding the right number of partitions is crucial. Too few partitions may not fully utilize the resources available, whereas too many partitions might increase the overhead due to task management.\n",
    "   \n",
    "2. **Data Skewness:** Sometimes the data might be unevenly distributed across partitions, a situation referred to as data skewness. It's essential to have a balanced distribution to ensure that all nodes in the cluster do work approximately equally.\n",
    "   \n",
    "3. **Tuning for Specific Operations:** Certain operations might benefit from a specific type of partitioning. For example, operations like 'join' can be optimized using partitioning to minimize data shuffling.\n",
    "\n",
    "Let's delve into an example to understand partitioning and how to optimize it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "852f6c11-4c6f-406e-a61f-646a1ea01a5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">The RDD is divided into 4 partitions.\n",
       "Partition: 0 | Elements: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
       "Partition: 1 | Elements: 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49\n",
       "Partition: 2 | Elements: 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74\n",
       "Partition: 3 | Elements: 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">The RDD is divided into 4 partitions.\nPartition: 0 | Elements: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\nPartition: 1 | Elements: 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49\nPartition: 2 | Elements: 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74\nPartition: 3 | Elements: 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initializing an RDD with a range of numbers and specifying the number of partitions\n",
    "rdd = sc.parallelize(range(100), 4)\n",
    "\n",
    "# Getting the number of partitions\n",
    "num_partitions = rdd.getNumPartitions()\n",
    "\n",
    "# Printing the number of partitions\n",
    "print(f\"The RDD is divided into {num_partitions} partitions.\")\n",
    "\n",
    "# Function to print the index and elements of each partition\n",
    "def show_partitions(index, iterator): yield f\"Partition: {index} | Elements: {' '.join(map(str, iterator))}\"\n",
    "\n",
    "# Using the mapPartitionsWithIndex method to apply the function to each partition\n",
    "partitions = rdd.mapPartitionsWithIndex(show_partitions).collect()\n",
    "\n",
    "# Printing the details of each partition\n",
    "for partition in partitions:\n",
    "    print(partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8143a32-49de-41b7-acfa-bb230f7d5eea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element counts in each partition: [50, 50]\n"
     ]
    }
   ],
   "source": [
    "# Apache Spark Example: Demonstrating Optimization by Reducing Data Skewness\n",
    "\n",
    "# Initialize an RDD with a skewed list of numbers\n",
    "# This creates a dataset where the number 1 appears 30 times, 2 appears 40 times, and 3 appears 30 times\n",
    "# This simulates a skewed dataset where certain values are overrepresented\n",
    "rdd_skewed = sc.parallelize([1]*30 + [2]*40 + [3]*30, 2)\n",
    "\n",
    "# Define a function to calculate the count of elements in each partition\n",
    "# The function iterates over the elements of a partition and counts them\n",
    "def count_elements(iterator): \n",
    "    yield sum(1 for _ in iterator)  # Count the number of elements in the iterator\n",
    "\n",
    "# Apply the function to each partition of the RDD using mapPartitions\n",
    "# mapPartitions applies a function to each partition of the RDD\n",
    "# The result is a new RDD where each element represents the count of items in a partition\n",
    "skewed_partitions_count = rdd_skewed.mapPartitions(count_elements).collect()\n",
    "\n",
    "# Print the count of elements in each partition\n",
    "# This shows the distribution of data across the partitions\n",
    "# In a skewed dataset, some partitions may have significantly more data than others\n",
    "print(f\"Element counts in each partition: {skewed_partitions_count}\")\n",
    "\n",
    "# The output helps in understanding the skewness in data distribution across partitions\n",
    "# Based on this information, further steps can be taken to optimize and balance the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09eb5d0f-be56-4330-9013-a05c190b105d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "7. **Persistence**: Persistence, or caching, is a feature in Spark that allows users to control the storage level of RDDs, facilitating the optimization of computations particularly when an RDD is reused multiple times within an application. You can decide whether to store the RDD in memory (RAM), which allows for quicker access times at the cost of higher storage requirements, or to store it on the disk, which is slower but less memory-intensive.\n",
    "By wisely choosing the persistence level, you can greatly speed up computations that access the RDD multiple times, as the data does not have to be recomputed from scratch with each action. It's a handy feature to keep in mind, especially when working on iterative algorithms or interactive data analysis tasks.\n",
    "In the following section, we will look at a Python example demonstrating how to set different persistence levels and how it can impact the performance of your Spark application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3353a7b-3a62-40a3-bedf-10694530cdd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Serialized 1x Replicated\n",
       "Disk Serialized 1x Replicated\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Serialized 1x Replicated\nDisk Serialized 1x Replicated\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "# Creating an RDD\n",
    "rdd = sc.parallelize([1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "# Persisting RDD in Memory\n",
    "rdd.persist()\n",
    "\n",
    "# Performing some transformations and actions\n",
    "rdd1 = rdd.map(lambda x: x * 2)\n",
    "rdd1.collect()\n",
    "\n",
    "# Checking the persistence level\n",
    "print(rdd1.getStorageLevel())\n",
    "\n",
    "# Unpersisting the RDD from Memory\n",
    "rdd1.unpersist()\n",
    "\n",
    "# Persisting RDD on Disk\n",
    "rdd1 = rdd.map(lambda x: x * 2)\n",
    "rdd1.persist(StorageLevel.DISK_ONLY)\n",
    "rdd1.collect()\n",
    "\n",
    "# Checking the persistence level\n",
    "print(rdd1.getStorageLevel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2059eb93-47da-47e9-a708-59a827b438c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Apache Spark: `.persist()` vs `.cache()`\n",
    "\n",
    "**`.cache()` Method:**\n",
    "- `.cache()` is a shorthand for using `.persist()` with the default storage level.\n",
    "- The default storage level for `.cache()` is `MEMORY_ONLY`, meaning that it stores the RDD or DataFrame in memory.\n",
    "- If there isn't enough memory, some partitions won't be cached and will be recomputed as needed.\n",
    "- `.cache()` is commonly used for keeping data in memory when the same RDD needs to be accessed multiple times.\n",
    "\n",
    "**`.persist()` Method:**\n",
    "- `.persist()` provides more flexibility by allowing you to specify the storage level.\n",
    "- Various storage levels include `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`, `MEMORY_AND_DISK_SER`, `DISK_ONLY`, etc.\n",
    "- With `.persist()`, you can control whether Spark stores the RDD:\n",
    "    - In memory\n",
    "    - On disk\n",
    "    - Both in memory and on disk\n",
    "    - In a serialized or deserialized format\n",
    "- `.persist()` is particularly useful for managing large datasets that may not fit entirely in memory, or for optimizing performance by choosing an appropriate storage strategy for specific use cases.\n",
    "\n",
    "In summary, while `.cache()` is a simple and convenient way to store data in memory, `.persist()` offers more control over the storage and serialization of RDDs or DataFrames in Apache Spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "574fe179-7aa3-4141-b572-dfc325d1b863",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "8. **Integration with Other Data Types**: \n",
    "\n",
    "In PySpark, RDDs are one part of a larger ecosystem, allowing for cohesive and flexible data handling and analysis. They can integrate seamlessly with other prominent data structures in Spark, namely DataFrames and Datasets, to facilitate more streamlined and diversified data analysis and machine learning workflows. Here's how they interact with each other:\n",
    "\n",
    " - **DataFrames**: A DataFrame is a distributed collection of data organized into named columns, similar to a table in a relational database. You can easily convert an RDD to a DataFrame, and vice versa, which provides more options for data manipulation and analysis. It allows for operations like SQL queries to be performed, taking advantage of the SparkSQL optimization engine. (we will deep dive into DataFrames in posterior sessions of the training)\n",
    "\n",
    " - **Datasets**: Datasets are a type-safe version of DataFrames, available in the Scala and Java Spark API. They combine the benefits of RDDs (type safety, user functions) and DataFrames (optimized execution plans). Although not available in PySpark, understanding how Datasets work can be beneficial when working with Spark in other programming languages. However, we don't work with datasets in Omnichannel.\n",
    "\n",
    "Let's see how to convert RDDs to DataFrames and leverage the additional functionalities offered by DataFrames in PySpark through an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1969456-d2e8-4d8b-a604-a54c8ef0d12c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ID</th><th>Name</th></tr></thead><tbody><tr><td>1</td><td>Alice</td></tr><tr><td>2</td><td>Bob</td></tr><tr><td>3</td><td>Charlie</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Alice"
        ],
        [
         2,
         "Bob"
        ],
        [
         3,
         "Charlie"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Create an RDD\n",
    "rdd = spark.sparkContext.parallelize([(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")])\n",
    "\n",
    "# Define a schema\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Convert RDD to DataFrame using the schema\n",
    "df = spark.createDataFrame(rdd, schema=schema)\n",
    "\n",
    "# Show DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c36637f9-cdbd-43e5-bbf4-41d232d1477b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1.5 Understanding DataFrames\n",
    "\n",
    "DataFrames are a vital part of Spark, introduced to overcome some of the limitations associated with RDDs and to provide a more structured and optimized way to handle data. Let's unravel the concept of DataFrames in Spark:\n",
    "\n",
    "1. **Definition and Structure**: A DataFrame in Spark is a distributed collection of data that is organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in Python's pandas library, but with more optimization and functionality under the hood.\n",
    "\n",
    "2. **Advantages over RDDs**:\n",
    "    - **Optimization**: DataFrames are built on top of the RDDs and optimized using Catalyst Optimizer which generates an optimized execution plan, making data processing faster and more efficient.\n",
    "    - **Ease of Use**: With their structured format and ability to use SQL queries directly, DataFrames are easier and more intuitive to use compared to RDDs.\n",
    "    - **Integration with Various Data Formats**: DataFrames can integrate seamlessly with various data formats (like JSON, CSV, Parquet) and databases, providing more flexibility in data handling and analysis.\n",
    "\n",
    "3. **Similitude with SQL and Pandas**:\n",
    "    - **SQL**: DataFrames can be queried using SQL queries directly in Spark, which makes it a handy tool for people with a background in relational databases.\n",
    "    - **Pandas**: For those familiar with Python's Pandas library, transitioning to using DataFrames in Spark is relatively straightforward due to the similarities in their structure and functionalities.\n",
    "\n",
    "Let's go through a basic example where we create a DataFrame and perform some operations, akin to SQL/Pandas operations, to give you a glimpse of how DataFrames function in Spark.\n",
    "\n",
    "Further Reading:\n",
    "- [Introduction to DataFrames - Apache Spark](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- [DataFrames API in Python - Databricks](https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7ef9077-4b9f-4779-8f5e-5703595c42f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th></tr></thead><tbody><tr><td>Alice</td><td>25</td></tr><tr><td>Bob</td><td>30</td></tr><tr><td>Charlie</td><td>35</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Alice",
         25
        ],
        [
         "Bob",
         30
        ],
        [
         "Charlie",
         35
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating a DataFrame and performing basic operations\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Create a list of Row objects\n",
    "row_list = [Row(name=\"Alice\", age=25), Row(name=\"Bob\", age=30), Row(name=\"Charlie\", age=35)]\n",
    "\n",
    "# Create a DataFrame from the list of Row objects\n",
    "df = spark.createDataFrame(row_list)\n",
    "\n",
    "# Show the DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82740a61-f90f-455f-baf1-bd8bd578aea6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Exercise 1: Analyzing Sales Data\n",
    "\n",
    "#### Context\n",
    "\n",
    "You are given a dataset containing sales data from different regions. Your task is to analyze this data using PySpark to gain insights into the sales performance of different products and regions.\n",
    "\n",
    "#### Task 1: Data Preparation\n",
    "- Load the provided data into an RDD.\n",
    "- Inspect the first few entries in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d2ca11b-d181-477e-871e-59dfa6385ab7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data to use\n",
    "data = [\n",
    "    ('North', 'Product1', 100),\n",
    "    ('South', 'Product1', 200),\n",
    "    ('East', 'Product1', 300),\n",
    "    ('West', 'Product1', 400),\n",
    "    ('North', 'Product2', 150),\n",
    "    ('South', 'Product2', 250),\n",
    "    ('East', 'Product2', 350),\n",
    "    ('West', 'Product2', 450),\n",
    "    ('North', 'Product3', 200),\n",
    "    ('South', 'Product3', 300),\n",
    "    ('East', 'Product3', 400),\n",
    "    ('West', 'Product3', 500),\n",
    "    ('North', 'Product4', 250),\n",
    "    ('South', 'Product4', 350),\n",
    "    ('East', 'Product4', 450),\n",
    "    ('West', 'Product4', 550),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "937a51cc-5200-4833-a0a6-ef2e7caadc80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here your code\n",
    "# (add appropriate PySpark operations to achieve each of the subtasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "069487ae-f4c6-4fad-a360-404bf31c7942",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Task 2: Data Transformation and Analysis\n",
    "- Find out the total sales per region.\n",
    "- Find out the total sales per product.\n",
    "- Find the region with the highest sales.\n",
    "- Find the product with the highest sales in each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb55616c-a127-46da-9683-9a61f8d258b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Here your code\n",
    "# (add appropriate PySpark operations to achieve each of the subtasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36d859e9-ac63-4276-96b5-6b8631f89bae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Task 3: Data Optimization and Persistence\n",
    "- Optimize the data partitioning to enhance the performance of your analysis.\n",
    "- Persist the intermediate RDDs that are reused multiple times in the application to optimize the computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e01c178b-c38d-4c82-ac80-5177d737cd55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Here your code\n",
    "# (add appropriate PySpark operations to achieve each of the subtasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d25e7441-23cf-47c5-be46-34991a190232",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Task 4: Integration with Other Data Types\n",
    "- Convert the RDD to a DataFrame and perform a simple SQL query to find the total sales per region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdf850fb-ce42-4033-a6e1-8e9fd7d9da1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Here your code\n",
    "# (add appropriate PySpark operations to achieve each of the subtasks)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) 1. Introduction to Big Data & PySpark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
