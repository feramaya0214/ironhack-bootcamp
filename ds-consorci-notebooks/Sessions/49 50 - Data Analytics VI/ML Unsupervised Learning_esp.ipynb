{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0225c1f",
   "metadata": {},
   "source": [
    "# Machine Learning: Técnicas de Aprendizaje No Supervisado\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*PnrNu1pepOF4euxKx4brGw.png)\n",
    "\n",
    "El Machine Learning (ML) es un campo dinámico y en constante evolución que se sitúa en la intersección de la informática y la estadística. En esencia, el ML consiste en crear y utilizar modelos capaces de encontrar patrones en los datos y utilizarlos para hacer predicciones sobre datos nuevos y desconocidos. Es como enseñar a un ordenador a tomar decisiones basándose en pruebas y no en un estricto conjunto de instrucciones programadas.\n",
    "\n",
    "**¿Cómo funciona el Machine Learning?**\n",
    "\n",
    "1. **Ingesta de Datos:** El ML comienza con datos, muchos de ellos. Estos datos pueden ser cualquier cosa, desde números e imágenes hasta texto y sonido.\n",
    "2. **Reconocimiento de Patrones:** Se utilizan algoritmos para buscar patrones y relaciones en los datos. Esto es similar a encontrar las piezas de un rompecabezas.\n",
    "3. **Construcción del Modelo:** Una vez descubiertos los patrones, el ML los utiliza para construir un modelo predictivo. Es como montar las piezas del rompecabezas para ver una parte de la imagen.\n",
    "4. **Predicciones:** El modelo predictivo puede ahora tomar nuevos datos, aplicar los patrones aprendidos y hacer predicciones o tomar decisiones sin estar explícitamente programado para realizar la tarea.\n",
    "5. **Aprendizaje y Mejora:** Los modelos de ML están diseñados para aprender y mejorar con el tiempo a medida que se exponen a más datos.\n",
    "\n",
    "**La relación entre Machine Learning e Inteligencia Artificial**\n",
    "\n",
    "El Machine Learning es el motor que impulsa las tecnologías de Inteligencia Artificial. He aquí cómo se relacionan:\n",
    "\n",
    "- **Inteligencia Artificial (IA):** La IA es la ciencia más amplia de imitar las habilidades humanas. Se trata de crear máquinas que puedan realizar tareas que normalmente requieren inteligencia humana.\n",
    "- **Machine Learning:** El ML es un subconjunto de la IA que dota a las máquinas de la capacidad de aprender de los datos y mejorar con el tiempo. Es una de las formas de conseguir IA.\n",
    "\n",
    "**Tipos de aprendizaje**\n",
    "\n",
    "- **Aprendizaje Supervisado:** El modelo aprende utilizando un conjunto de datos etiquetado, que actúa como un profesor que da las respuestas correctas (etiquetas) para los puntos de datos.\n",
    "- **Aprendizaje No Supervisado:** El modelo busca patrones y relaciones en datos no etiquetados, aprendiendo de la estructura inherente a los datos.\n",
    "- **Aprendizaje por Refuerzo:** El modelo aprende a tomar decisiones realizando acciones y recibiendo retroalimentación en forma de recompensas o penalizaciones.\n",
    "\n",
    "**¿Y qué es el Deep Learning?**\n",
    "\n",
    "El Deep Learning es un subconjunto sofisticado y potente del machine learning. Se inspira en la estructura y el funcionamiento del cerebro humano en forma de red neuronal. Los modelos de deep learning pueden aprender automáticamente representaciones a partir de datos como imágenes, vídeo o texto, sin introducir reglas codificadas a mano ni conocimientos humanos del dominio.\n",
    "\n",
    "Una red neuronal, en el contexto del deep learning, es una arquitectura compleja de nodos interconectados (neuronas) dispuestos en capas. Estas capas pueden pensarse como una jerarquía de conceptos, donde cada capa combina la información de las anteriores para crear representaciones más complejas.\n",
    "\n",
    "1. **Capa de Entrada:** Recibe los datos en bruto. Es el punto de contacto inicial entre los datos y la red neuronal.\n",
    "2. **Capas Ocultas:** Estas capas realizan la mayor parte del trabajo computacional. Cada capa oculta transforma los datos en representaciones más abstractas y compuestas.\n",
    "3. **Capa de Salida:** Esta capa proporciona la salida final, que puede ser una clasificación (como identificar un objeto en una imagen) o un valor en una tarea de regresión.\n",
    "\n",
    "Las redes de deep learning aprenden a través de un proceso iterativo llamado retropropagación, donde el modelo hace predicciones, mide el error de esas predicciones y luego vuelve a través de la red para ajustar los pesos. Este proceso se repite miles o millones de veces, y con cada pasada, la red mejora en la producción de predicciones precisas.\n",
    "\n",
    "Los modelos de deep learning, debido a su profundidad y complejidad, requieren una potencia de cálculo considerable. Sin embargo, con el auge de la computación en la nube y de hardware especializado como las GPU, se ha vuelto más accesible, allanando el camino para investigaciones y aplicaciones revolucionarias.\n",
    "\n",
    "\n",
    "**Machine Learning, IA y mucho más**\n",
    "\n",
    "Entender el Machine Learning y la Inteligencia Artificial no es solo una cuestión de tecnología. Se trata de comprender un cambio fundamental en la forma en que analizamos los datos, tomamos decisiones y creamos software para interactuar con el mundo. Al embarcarnos en este viaje, exploraremos cómo se construyen estas tecnologías, cómo pueden utilizarse y las consideraciones éticas que conllevan.\n",
    "\n",
    "En las siguientes secciones, profundizaremos en cada tipo de aprendizaje, exploraremos algoritmos clave y examinaremos su impacto en diversos sectores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bf1faa",
   "metadata": {},
   "source": [
    "## Diferencia entre Machine Learning y Estadística\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:400/0*0o8cCtW-obWCQBHA.png)\n",
    "\n",
    "Comprender la diferencia entre Machine Learning (ML) y estadística es esencial, ya que ambos campos son fundamentales para hacer sentido de los datos en el mundo moderno. Aunque comparten terreno común, sus filosofías, objetivos y enfoques a menudo difieren.\n",
    "\n",
    "**Estadística: La Ciencia de los Datos**\n",
    "\n",
    "La estadística es una rama de las matemáticas que se ocupa de la recolección, análisis, interpretación y presentación de masas de datos numéricos. Se trata de entender y cuantificar la incertidumbre y la variabilidad en los datos.\n",
    "\n",
    "- **Estadísticas Descriptivas** ayudan a resumir y visualizar datos para describir sus características principales.\n",
    "- **Estadísticas Inferenciales** utilizan muestras para hacer generalizaciones o inferencias sobre una población más grande.\n",
    "\n",
    "Ofrece un conjunto de algoritmos y métodos establecidos diseñados para tipos específicos de análisis de datos:\n",
    "\n",
    "- **Métodos Paramétricos** asumen una forma específica para la distribución de los datos y usan los datos de la muestra para inferir los parámetros de la población.\n",
    "- **Métodos No Paramétricos** no asumen una forma fija y son más flexibles, permitiendo el análisis sin suposiciones estrictas de distribución.\n",
    "\n",
    "Los estadísticos han desarrollado métodos rigurosos para pruebas de hipótesis, estimación y conclusiones a partir de datos. Estos métodos están diseñados para interpretar datos y cuantificar la incertidumbre.\n",
    "\n",
    "**Machine Learning: El Enfoque Algorítmico**\n",
    "\n",
    "El Machine Learning, por otro lado, es un subcampo de la informática que evolucionó del estudio del reconocimiento de patrones y la teoría del aprendizaje computacional en la IA. Involucra el desarrollo de algoritmos que pueden aprender de los datos y hacer predicciones sobre ellos.\n",
    "\n",
    "- **Aprendizaje Supervisado** los algoritmos hacen predicciones basadas en un conjunto de ejemplos (pares entrada-salida).\n",
    "- **Aprendizaje No Supervisado** los algoritmos encuentran patrones ocultos o estructuras intrínsecas en los datos de entrada.\n",
    "\n",
    "Los algoritmos que emplea no son fijos, sino que se desarrollan y refinan a medida que se exponen a más datos:\n",
    "\n",
    "- **Aprender de los Datos:** En lugar de usar un algoritmo predeterminado, los métodos de ML modifican y mejoran sus algoritmos continuamente a medida que aprenden de más datos.\n",
    "- **Enfoque Predictivo:** El ML es menos acerca de probar una hipótesis y más sobre hacer predicciones precisas. Utiliza métodos estadísticos como bloques de construcción para crear algoritmos adaptativos que mejoran a medida que ingieren más datos.\n",
    "\n",
    "La distinción es similar a la diferencia entre usar un plan de construcción prefabricado (estadística) versus construir un edificio que se diseña a sí mismo basado en el entorno en el que se encuentra (machine learning).\n",
    "\n",
    "**Disciplinas Complementarias en el Análisis de Datos**\n",
    "\n",
    "El ML no reemplaza a la estadística; más bien, se basa en teorías estadísticas para crear algoritmos flexibles que pueden ajustarse y mejorar. Así es como se complementan entre sí:\n",
    "\n",
    "- **Rigor Estadístico en ML:** El ML aprovecha conceptos estadísticos como regresión, métodos bayesianos y estimaciones de verosimilitud para comprender y construir sobre los patrones en los datos.\n",
    "- **Innovación de ML en Estadística:** Los algoritmos adaptativos de ML están mejorando el análisis estadístico, especialmente en áreas con datos complejos que no se ajustan bien a los modelos tradicionales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc1d8b",
   "metadata": {},
   "source": [
    "# Plan de la sesión: Técnicas de aprendizaje no supervisado en Python\n",
    "\n",
    "## 1. Introducción al Aprendizaje No Supervisado\n",
    "### 1.1 Visión general del aprendizaje no supervisado\n",
    "- Definición detallada, alcance y aplicaciones en el mundo real.\n",
    "### 1.2 Distinción entre Aprendizaje Supervisado y No Supervisado\n",
    "- Comparación en profundidad con ejemplos ilustrativos.\n",
    "\n",
    "## 2. Técnicas de clusterización\n",
    "### 2.1 Estudio en profundidad de la clusterización K-means\n",
    "- Conceptos teóricos, pasos del algoritmo e implementación en Python.\n",
    "- Actividad práctica:** Aplicación de K-means a un conjunto de datos de muestra, incluyendo visualización de clusters.\n",
    "### 2.2 Exploración del Clustering Jerárquico\n",
    "- Explicación detallada de los métodos de clustering jerárquico y sus usos.\n",
    "- **Ejercicio Práctico:** Clustering Jerárquico en Python con análisis de dendrogramas.\n",
    "### 2.3 Comprensión e Implementación de DBSCAN\n",
    "- Exploración exhaustiva de los principios y casos de uso de DBSCAN.\n",
    "- Sesión interactiva: DBSCAN en Python con ajuste de parámetros.\n",
    "\n",
    "## 3. Perfiles de Clusters\n",
    "### 3.1 Profundización en el perfilado de clusters\n",
    "- Importancia y estrategias para el perfilado efectivo de clusters.\n",
    "### 3.2 Técnicas avanzadas para el perfilado de clusters\n",
    "- Guía detallada de resúmenes estadísticos y técnicas avanzadas de visualización.\n",
    "- **Taller Interactivo:** Perfilado de Clusters en Python usando varias librerías.\n",
    "\n",
    "## 4. Reducción de Dimensionalidad con PCA\n",
    "### 4.1 Dominio de PCA\n",
    "- Discusión extensa sobre conceptos, matemáticas y aplicaciones de PCA.\n",
    "### 4.2 PCA práctico en Python\n",
    "- Guía paso a paso para la implementación de PCA.\n",
    "- **Estudio de caso:** Aplicación de PCA a un conjunto de datos multidimensionales e interpretación de resultados.\n",
    "\n",
    "## 5. Minería de Reglas de Asociación (Association Rules)\n",
    "### 5.1 Fundamentos de las Reglas de Asociación\n",
    "- Profundización en conceptos, métricas (soporte, confianza, elevación) y significado en minería de datos.\n",
    "### 5.2 Algoritmo Apriori en Acción\n",
    "- Explicación detallada e implementación en Python.\n",
    "- **Aplicación en el mundo real:** Ejecución de la minería de reglas de asociación en un conjunto de datos de venta al por menor.\n",
    "\n",
    "## 6. Detección de Anomalías\n",
    "### 6.1 Técnicas Avanzadas de Detección de Anomalías\n",
    "- Exploración elaborada de métodos como Isolation Forest y One-Class SVM.\n",
    "### 6.2 Detección de Anomalías en Python\n",
    "- Guía detallada de implementación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76acd17",
   "metadata": {},
   "source": [
    "## 1. Introducción al Aprendizaje No Supervisado\n",
    "\n",
    "### 1.1 Visión General del Aprendizaje No Supervisado\n",
    "\n",
    "El aprendizaje no supervisado es un tipo de aprendizaje automático que se ocupa del reconocimiento de patrones y el análisis exploratorio de datos. A diferencia del aprendizaje supervisado, los algoritmos de aprendizaje no supervisado no reciben datos de salida etiquetados. En cambio, estos algoritmos exploran la estructura de los datos para extraer información significativa sin orientación.\n",
    "\n",
    "**Puntos Clave:**\n",
    "- **Exploración de Datos:** El aprendizaje no supervisado se utiliza a menudo para descubrir la estructura subyacente de los datos.\n",
    "- **Agrupamiento:** Una tarea común de aprendizaje no supervisado es agrupar datos en clústeres basados en métricas de similitud o distancia.\n",
    "- **Reducción de Dimensionalidad:** Reducir el número de variables en consideración, que puede dividirse en selección de características y extracción de características.\n",
    "\n",
    "**Aplicaciones en el Mundo Real:**\n",
    "- **Segmentación de Mercado:** Identificar grupos distintos de clientes para adaptar estrategias de marketing.\n",
    "- **Genómica:** Clasificar genes con funcionalidades similares sin conocimiento previo.\n",
    "- **Detección de Anomalías:** Identificar patrones inusuales que no se ajustan al comportamiento esperado.\n",
    "\n",
    "### 1.2 Distinción entre Aprendizaje Supervisado y No Supervisado\n",
    "\n",
    "![](https://www.researchgate.net/publication/351953193/figure/fig3/AS:11431281117150742@1675395484096/Supervised-and-unsupervised-machine-learning-a-Schematic-representation-of-an.png)\n",
    "\n",
    "El aprendizaje supervisado y el no supervisado representan dos enfoques fundamentales del aprendizaje automático, cada uno con sus casos de uso y métodos únicos.\n",
    "\n",
    "**Aprendizaje Supervisado:**\n",
    "- **Datos Etiquetados:** Los algoritmos de aprendizaje supervisado se entrenan con datos etiquetados, lo que significa que cada ejemplo de entrenamiento se empareja con una etiqueta de salida.\n",
    "- **Retroalimentación Directa:** El objetivo es aprender un mapeo de entradas a salidas, con retroalimentación directa sobre la precisión de la predicción.\n",
    "- **Tareas de Predicción:** Las tareas comunes incluyen la clasificación y la regresión.\n",
    "\n",
    "**Aprendizaje No Supervisado:**\n",
    "- **Sin Etiquetas:** En contraste, el aprendizaje no supervisado trata con datos que no tienen etiquetas históricas.\n",
    "- **Sin Retroalimentación:** El sistema no se le dice la respuesta 'correcta'; debe descubrir qué se le está mostrando.\n",
    "- **Tareas de Descubrimiento:** El objetivo es explorar los datos y encontrar alguna estructura dentro.\n",
    "\n",
    "**Ilustración Comparativa:**\n",
    "Imagina el aprendizaje supervisado como un estudiante aprendiendo con un profesor para resolver problemas matemáticos con una clave de respuestas conocida. Por el contrario, el aprendizaje no supervisado es como un detective tratando de encontrar patrones y conexiones en un caso sin pistas previas.\n",
    "\n",
    "**Conectando Ambos:**\n",
    "Aunque el aprendizaje supervisado y el no supervisado pueden parecer distintos, pueden ser complementarios. El aprendizaje semisupervisado, por ejemplo, utiliza tanto datos etiquetados como no etiquetados, lo cual es particularmente útil cuando adquirir etiquetas es costoso.\n",
    "\n",
    "En las siguientes secciones, exploraremos varias técnicas de aprendizaje no supervisado en Python, entenderemos sus principios y las aplicaremos a conjuntos de datos reales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bc4a4b",
   "metadata": {},
   "source": [
    "## 2. Técnicas de Agrupamiento\n",
    "\n",
    "### 2.1 Estudio Profundo del Agrupamiento K-means\n",
    "\n",
    "El agrupamiento K-means es una técnica ampliamente utilizada en el aprendizaje automático no supervisado para dividir un conjunto de datos en grupos distintos, conocidos como 'clusters'. Es particularmente útil cuando tienes datos que se agrupan naturalmente pero no tienen etiquetas que guíen el agrupamiento.\n",
    "\n",
    "[VIDEO: Enlace K-means](https://www.youtube.com/watch?v=4b5d3muPQmA&ab_channel=StatQuestwithJoshStarmer)\n",
    "\n",
    "#### Entendimiento del Agrupamiento K-means:\n",
    "\n",
    "Piensa en K-means como una forma de organizar puntos de datos en grupos basados en su similitud. Aquí tienes una analogía simple: imagina que tienes una mezcla de bolas de diferentes colores esparcidas por el suelo, y quieres organizarlas en grupos del mismo color. El agrupamiento K-means sería como encontrar el punto central para cada grupo de color y luego agrupar las bolas en función de qué punto central están más cerca.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1200/1*rw8IUza1dbffBhiA4i0GNQ.png)\n",
    "\n",
    "#### Conceptos Teóricos:\n",
    "\n",
    "- **Centroides**: Estos son los 'puntos centrales' de cada cluster. Imagina cada cluster como un círculo de puntos de datos, el centroide estaría justo en el centro de este círculo.\n",
    "- **Paso de Asignación**: Aquí, se verifica cada punto de datos para ver a qué centroide está más cerca (como cada bola averiguando qué punto central está más cerca).\n",
    "- **Paso de Actualización**: Después de que todos los puntos de datos hayan sido asignados a los centroides, los centroides recalibran — se mueven al verdadero centro de sus respectivos clusters.\n",
    "- **Iteración**: Estos pasos se repiten. Con cada iteración, los centroides se mueven, y los puntos de datos pueden cambiar a un centroide más cercano, hasta que los centroides se estabilizan y no se mueven más.\n",
    "\n",
    "#### Pasos del Algoritmo Explicados:\n",
    "\n",
    "1. **Inicialización**: Imagina colocar banderas al azar en tu conjunto de datos. Estas banderas son tus 'centroides' iniciales.\n",
    "2. **Asignación**: Cada punto de datos en tu conjunto 'decide' a qué bandera (centroide) está más cerca y se agrupa allí.\n",
    "3. **Actualización**: Cada bandera (centroide) luego se mueve al centro de su grupo de puntos de datos.\n",
    "4. **Repetir**: Los pasos 2 y 3 se siguen repitiendo. Con cada repetición, los grupos se refinan más y las banderas se mueven menos, hasta que no necesitan moverse más.\n",
    "\n",
    "#### Objetivo de K-means:\n",
    "\n",
    "El objetivo es minimizar la distancia entre los puntos en un cluster y su centroide. Cuando los centroides dejan de moverse (o el movimiento es mínimo), significa que los clusters son tan ajustados y precisos como pueden ser, basados en el algoritmo K-means.\n",
    "\n",
    "#### Cómo seleccionar el valor de K:\n",
    "\n",
    "##### Método del Puntaje Silhouette\n",
    "\n",
    "El Puntaje Silhouette es una medida utilizada para determinar el grado de separación entre los clusters en el agrupamiento K-means. Proporciona una visión de la distancia entre los clusters resultantes.\n",
    "\n",
    "- **Cómo Funciona**: El Puntaje Silhouette evalúa qué tan cerca está cada punto en un cluster de puntos en los clusters vecinos. Varía de -1 a +1, donde un valor alto indica que el objeto está bien emparejado con su propio cluster y mal emparejado con los clusters vecinos.\n",
    "- **Cálculo**: Para cada muestra, el Puntaje Silhouette se calcula como `(b - a) / max(a, b)`, donde `a` es la distancia media a los otros puntos en el mismo cluster y `b` es la distancia media al cluster más cercano.\n",
    "- **Interpretación**: Un puntaje cercano a +1 indica que los puntos de datos están lejos de los clusters vecinos, mientras que un puntaje cercano a 0 indica que los clusters se están superponiendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36543d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "# Load the IRIS dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # Feature data\n",
    "y = iris.target  # Target labels (not used in K-means clustering)\n",
    "\n",
    "# Setting up the matplotlib figure with multiple subplots\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15,8))\n",
    "\n",
    "# Loop through different numbers of clusters (from 2 to 5)\n",
    "for i in [2, 3, 4, 5]:\n",
    "    # Create KMeans instance for different number of clusters\n",
    "    # 'k-means++' for smart centroid initialization, 10 different centroid initializations\n",
    "    # 100 iterations max for each run, and set a random state for reproducibility\n",
    "    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)\n",
    "\n",
    "    # Determine the position of the subplot\n",
    "    q, mod = divmod(i, 2)\n",
    "\n",
    "    # Create a SilhouetteVisualizer with the KMeans instance\n",
    "    # Colors are set to 'yellowbrick' palette, and the subplot ax is defined\n",
    "    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])\n",
    "\n",
    "    # Fit the visualizer to the data to produce the silhouette plot\n",
    "    visualizer.fit(X)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0e55a0",
   "metadata": {},
   "source": [
    "- Para una K determinada, todos los clústeres deben tener una puntuación Silhouette superior a la puntuación media del conjunto de datos representado por la línea de puntos rojos. El eje x representa la puntuación Silhouette. Los clústeres con K=4 y 5 se eliminan porque no cumplen esta condición.\n",
    "- No debería haber grandes fluctuaciones en el tamaño de los clústeres. La anchura de los clústeres representa el número de puntos de datos. Para K=2, el cluster azul tiene casi el doble de anchura que el cluster verde. Este cúmulo azul se divide en dos subcúmulos para K=3, formando así cúmulos de tamaño uniforme.\n",
    "\n",
    "##### Método del codo (Elbow method)\n",
    "\n",
    "El método del codo es otra técnica utilizada para determinar el número óptimo de clústeres en la clusterización K-means. Se centra en minimizar la varianza dentro de cada cluster.\n",
    "\n",
    "- **Cómo funciona**: El método consiste en trazar la varianza explicada en función del número de clústeres y elegir el codo de la curva como el número de clústeres a utilizar.\n",
    "- **Cálculo**: Se calcula la suma de las distancias al cuadrado de las muestras a su centro de clúster más cercano, variando el número de clústeres `k` y observando el cambio en la suma de las distancias al cuadrado.\n",
    "- **Interpretación**: El punto en el que la distorsión/inercia empieza a disminuir más lentamente (el «codo») se considera un indicador del número adecuado de clústeres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e167452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# Load the IRIS dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # Feature data\n",
    "y = iris.target  # Target labels (not used in K-means clustering)\n",
    "\n",
    "# Instantiate the KMeans model\n",
    "# random_state=42 is used for reproducibility of results\n",
    "km = KMeans(random_state=42)\n",
    "\n",
    "# Instantiate the KElbowVisualizer with the KMeans model\n",
    "# k=(2,10) indicates the range of number of clusters to try (from 2 to 10)\n",
    "visualizer = KElbowVisualizer(km, k=(2,10))\n",
    "\n",
    "# Fit the visualizer to the data\n",
    "# This will run K-means clustering for each value of k and calculate the distortion score for each\n",
    "visualizer.fit(X)\n",
    "\n",
    "# Render the plot\n",
    "# The Elbow plot displays the distortion score for each k\n",
    "# The point where the distortion score starts to level off ('elbow') is the recommended number of clusters\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1274695",
   "metadata": {},
   "source": [
    "##### Comparación de la puntuación de la silueta y el método del codo\n",
    "\n",
    "- **Objetivo**: Mientras que la puntuación Silhouette Score se centra en la calidad de la clusterización (es decir, lo distintos que son los clusters), el método Elbow se centra más en minimizar la varianza dentro de cada cluster.\n",
    "- **Claridad de la indicación**: El método del codo a veces puede ser ambiguo, ya que el «codo» puede no ser muy pronunciado. En cambio, la puntuación de silueta proporciona un valor numérico más definitivo que puede ser más fácil de interpretar.\n",
    "- **Casos prácticos**: La puntuación de silueta es especialmente útil cuando los datos no son demasiado densos y cuando se espera que los clústeres sean densos y estén bien separados. El método del codo se utiliza a menudo como pauta general, especialmente cuando se desea simplicidad computacional.\n",
    "\n",
    "Ambos métodos tienen sus puntos fuertes y pueden utilizarse de forma complementaria. El método del codo puede dar una buena estimación inicial del número de clústeres, que luego puede refinarse utilizando la puntuación de silueta para una evaluación más precisa y cualitativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21968310",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go  #for 3D plot\n",
    "\n",
    "## K-means using k = 3\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "## 3D plot \n",
    "Scene = dict(xaxis = dict(title  = 'sepal_length -->'),yaxis = dict(title  = 'sepal_width--->'),zaxis = dict(title  = 'petal_length-->'))\n",
    "\n",
    "labels = kmeans.labels_\n",
    "trace = go.Scatter3d(x=X[:, 0], y=X[:, 1], z=X[:, 2], mode='markers',marker=dict(color = labels, size= 10, line=dict(color= 'black',width = 10)))\n",
    "layout = go.Layout(margin=dict(l=0,r=0),scene = Scene,height = 800,width = 800)\n",
    "data = [trace]\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182b7a90",
   "metadata": {},
   "source": [
    "#### Comparación de las predicciones de clusterización de K-means con las etiquetas reales\n",
    "\n",
    "En este análisis, exploramos la eficacia de la clusterización K-means, un algoritmo de aprendizaje no supervisado, para identificar correctamente grupos en el conocido conjunto de datos Iris. Este conjunto de datos contiene tres especies de flores de Iris, lo que lo convierte en un candidato ideal para probar la capacidad de clusterización de K-means.\n",
    "\n",
    "##### Descripción general del proceso\n",
    "\n",
    "1. **Carga de datos**: Comenzamos cargando el conjunto de datos de Iris, que incluye características como la longitud de los sépalos, la anchura de los sépalos, la longitud de los pétalos y la anchura de los pétalos.\n",
    "\n",
    "2. **Clasificación K-means**: Aplicamos el algoritmo de clusterización K-means a estos datos. K-means se encarga de dividir los datos en tres clusters, correspondientes a las tres especies de Iris del conjunto de datos.\n",
    "\n",
    "3. **Predicción y alineación**: El algoritmo K-means asigna cada punto de datos a uno de estos clusters. Dado que K-means no está supervisado, no utiliza las etiquetas reales (especies) durante la clusterización. Por lo tanto, alineamos las etiquetas de clústeres predichas con las etiquetas reales para realizar una comparación significativa.\n",
    "\n",
    "###### Introducción a la matriz de confusión\n",
    "\n",
    "La matriz de confusión es una potente herramienta para evaluar el rendimiento de un algoritmo de clasificación. Compara las etiquetas reales de los datos con las etiquetas predichas por el modelo, proporcionando una representación visual clara de la precisión del modelo y de los errores de clasificación.\n",
    "\n",
    "- Cada fila de la matriz representa las instancias de una clase real.\n",
    "- Cada columna representa los casos predichos por el modelo.\n",
    "- Los elementos diagonales indican el número de clasificaciones correctas de cada clase.\n",
    "- Los elementos no diagonales indican los errores o clasificaciones erróneas.\n",
    "\n",
    "Al analizar la matriz de confusión, no sólo podemos ver cuántas instancias se clasificaron correctamente, sino también qué clases específicas confunde el algoritmo. Esta información es valiosa para comprender el rendimiento y las limitaciones de nuestro modelo de clusterización.\n",
    "\n",
    "En el siguiente script de Python, vemos esta comparación utilizando una matriz de confusión para evaluar el rendimiento de la clusterización de K-means a la hora de distinguir las diferentes especies de flores de Iris en función de sus características.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be296b0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # Feature data\n",
    "y = iris.target  # Target labels\n",
    "\n",
    "# Apply K-means clustering\n",
    "# KMeans is initialized with 3 clusters and a fixed random state for reproducibility\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)  # Fit KMeans model to the data\n",
    "y_kmeans = kmeans.predict(X)  # Predict the clusters for each data point\n",
    "\n",
    "# Function to align K-means labels with true labels\n",
    "def align_labels(kmeans_labels, true_labels):\n",
    "    \"\"\"\n",
    "    Aligns K-means cluster labels with true labels for maximum correspondence.\n",
    "    \n",
    "    :param kmeans_labels: Labels predicted by K-means\n",
    "    :param true_labels: Actual true labels of the data\n",
    "    :return: Aligned labels\n",
    "    \"\"\"\n",
    "    labels = np.zeros_like(kmeans_labels)\n",
    "    for i in range(3):\n",
    "        mask = (kmeans_labels == i)\n",
    "        # Assign the mode (most frequent label) of true labels to each cluster\n",
    "        labels[mask] = mode(true_labels[mask])[0]\n",
    "    return labels\n",
    "\n",
    "# Align the predicted labels with the true labels for meaningful comparison\n",
    "y_aligned = align_labels(y_kmeans, y)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y, y_aligned)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues',\n",
    "            xticklabels=iris.target_names,  # Set x-axis labels as target names\n",
    "            yticklabels=iris.target_names)  # Set y-axis labels as target names\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Clustered Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1051994",
   "metadata": {},
   "source": [
    "### Ejercicio de Clustering K-means: Análisis del conjunto de datos Wine\n",
    "\n",
    "En este ejercicio, aplicará el clustering K-means al conjunto de datos Vino, un conjunto de datos popular para tareas de clasificación. Este conjunto de datos contiene los resultados de un análisis químico de vinos cultivados en la misma región de Italia a partir de tres cultivares diferentes. El análisis determinó las cantidades de 13 constituyentes encontrados en cada uno de los tres tipos de vinos.\n",
    "\n",
    "#### Objetivo\n",
    "\n",
    "Su tarea consiste en utilizar la clusterización de K-means para analizar el conjunto de datos Wine y agrupar los vinos en función de su composición química. Después de la clusterización, compare sus resultados con los cultivares de vino reales para evaluar el rendimiento del algoritmo de clusterización.\n",
    "\n",
    "#### Descripción general del conjunto de datos\n",
    "\n",
    "- Número de instancias 178\n",
    "- Características**: 13 características numéricas, que representan diferentes componentes químicos como el alcohol, el ácido málico, la intensidad del color, etc.\n",
    "- Clases**: El conjunto de datos contiene tres tipos de vino, que representan tres cultivares diferentes.\n",
    "\n",
    "#### Pasos a seguir\n",
    "\n",
    "1. **Cargar el conjunto de datos**: Importar el conjunto de datos Vino desde `sklearn.datasets`.\n",
    "2. **Exploración de datos**: Familiarizarse con el conjunto de datos. Comprueba el número de características, instancias y explora las estadísticas básicas del conjunto de datos.\n",
    "3. **Aplicar K-means Clustering**:\n",
    "   - Utilice el algoritmo K-means de `sklearn.cluster`.\n",
    "   - Experimente con diferentes números de clusters (comience con `n_clusters=3`).\n",
    "   - Ajuste el modelo a los datos y prediga el cluster para cada instancia de vino.\n",
    "4. **Evaluar la clusterización**:\n",
    "   - Utilizar métricas como la puntuación de silueta o comparar con los cultivares de vino reales utilizando una matriz de confusión.\n",
    "   - Visualice los clusters si es posible (puede que necesite utilizar técnicas de reducción de dimensionalidad como PCA para la visualización).\n",
    "5. **Interpretar los resultados**:\n",
    "   - Analice las características de cada clúster.\n",
    "   - Determine si los clusters se corresponden bien con los cultivares reales.\n",
    "   - Reflexione sobre la eficacia de K-means para este tipo de datos.\n",
    "\n",
    "#### Preguntas para análisis posteriores\n",
    "\n",
    "- ¿Cómo afectó el número de clústeres (`n_clusteres`) al resultado de la clusterización?\n",
    "- ¿Qué características parecen contribuir más a distinguir entre los distintos clusters?\n",
    "- ¿Podría algún paso de preprocesamiento (como el escalado) mejorar los resultados de la clusterización?\n",
    "\n",
    "Este ejercicio mejorará la comprensión del clustering K-means y su aplicación en conjuntos de datos del mundo real. También te permitirá comprender mejor los retos del aprendizaje no supervisado, sobre todo a la hora de interpretar los resultados y evaluar el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ac01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial code for the Wine dataset K-means clustering exercise\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Convert to DataFrame for easier data manipulation and visualization\n",
    "wine_df = pd.DataFrame(X, columns=wine.feature_names)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"First five rows of the dataset:\")\n",
    "print(wine_df.head())\n",
    "\n",
    "print(\"\\nDataset statistics:\")\n",
    "print(wine_df.describe())\n",
    "\n",
    "# Initial K-means clustering setup\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Predicting the clusters\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Calculating silhouette score\n",
    "silhouette_avg = silhouette_score(X, labels)\n",
    "print(\"\\nSilhouette Score for 3 clusters: \", silhouette_avg)\n",
    "\n",
    "# Adding cluster information to the DataFrame\n",
    "wine_df['cluster'] = labels\n",
    "\n",
    "# Initial visualization - pairplot (optional, based on time and requirement)\n",
    "# sns.pairplot(wine_df, hue='cluster')\n",
    "# plt.show()\n",
    "\n",
    "# The code snippet above sets the stage for the exercise. It includes loading the data, basic exploration,\n",
    "# applying K-means clustering, and calculating the silhouette score. Encourage your students to explore\n",
    "# further by varying the number of clusters, visualizing the data, and comparing the clustering results\n",
    "# with the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d02c56a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Instantiate the KMeans model\n",
    "# random_state=42 is used for reproducibility of results\n",
    "km = KMeans(random_state=42)\n",
    "\n",
    "# Instantiate the KElbowVisualizer with the KMeans model\n",
    "# k=(2,10) indicates the range of number of clusters to try (from 2 to 10)\n",
    "visualizer = KElbowVisualizer(km, k=(2,10))\n",
    "\n",
    "# Fit the visualizer to the data\n",
    "# This will run K-means clustering for each value of k and calculate the distortion score for each\n",
    "visualizer.fit(X)\n",
    "\n",
    "# Render the plot\n",
    "# The Elbow plot displays the distortion score for each k\n",
    "# The point where the distortion score starts to level off ('elbow') is the recommended number of clusters\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59af3427",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b44a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go  #for 3D plot\n",
    "\n",
    "## K-means using k = 3\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "## 3D plot \n",
    "Scene = dict(xaxis = dict(title  = 'sepal_length -->'),yaxis = dict(title  = 'sepal_width--->'),zaxis = dict(title  = 'petal_length-->'))\n",
    "\n",
    "labels = kmeans.labels_\n",
    "trace = go.Scatter3d(x=X[:, 0], y=X[:, 1], z=X[:, 2], mode='markers',marker=dict(color = labels, size= 10, line=dict(color= 'black',width = 10)))\n",
    "layout = go.Layout(margin=dict(l=0,r=0),scene = Scene,height = 800,width = 800)\n",
    "data = [trace]\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d9e21a",
   "metadata": {},
   "source": [
    "#### Aplicación de K-means Clustering en la vida real\n",
    "\n",
    "El clustering de K-means no es sólo para el estudio académico; también es una herramienta práctica utilizada en muchas situaciones empresariales cotidianas. Entender cómo utilizar K-means en escenarios de la vida real puede mostrar su importancia y cómo puede ayudar en los negocios.\n",
    "\n",
    "- **Segmentación de clientes:** Las empresas suelen utilizar K-means para agrupar a los clientes en función de lo que compran, sus preferencias y su lugar de residencia. Esto ayuda a las empresas a crear planes de marketing especiales, desarrollar productos y ofrecer servicios que atraigan más a cada grupo de clientes.**Ejemplo:** Una tienda agrupa a sus clientes en función de lo que compran. Esto ayuda a la tienda a enviar mensajes de marketing a cada grupo que tienen más probabilidades de captar su interés.\n",
    "\n",
    "- **Gestión del inventario:** K-means puede ayudar a las empresas a gestionar mejor sus existencias. Agrupando los productos en función de lo bien que se venden y otros factores, las empresas pueden mantener la cantidad adecuada de existencias, reducir costes y mejorar su cadena de suministro. **Ejemplo:** Una tienda de ropa utiliza K-means para clasificar la ropa en función de lo rápido que se vende y de la época del año. Esto ayuda a la tienda a saber qué cantidad de cada tipo de ropa debe tener en stock.\n",
    "\n",
    "- **Clasificación de documentos:** K-means es útil para organizar muchos documentos. Esto es bueno para encontrar información rápidamente, mantener grandes bases de datos organizadas, y hacer el trabajo con documentos más eficiente.**Ejemplo:** Un bufete de abogados utiliza K-means para ordenar muchos archivos de casos. Esto hace que sea más fácil encontrar los documentos adecuados para cada caso legal.\n",
    "\n",
    "- **Estudio del mercado:** K-means puede ayudar a entender las tendencias del mercado mediante el análisis de datos sobre el mercado. Esto es importante para la planificación, la entrada en nuevos mercados, y la comprensión de la competencia.**Ejemplo**: Una empresa que estudia mercados utiliza K-means para analizar datos de encuestas. Esto les ayuda a ver los diferentes grupos de clientes y lo que les gusta en un área específica.\n",
    "\n",
    "**Conclusión:** La clusterización de K-means ayuda a las empresas de muchas maneras. Puede encontrar patrones en los datos y agrupar datos complejos en categorías útiles. Esto ayuda a tomar buenas decisiones y a trabajar de forma más eficiente. A medida que aprendes sobre el análisis de datos, saber cómo utilizar K-means en estas situaciones será muy útil para tu carrera.\n",
    "\n",
    "En nuestros ejercicios, hemos visto cómo funciona K-means con conjuntos de datos como Iris y Wine. Ahora piensa en cómo puedes utilizar este método para resolver problemas empresariales reales. Piensa en posibles usos en tu propio campo o industria, y en cómo la clusterización de datos puede aportar nuevas perspectivas y ayudar a tomar decisiones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a71d521",
   "metadata": {},
   "source": [
    "### 2.2 Exploración de la clusterización jerárquica\n",
    "\n",
    "La clusterización jerárquica es un método matizado y perspicaz de análisis de clústeres, ampliamente reconocido por su aplicación en diversos campos que van desde la bioinformática a las ciencias sociales. Funciona de forma fundamentalmente diferente a los métodos de partición como K-means, proporcionando una jerarquía de clústeres que puede visualizarse en un dendrograma.\n",
    "\n",
    "[Explicación de la clusterización jerárquica](https://www.youtube.com/watch?v=7xHsRkOdVwo&ab_channel=StatQuestwithJoshStarmer)\n",
    "\n",
    "![](https://www.kdnuggets.com/wp-content/uploads/c_unveiling_hidden_patterns_introduction_hierarchical_clustering_9.png)\n",
    "\n",
    "#### Antecedentes teóricos de la clusterización jerárquica:\n",
    "\n",
    "1. **Concepto y tipos**:\n",
    "   - El clustering jerárquico construye clusters anidados fusionándolos o dividiéndolos sucesivamente. El proceso se visualiza como un diagrama en forma de árbol llamado dendrograma.\n",
    "   - Aglomerativo (ascendente)**: Este enfoque comienza considerando cada punto de datos como un clúster independiente y luego los fusiona en función de la similitud. Es similar a construir un árbol genealógico de hijos a padres ([Johnson, 1967](https://doi.org/10.1007/BF02289588)).\n",
    "   - **Divisivo (Top-Down)**: Aquí, todos los puntos de datos comienzan en un clúster, que se divide recursivamente en clústeres más pequeños. Se asemeja a dividir un árbol en ramas y luego en hojas.\n",
    "\n",
    "2. **Métricas de distancia y criterios de vinculación**:\n",
    "   - Métricas de distancia: Definen cómo se mide la «distancia» entre los puntos de datos. Las métricas comunes incluyen la similitud euclidiana, Manhattan y coseno.\n",
    "   - Criterios de enlace**: Este criterio determina cómo medir la distancia entre clusters. Las opciones incluyen:\n",
    "     - *Vinculación simple*: Distancia entre los miembros más cercanos de los clusters.\n",
    "     - *Enlace Completo*: Distancia entre los miembros más alejados.\n",
    "     - *Average Linkage*: Distancia media entre todos los pares de miembros.\n",
    "     - *Método de Ward*: Minimización de la varianza dentro de cada clúster ([Ward, 1963](https://www.jstor.org/stable/2528416)).\n",
    "\n",
    "#### Importancia en el análisis de datos:\n",
    "\n",
    "- **Versatilidad**: Puede aplicarse a cualquier conjunto de datos en el que pueda definirse una medida de distancia.\n",
    "- Sin clústeres preestablecidos**: A diferencia de K-means, no requiere especificar el número de clústeres de antemano, lo que ofrece flexibilidad para determinar la granularidad de los clústeres basándose en el dendrograma.\n",
    "- Interpretación detallada de los datos**: El dendrograma presenta una imagen detallada de los datos, revelando estructuras y relaciones intrincadas ([Murtagh, F., & Contreras, P., 2012](https://link.springer.com/article/10.1007/s00357-012-9101-4)).\n",
    "\n",
    "La clusterización jerárquica se utiliza ampliamente en biología para la clasificación de genes y proteínas ([Eisen et al., 1998](https://www.pnas.org/content/95/25/14863)), en lingüística para agrupar idiomas similares y en marketing para la segmentación de clientes. Su capacidad para descubrir relaciones detalladas entre datos la convierte en una potente herramienta para el análisis exploratorio de datos.\n",
    "\n",
    "En el próximo ejercicio práctico, aplicaremos la clusterización jerárquica a un conjunto de datos y exploraremos cómo interpretar su estructura jerárquica mediante el análisis de dendrogramas. Esta experiencia práctica es crucial para comprender cómo se puede aprovechar la clusterización jerárquica para extraer información significativa de conjuntos de datos complejos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4226ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Hierarchical Clustering Example using the Iris Dataset\n",
    "\n",
    "# Importing necessary libraries\n",
    "from sklearn import datasets  # For loading the Iris dataset\n",
    "import matplotlib.pyplot as plt  # For plotting the dendrogram\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage  # For hierarchical clustering\n",
    "import numpy as np  # For numerical operations\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # The feature data (sepal length, sepal width, petal length, petal width)\n",
    "\n",
    "# Performing hierarchical clustering\n",
    "# 'linkage' is used to perform the hierarchical clustering\n",
    "# The 'ward' method is an approach to minimize the variance within each cluster\n",
    "linked = linkage(X, 'ward')\n",
    "\n",
    "# Creating a dendrogram to visualize the clustering\n",
    "# A dendrogram is a diagram that shows the hierarchical relationship between objects\n",
    "# It's often used in hierarchical clustering to visualize the structure of the clusters\n",
    "plt.figure(figsize=(10, 7))  # Setting the size of the plot\n",
    "dendrogram(linked,\n",
    "           orientation='top',  # The direction to plot the dendrogram\n",
    "           labels=np.array(iris.target),  # Labels for each point\n",
    "           distance_sort='descending',  # Sorting the distances in descending order\n",
    "           show_leaf_counts=True)  # Show the number of samples in each cluster\n",
    "plt.title('Hierarchical Clustering Dendrogram (Ward)')  # Title of the plot\n",
    "plt.xlabel('Sample Index')  # X-axis label\n",
    "plt.ylabel('Distance')  # Y-axis label representing the distances between clusters\n",
    "plt.show()  # Display the plot\n",
    "\n",
    "# In this plot, the y-axis represents the distance or dissimilarity between clusters.\n",
    "# The x-axis represents the individual samples or clusters (after merging).\n",
    "# The height of each U-shaped line represents the distance between the two data points being linked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91214b8e",
   "metadata": {},
   "source": [
    "Volvamos de nuevo a la **teoría**:\n",
    "\n",
    "#### **Técnica de clusterización jerárquica aglomerativa**\n",
    "\n",
    "En esta técnica, inicialmente cada punto de datos se considera un clúster individual. En cada iteración, los clusters similares se fusionan con otros clusters hasta que se forma un cluster o K clusters.\n",
    "\n",
    "El algoritmo básico de Aglomerativo es sencillo.\n",
    "- Cálculo de la matriz de proximidad\n",
    "- Cada punto de datos es un clúster\n",
    "- Repetir el fusionar los dos clusters más cercanos y actualizar la matriz de proximidad\n",
    "- Solo debe quedar un clúster\n",
    "\n",
    "La operación clave es el cálculo de la proximidad de dos clusters.\n",
    "\n",
    "Para entenderlo mejor, veamos una representación pictórica de la técnica de agrupación jerárquica aglomerativa. Supongamos que tenemos seis puntos de datos {A,B,C,D,E,F}.\n",
    "\n",
    "- **Paso 1**: En el paso inicial, calculamos la proximidad de los puntos individuales y consideramos los seis puntos de datos como clusters individuales como se muestra en la siguiente imagen.\n",
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*3pMZjFiiaaLcfSZBKDjbXA.png)\n",
    "\n",
    "- **Paso 2**: En el segundo paso, los clusters similares se fusionan y forman un único cluster. Consideremos que B,C, y D,E son clusters similares que se fusionan en el paso dos. Ahora nos quedan cuatro clusters: A, BC, DE, F.\n",
    "- **Paso 3**: Volvemos a calcular la proximidad de los nuevos clusters y fusionamos los clusters similares para formar los nuevos clusters A, BC, DEF.\n",
    "- **Paso 4**: Calcular la proximidad de los nuevos clusters. Los clusters DEF y BC son similares y se fusionan para formar un nuevo cluster. Ahora nos quedan dos clusters A, BCDEF.\n",
    "- **Paso 5**: Finalmente, todos los clusters se fusionan y forman un único cluster.\n",
    "\n",
    "La técnica de clusterización jerárquica puede visualizarse mediante un dendrograma.\n",
    "\n",
    "Un dendrograma es un diagrama en forma de árbol que registra las secuencias de fusiones o divisiones.\n",
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*JPQRbJDw2E1_HEvwzVTDDw.jpeg)\n",
    "![](https://miro.medium.com/v2/resize:fit:1000/format:webp/1*fw1vlNtq2vPFmAXsBy1_dA.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12199cde",
   "metadata": {},
   "source": [
    "#### **Técnica de clustering Jerárquico Divisivo**: \n",
    "\n",
    "Dado que la Técnica de clustering Jerárquico Divisivo no se utiliza mucho en el mundo real, daré una breve descripción de la Técnica de clustering Jerárquico Divisivo.\n",
    "\n",
    "En palabras simples, podemos decir que el clustering Jerárquico Divisivo es exactamente lo contrario del clustering Jerárquico Aglomerativo. En el clustering jerárquico divisivo, consideramos todos los puntos de datos como un único cluster y, en cada iteración, separamos los puntos de datos del cluster que no son similares. Cada punto de datos que se separa se considera un clúster individual. Al final, nos quedarán n clusters.\n",
    "\n",
    "Como estamos dividiendo los clusters individuales en n clusters, se denomina Clustering Jerárquico Divisivo.\n",
    "\n",
    "Por lo tanto, hemos discutido los dos tipos de la técnica de clustering jerárquico.\n",
    "\n",
    "Pero espera, todavía nos queda la parte importante del Clustering Jerárquico.\n",
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*dpL-5neZyXbiX30zPNc5rQ.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16644c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Enhanced Hierarchical Clustering Example with Comparison to Actual Labels\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target  # Actual labels\n",
    "\n",
    "# Perform hierarchical clustering using the 'ward' method\n",
    "linked = linkage(X, 'ward')\n",
    "\n",
    "# Set a maximum distance (threshold) to cut the dendrogram\n",
    "max_distance = 10  # Adjust this threshold based on the dendrogram\n",
    "cluster_labels = fcluster(linked, max_distance, criterion='distance')\n",
    "\n",
    "# Plotting dendrogram with the threshold line\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)\n",
    "plt.axhline(y=max_distance, color='r', linestyle='--')\n",
    "plt.title('Hierarchical Clustering Dendrogram (Ward) with threshold')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Compare the obtained clusters with the actual labels\n",
    "cm = confusion_matrix(y, cluster_labels - 1)  # Adjusting labels to start from 0\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Print the number of clusters and cluster labels\n",
    "num_clusters = np.unique(cluster_labels).size\n",
    "print(f'Number of clusters formed: {num_clusters}')\n",
    "print(f'Cluster labels for each point: {cluster_labels}')\n",
    "\n",
    "# This comparison helps in evaluating the accuracy of hierarchical clustering \n",
    "# in grouping the data points as per the actual species in the Iris dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752b5d01",
   "metadata": {},
   "source": [
    "#### Decidir la distancia de corte en la clusterización jerárquica\n",
    "\n",
    "Determinar el lugar adecuado para «cortar» el dendrograma en la agrupación jerárquica es crucial para definir el número de clústeres. Existen varios métodos numéricos para guiar esta decisión:\n",
    "\n",
    "##### 1. Inspección visual\n",
    "- **Descripción**: Este método consiste en observar el dendrograma e identificar el mayor hueco vertical que no esté cruzado por líneas horizontales extendidas.\n",
    "- **Utilización**: Se utiliza para encontrar una división natural entre clusters, donde la fusión de clusters aumenta significativamente la distancia.\n",
    "\n",
    "##### 2. Método del coeficiente de inconsistencia\n",
    "- **Descripción**: Calcula el coeficiente de inconsistencia para cada enlace, que es una medida de la disimilitud de un enlace en comparación con los enlaces que están por debajo de él.\n",
    "- **Utilización**: Los enlaces con coeficientes de incoherencia elevados sugieren divisiones significativas y pueden utilizarse para establecer el umbral de corte.\n",
    "\n",
    "##### 3. Puntuación de silueta\n",
    "- **Descripción**: Aunque es más común en K-means, mide la similitud de un objeto con su propio cluster en comparación con otros clusters.\n",
    "- Uso**: Al cortar el dendrograma a varias alturas y calcular la puntuación de silueta para cada corte, la altura que arroja la mejor puntuación indica una agrupación óptima.\n",
    "\n",
    "##### 4. Uso de criterios predefinidos\n",
    "- **Descripción**: En función de las necesidades empresariales externas o de preguntas de investigación específicas, se predefine el número necesario de clusters.\n",
    "- **Utilización**: Este número predefinido de clusters puede determinar la distancia de corte adecuada en el dendrograma.\n",
    "\n",
    "##### 5. Ensayo y error\n",
    "- **Descripción**: Implica experimentar con diferentes distancias, observar las formaciones de clusters resultantes y elegir la que parezca más razonable.\n",
    "- **Utilización**: Este método se utiliza a menudo cuando otros métodos no dan resultados claros o satisfactorios.\n",
    "\n",
    "##### Conclusión\n",
    "Cada uno de estos métodos ofrece un enfoque diferente para determinar el número de clusters en la agrupación jerárquica. La elección del método puede depender de las características del conjunto de datos, los objetivos específicos del análisis y la familiaridad del analista con los datos. La comprensión de estos métodos puede mejorar el proceso de toma de decisiones en las tareas de clustering y conducir a resultados de clustering más significativos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b240a0e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Python Code for Determining Cutting Distance in Hierarchical Clustering\n",
    "\n",
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, inconsistent\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Perform hierarchical clustering using the 'ward' method\n",
    "Z = linkage(X, 'ward')\n",
    "\n",
    "# 1. Inconsistency Coefficient Method\n",
    "# The inconsistency coefficient compares the height of each link in a cluster hierarchy with the average height of links below it\n",
    "depth = 5  # Specify the depth for calculating inconsistency\n",
    "inconsistencies = inconsistent(Z, depth)\n",
    "inconsistency_threshold = np.average(inconsistencies[:, -1])  # Average inconsistency value\n",
    "clusters_icm = fcluster(Z, inconsistency_threshold, criterion='inconsistent')\n",
    "\n",
    "# 2. Silhouette Score Method\n",
    "# This approach calculates the silhouette score for each possible number of clusters and selects the one with the best score\n",
    "silhouette_scores = []\n",
    "range_n_clusters = list(range(2, 10))  # Range of possible clusters\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    clusters = fcluster(Z, n_clusters, criterion='maxclust')\n",
    "    score = silhouette_score(X, clusters)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "best_cluster_num = range_n_clusters[np.argmax(silhouette_scores)]  # Number of clusters with highest silhouette score\n",
    "clusters_ssm = fcluster(Z, best_cluster_num, criterion='maxclust')\n",
    "\n",
    "# Plotting the Silhouette Scores\n",
    "plt.plot(range_n_clusters, silhouette_scores)\n",
    "plt.title('Silhouette Score Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()\n",
    "\n",
    "# Results\n",
    "print(f'Optimal number of clusters (Inconsistency Coefficient Method): {np.unique(clusters_icm).size}')\n",
    "print(f'Optimal number of clusters (Silhouette Score Method): {best_cluster_num}')\n",
    "\n",
    "# These techniques provide a more data-driven approach to finding the optimal number of clusters \n",
    "# in hierarchical clustering, enhancing the accuracy and reliability of the clustering process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81898c21",
   "metadata": {},
   "source": [
    "#### Clustering Jerárquico vs K-means\n",
    "\n",
    "Cuando se trata de análisis de clústeres en data science, tanto el Clustering Jerárquico como K-means son opciones populares. Comprender sus diferencias es crucial para seleccionar el método adecuado para un conjunto de datos y un objetivo de análisis determinados.\n",
    "\n",
    "##### Clustering Jerárquico\n",
    "\n",
    "- **Naturaleza**: Crea una jerarquía de clusters mediante un enfoque ascendente (aglomerativo) o descendente (divisivo).\n",
    "- **Número de clusters**: No es necesario especificar de antemano el número de clusters. El número de clusters puede determinarse analizando el dendrograma.\n",
    "- **Flexibilidad**: Ofrece más flexibilidad con respecto al número de clusters.\n",
    "- **Complejidad computacional**: Generalmente más intensivo computacionalmente, especialmente para grandes conjuntos de datos, debido a la naturaleza jerárquica del algoritmo.\n",
    "- **Interpretabilidad**: El dendrograma proporciona una visión profunda de la estructura de datos.\n",
    "- **Casos prácticos**: Especialmente útil en el análisis exploratorio de datos cuando se desconoce la estructura de los datos y las relaciones entre los clusters.\n",
    "\n",
    "##### Agrupación de K-means\n",
    "\n",
    "- **Naturaleza**: Método de partición basado en centroides que divide los datos en subconjuntos no superpuestos (clusters) sin establecer ninguna estructura jerárquica.\n",
    "- **Número de clusters**: Requiere la especificación previa del número de clusters (k).\n",
    "- **Velocidad y escalabilidad**: Tiende a ser más rápido y más escalable para grandes conjuntos de datos en comparación con la agrupación jerárquica.\n",
    "- **Simplicidad**: El algoritmo es más fácil de entender e implementar.\n",
    "- **Casos de uso**: Ideal para situaciones en las que se conoce la estructura subyacente de los datos o se desea un número específico de clusters para análisis posteriores.\n",
    "\n",
    "##### Diferencias clave\n",
    "\n",
    "- **Estructura del algoritmo**: El clustering jerárquico crea un modelo arborescente de los datos, mientras que K-means divide los datos en clusters distintos.\n",
    "- **Determinación del número de clústeres**: El clustering jerárquico no requiere especificar el número de clusters de antemano, a diferencia de K-means.\n",
    "- **Recursos informáticos**: El clustering jerárquico es más exigente computacionalmente, por lo que K-means es la opción preferida para conjuntos de datos muy grandes.\n",
    "- **Interpretación de los resultados**: El dendrograma del clustering jerárquico puede proporcionar más información sobre los datos, mientras que K-means ofrece una partición directa de los datos.\n",
    "\n",
    "##### Elección entre Clustering Jerárquico y K-means\n",
    "\n",
    "La elección entre clustering jerárquico y K-means debe basarse en el tamaño del conjunto de datos, los recursos informáticos disponibles y los objetivos específicos del análisis. Para el análisis exploratorio, en el que las relaciones entre los puntos de datos son de interés, el clustering jerárquico es ventajoso. Por otro lado, K-means es adecuado para grandes conjuntos de datos en los que el objetivo es dividir los datos en un número predeterminado de clústeres.\n",
    "\n",
    "Comprender estas diferencias le ayudará a seleccionar el método más adecuado para sus necesidades de clustering, garantizando un análisis de datos eficaz y eficiente.\n",
    "\n",
    "## Ejercicio: Práctica de Clustering Jerárquico\n",
    "\n",
    "### Objetivo:\n",
    "En este ejercicio, aplicarás el Clustering Jerárquico a un conjunto de datos práctico. Tu objetivo es comprender cómo funciona y cómo interpretar los resultados, especialmente a través del dendrograma.\n",
    "\n",
    "### Conjunto de Datos: El Conjunto de Datos del Vino\n",
    "Para este ejercicio, usaremos el conocido conjunto de datos del vino. Este conjunto de datos contiene los resultados de un análisis químico de vinos cultivados en la misma región en Italia de tres diferentes cultivos. El análisis determinó las cantidades de 13 constituyentes encontrados en cada uno de los tres tipos de vinos.\n",
    "\n",
    "### Pasos a seguir:\n",
    "\n",
    "1. **Cargar el Conjunto de Datos**:\n",
    "   - Utiliza `sklearn.datasets` para cargar el conjunto de datos del vino.\n",
    "   - Explora brevemente el conjunto de datos (tamaño, número de características).\n",
    "\n",
    "2. **Aplicar Clustering Jerárquico**:\n",
    "   - Usa la función `linkage` de `scipy.cluster.hierarchy` para realizar el agrupamiento jerárquico.\n",
    "   - Elige un método de enlace apropiado (por ejemplo, 'ward', 'single', 'complete').\n",
    "\n",
    "3. **Crear y Analizar un Dendrograma**:\n",
    "   - Utiliza la función `dendrogram` para visualizar la jerarquía de los clusters.\n",
    "   - Determina el número de clusters identificando la distancia vertical más grande que no intersecta ninguna línea horizontal (o utiliza otro método discutido).\n",
    "\n",
    "4. **Comparar los Clusters con las Etiquetas Reales**:\n",
    "   - Usa `fcluster` para formar clusters en la distancia elegida.\n",
    "   - Compara los clusters obtenidos con las etiquetas reales (si están disponibles) usando una matriz de confusión.\n",
    "\n",
    "### Resultados Esperados:\n",
    "- Un entendimiento claro de cómo el agrupamiento jerárquico agrupa datos.\n",
    "- Perspectivas sobre las diferencias en los resultados del clustering basados en varios métodos de enlace.\n",
    "- Capacidad para interpretar un dendrograma y decidir sobre el número de clusters.\n",
    "\n",
    "### Preguntas para Reflexión:\n",
    "- ¿Cómo afectó la elección del método de enlace a los resultados del clustering?\n",
    "- ¿Qué tan cercanamente coinciden los clusters formados con las etiquetas reales en el conjunto de datos?\n",
    "- ¿Qué percepciones sobre los datos del vino puedes inferir de los resultados del clustering?\n",
    "\n",
    "Este ejercicio fortalecerá tu comprensión del agrupamiento jerárquico y su aplicación en el análisis de datos. También destacará las sutilezas de interpretar dendrogramas y la importancia de elegir el método de enlace correcto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b73ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Python Code for Hierarchical Clustering Exercise using the Wine Dataset\n",
    "\n",
    "# Importing necessary libraries\n",
    "from sklearn import datasets\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data  # Feature data\n",
    "y = wine.target  # Actual labels\n",
    "\n",
    "# Perform hierarchical clustering using the 'ward' method\n",
    "linked = linkage(X, 'ward')\n",
    "\n",
    "# Plotting the dendrogram to visualize the clustering hierarchy\n",
    "plt.figure(figsize=(12, 7))\n",
    "dendrogram(linked, orientation='top', labels=y, distance_sort='descending', show_leaf_counts=True)\n",
    "plt.title('Hierarchical Clustering Dendrogram (Ward)')\n",
    "plt.xlabel('Sample Index or Cluster Size')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Choosing a distance threshold to determine the number of clusters\n",
    "# This threshold can be set by visually inspecting the dendrogram\n",
    "distance_threshold = 150  # This is an example value\n",
    "clusters = fcluster(linked, distance_threshold, criterion='distance')\n",
    "\n",
    "# Comparing the obtained clusters with the actual labels using a confusion matrix\n",
    "cm = confusion_matrix(y, clusters)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')\n",
    "plt.title('Confusion Matrix between Hierarchical Clusters and Actual Labels')\n",
    "plt.xlabel('Predicted Clusters')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Additional instructions for the exercise can include experimenting with different linkage methods,\n",
    "# such as 'single', 'complete', 'average', and interpreting the impact on the dendrogram and clustering results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c352642e",
   "metadata": {},
   "source": [
    "### 2.3 Comprender e implementar DBSCAN\n",
    "\n",
    "\n",
    "**Ejemplo:** Considere un parque con personas dispersas: algunas en grupos, otras solas. DBSCAN identifica los grupos de personas que se encuentran cerca (dentro de una distancia `eps` y en grupos mayores que `min_samples`). Las personas que forman parte de un grupo están en un conglomerado, mientras que los individuos alejados de cualquier grupo se tratan como valores atípicos.\n",
    "\n",
    "[Enlace DBSCAN](https://www.youtube.com/watch?v=RDZUdRSDOok&ab_channel=StatQuestwithJoshStarmer)\n",
    "\n",
    "- **Comparación con K-means y Clustering Jerárquico:**\n",
    "\n",
    "    - **K-means**: Un método de clustering basado en el centroide que divide los datos en K clusters. K-means asume que los clusters tienen forma convexa (como círculos o esferas) y requiere que el número de clusters se especifique de antemano. A diferencia de DBSCAN, K-means puede tener problemas con los conglomerados no convexos y es sensible a los valores atípicos.\n",
    "  \n",
    "    - **Agrupación jerárquica**: Construye un árbol de conglomerados de abajo arriba o de arriba abajo. No es necesario especificar previamente el número de conglomerados, pero puede requerir un gran esfuerzo informático y es posible que no gestione eficazmente el ruido y los valores atípicos. La agrupación jerárquica es útil para comprender la estructura jerárquica de los datos, pero no categoriza el ruido de forma inherente como lo hace DBSCAN.\n",
    "\n",
    "    La capacidad de DBSCAN para manejar conglomerados de forma irregular e identificar valores atípicos le da una ventaja significativa en los conjuntos de datos del mundo real, donde estas características son comunes. Esto convierte a DBSCAN en una herramienta muy valiosa para el análisis exploratorio de datos, sobre todo en situaciones complejas en las que K-means o la agrupación jerárquica podrían no dar resultados óptimos.\n",
    "    \n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*rfi9uHjGPdNgXgxe9xWvVw.png)\n",
    "\n",
    "#### Explicación sencilla de DBSCAN:\n",
    "\n",
    "1. **Qué hace DBSCAN**: Agrupa los puntos que están cerca unos de otros basándose en una distancia establecida y un número mínimo de puntos. Los puntos que no se encuentran en áreas densas se marcan como valores atípicos.\n",
    "\n",
    "2. **Términos clave en DBSCAN**:\n",
    "   - **eps (ε)**: Medida de distancia que define lo cerca que deben estar unos puntos de otros para ser considerados parte de un cluster.\n",
    "   - **min_muestras**: El número mínimo de puntos necesarios para formar una región densa, que DBSCAN trata como un cluster.\n",
    "\n",
    "3. **Tipos de puntos**\n",
    "   - **Puntos núcleo**: Un punto que tiene al menos un número mínimo de otros puntos (min_muestras) dentro de una cierta distancia (eps).\n",
    "   - **Puntos frontera**: Puntos que están dentro de la distancia (eps) de un punto núcleo pero que no tienen suficientes puntos cercanos para ser ellos mismos puntos núcleo.\n",
    "   - **Puntos de ruido**: Puntos que no son ni puntos núcleo ni puntos frontera.\n",
    "\n",
    "#### ¿Por qué usar DBSCAN?\n",
    "\n",
    "- Bueno para formas complejas**: A diferencia de K-means, DBSCAN no asume que los clusters son redondos, por lo que puede encontrar clusters de cualquier forma.\n",
    "- Maneja bien los valores atípicos**: Puede identificar e ignorar valores atípicos, haciéndolo robusto en escenarios de datos del mundo real.\n",
    "- No es necesario especificar el número de conglomerados**: No necesita saber cuántos clusters está buscando, lo cual es un requisito en otros algoritmos como K-means.\n",
    "\n",
    "#### Casos de uso comunes:\n",
    "\n",
    "- Análisis de datos geográficos**: Como la identificación de áreas de uso similar del suelo en imágenes de satélite.\n",
    "- Detección de anomalías**: Como la identificación de transacciones fraudulentas en la banca.\n",
    "\n",
    "#### Ejercicio: Implementación de DBSCAN Clustering\n",
    "\n",
    "En este ejercicio, obtendrá experiencia práctica con el algoritmo de clustering DBSCAN utilizando el conjunto de datos Iris. DBSCAN es conocido por su capacidad para identificar conglomerados de forma arbitraria y por su robustez frente a valores atípicos. Este ejercicio le guiará a través de la aplicación de DBSCAN, el ajuste de sus parámetros y la visualización de los resultados.\n",
    "\n",
    "##### Objetivo:\n",
    "Aprender a aplicar el algoritmo de clustering DBSCAN a un conjunto de datos real, comprender la importancia del ajuste de parámetros e interpretar el resultado del clustering.\n",
    "\n",
    "##### Conjunto de datos: Conjunto de datos Iris\n",
    "El conjunto de datos Iris es un conjunto de datos clásico en el aprendizaje automático y la estadística, con mediciones de 150 flores de iris de tres especies diferentes. Utilizarás DBSCAN para agrupar estas flores basándote en las medidas de sus pétalos y sépalos.\n",
    "\n",
    "##### Pasos:\n",
    "\n",
    "1. **Cargar el conjunto de datos**:\n",
    "   - Comience cargando el conjunto de datos Iris. Considere estandarizar los datos, ya que el rendimiento de DBSCAN se ve afectado por la escala del conjunto de datos.\n",
    "\n",
    "2. **Aplicar DBSCAN**:\n",
    "   - Utilice la clase `DBSCAN` de `sklearn.cluster` para agrupar el conjunto de datos. Comience con `eps=0.5` y `min_samples=5` como parámetros iniciales.\n",
    "\n",
    "3. **Ajuste de parámetros**:\n",
    "   - Experimente con diferentes valores de `eps` y `min_samples` para ver cómo afectan a los resultados del clustering. Intente encontrar un equilibrio en el que los clusters tengan un sentido intuitivo y se minimice el ruido.\n",
    "\n",
    "4. **Visualizar los resultados**:\n",
    "   - Visualice el resultado de la agrupación utilizando un gráfico de dispersión. Dado que el conjunto de datos Iris tiene cuatro características, seleccione dos características para un gráfico 2D o explore otras técnicas de visualización para dimensiones superiores.\n",
    "\n",
    "5. **Analizar el resultado**:\n",
    "   - Evalúe el clustering considerando el número de clusters formados y el número de puntos marcados como ruido. Reflexione sobre cómo el enfoque basado en la densidad de DBSCAN se compara con otros métodos de clustering que ha aprendido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f867e988",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# It's often a good idea to scale the data for clustering algorithms\n",
    "X_scaled = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5053ad3a",
   "metadata": {},
   "source": [
    "Antes de aplicar DBSCAN, es crucial estandarizar las características del conjunto de datos. Esto se consigue utilizando `StandardScaler` de `sklearn.preprocessing`. He aquí por qué y cómo funciona:\n",
    "\n",
    "#### ¿Qué hace StandardScaler?\n",
    "\n",
    "- StandardScaler** ajusta las características de su conjunto de datos para que tengan una media de 0 y una desviación estándar de 1. Este proceso se conoce como **standardization** o **z-score normalization**.\n",
    "- La fórmula utilizada es: `(X - media) / desviación_estándar`, donde `X` es el valor original de la característica.\n",
    "\n",
    "#### ¿Por qué normalizar?\n",
    "\n",
    "- **Igualdad de peso**: Muchos algoritmos de clustering, incluyendo DBSCAN, calculan distancias entre puntos de datos. Si las características están en diferentes escalas, las características de mayor escala podrían dominar los cálculos de distancia, sesgando el algoritmo. La normalización garantiza que cada característica contribuya por igual.\n",
    "- **Convergencia mejorada**: Los algoritmos convergen más rápidamente cuando los datos están a una escala similar, lo que es especialmente importante en el caso de grandes conjuntos de datos o modelos complejos.\n",
    "\n",
    "Ejemplo:\n",
    "Imagine un conjunto de datos con dos características: altura en metros y peso en kilogramos. Sin estandarización, el peso influiría desproporcionadamente en la agrupación porque sus valores son numéricamente mayores que los de la altura. StandardScaler mitiga este problema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e926f715",
   "metadata": {},
   "source": [
    "En DBSCAN, hay dos parámetros principales, `eps` y `min_samples`, juegan un papel crucial en la definición de los clusters:\n",
    "\n",
    "#### `eps` (épsilon)\n",
    "- **Definición**: La distancia máxima entre dos muestras para que una sea considerada como vecina de la otra.\n",
    "- **Impacto**: Un valor `eps` más pequeño hace que el algoritmo sea más estricto sobre lo que constituye una vecindad, dando lugar a más conglomerados. Un valor mayor de `eps` aumenta el tamaño de la vecindad, reduciendo potencialmente el número de clusters y fusionando clusters distintos.\n",
    "- **Elección de `eps`**: Seleccionar el valor correcto de `eps` es crítico y a menudo específico del conjunto de datos. El método del gráfico de distancia k puede ayudar a encontrar un punto de partida.\n",
    "\n",
    "#### `min_samples` (muestras mínimas)\n",
    "- **Definición**: El número de muestras en un vecindario para que un punto se considere un punto central. Incluye el propio punto.\n",
    "- **Impacto**: Determina el tamaño mínimo de una región densa. Los valores más altos indican que se necesitan más puntos para formar un conglomerado, lo que puede dar lugar a conglomerados más pequeños y más grandes.\n",
    "- **Elección de `min_muestras`**: Este parámetro refleja el tamaño mínimo de cluster que se espera encontrar significativo. Suele fijarse en función del conocimiento del dominio.\n",
    "\n",
    "Ejemplo:\n",
    "Usar `eps=0.5` y `min_samples=5` significa que un punto necesita al menos 4 vecinos dentro de un radio de 0.5 unidades para ser considerado un punto núcleo de un cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying DBSCAN\n",
    "# eps: The maximum distance between two samples for them to be considered as in the same neighborhood\n",
    "# min_samples: The number of samples in a neighborhood for a point to be considered as a core point\n",
    "dbscan = DBSCAN(eps=0.55, min_samples=5)\n",
    "clusters = dbscan.fit_predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2ccd00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting the clustering result\n",
    "plt.figure(figsize=(10, 7))\n",
    "# The Iris dataset has 4 features, for simplicity, we'll plot two dimensions\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap='viridis', marker='o', edgecolor='k', s=50)\n",
    "plt.title('DBSCAN Clustering of Iris Dataset')\n",
    "plt.xlabel('Feature 1 (scaled)')\n",
    "plt.ylabel('Feature 2 (scaled)')\n",
    "plt.show()\n",
    "\n",
    "# Analyzing the results\n",
    "# -1 in clusters represents outliers detected by DBSCAN\n",
    "n_clusters_ = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
    "n_noise_ = list(clusters).count(-1)\n",
    "\n",
    "print(f'Estimated number of clusters: {n_clusters_}')\n",
    "print(f'Estimated number of noise points: {n_noise_}')\n",
    "\n",
    "# DBSCAN has clustered the data into groups and identified any outliers.\n",
    "# The eps and min_samples parameters were chosen as starting values; tuning these parameters\n",
    "# may yield better clustering results for different datasets or specific requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f319a5",
   "metadata": {},
   "source": [
    "Después de ejecutar DBSCAN, el análisis de la salida ayuda a comprender la estructura del clustering:\n",
    "\n",
    "#### Valores atípicos\n",
    "- Los puntos etiquetados como `-1` son considerados como outliers o ruido por DBSCAN. Esta etiqueta significa que no encajan bien en ningún cluster basándose en los valores de `eps` y `min_samples`.\n",
    "\n",
    "#### Número de clusters\n",
    "- El número de clusters se calcula contando las etiquetas únicas de los clusters. Dado que los valores atípicos se etiquetan con `-1`, se excluyen del recuento de conglomerados.\n",
    "- Este recuento ayuda a evaluar el grado de fragmentación de los datos agrupados, proporcionando información sobre la estructura subyacente de los datos.\n",
    "\n",
    "#### Puntos de ruido\n",
    "- `n_noise_` representa el recuento de estos puntos atípicos. Un número elevado de puntos de ruido puede indicar parámetros demasiado estrictos (`eps` y `min_samples`), lo que sugiere la necesidad de realizar ajustes.\n",
    "\n",
    "Ejemplo:\n",
    "Si DBSCAN devuelve muchos puntos como ruido, podría considerar aumentar `eps` o disminuir `min_samples` para permitir que más puntos se unan a los clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d163cf7",
   "metadata": {},
   "source": [
    "## Determinación de `eps` en DBSCAN con el gráfico de distancia K\n",
    "\n",
    "Cuando se utiliza DBSCAN, uno de los parámetros clave a establecer es `eps`, que define el radio dentro del cual buscar puntos vecinos. Una forma metódica de elegir un valor apropiado de `eps` es utilizando el **grafo de distancia-k**, similar al método del codo utilizado en K-means para encontrar el número óptimo de clusters.\n",
    "\n",
    "### ¿Qué es el gráfico de distancia K?\n",
    "\n",
    "El gráfico de distancia-k ayuda a identificar el valor óptimo de `eps` trazando la distancia de cada punto del conjunto de datos a su k-ésimo vecino más cercano. A continuación se explica cómo utilizarlo:\n",
    "\n",
    "#### Pasos para calcular `eps`:\n",
    "\n",
    "1. **Seleccionar `k`**:\n",
    "   - La elección de `k` suele depender del conjunto de datos y de los objetivos. Un punto de partida común es establecer `k` igual a `min_samples` - 1.\n",
    "\n",
    "2. **Calcular distancias K**:\n",
    "   - Para cada punto, calcular la distancia a su k-ésimo vecino más cercano.\n",
    "\n",
    "3. **3. Trazar el gráfico de distancias K**:\n",
    "   - Ordena los puntos por su distancia k en orden ascendente y traza estas distancias. El gráfico suele mostrar una curva pronunciada o «codo», que indica dónde empiezan a estar los puntos más alejados de sus vecinos.\n",
    "\n",
    "4. **Identificar los «codos »**:\n",
    "   - La coordenada y del punto del codo sugiere un buen valor para `eps`. Esta distancia marca el umbral en el que las densidades de puntos disminuyen bruscamente, señalando la frontera entre los puntos de cluster y el ruido.\n",
    "\n",
    "### ¿Por qué utilizar el gráfico de distancia K?\n",
    "\n",
    "- **Enfoque sistemático**: Proporciona un método basado en datos para establecer `eps`, reduciendo las conjeturas.\n",
    "- **Adaptabilidad**: Esta técnica es versátil, aplicable a través de conjuntos de datos con diferentes características.\n",
    "- **Perspicaz**: Ofrece una visión visual de la estructura de los datos, ayudando a comprender la distribución de la densidad.\n",
    "\n",
    "### Consideraciones:\n",
    "\n",
    "- **Método heurístico**: Aunque útil, este enfoque puede requerir la experimentación con diferentes valores de `k` para refinar la estimación `eps`.\n",
    "- **Densidades variables**: Para conjuntos de datos con densidades significativamente variables, considere la posibilidad de analizar diferentes segmentos por separado o ajustar los parámetros en consecuencia.\n",
    "\n",
    "Al emplear el gráfico de distancia k, puede seleccionar metódicamente un valor de `eps` que mejore el rendimiento del clustering DBSCAN, convirtiéndolo en una herramienta inestimable en su conjunto de herramientas de análisis de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a8ad7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Python Example: Using the k-distance Graph to Determine `eps` for DBSCAN\n",
    "\n",
    "# Importing necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Setting 'k' equal to min_samples - 1 (assuming min_samples is 4 for this example)\n",
    "k = 4 - 1\n",
    "\n",
    "# Initialize NearestNeighbors with n_neighbors as 'k'\n",
    "# Use the 'ball_tree' algorithm for efficient distance computation\n",
    "nbrs = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(X)\n",
    "\n",
    "# Find the distance to the k-th nearest neighbor for each point\n",
    "# distances: Array of distances to k-th nearest neighbor\n",
    "# indices: The indices of the k-th nearest neighbor in the dataset\n",
    "distances, indices = nbrs.kneighbors(X)\n",
    "\n",
    "# Sort the distances\n",
    "sorted_distances = np.sort(distances[:, k-1], axis=0)\n",
    "\n",
    "# Plotting the k-distance Graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(sorted_distances)\n",
    "plt.title('K-Distance Graph')\n",
    "plt.xlabel('Points sorted by distance to k-th nearest neighbor')\n",
    "plt.ylabel('k-th nearest neighbor distance')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Identifying 'eps':\n",
    "# Look for the \"elbow\" in the plotted graph. The y-value at this point is a good starting value for 'eps'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85450844",
   "metadata": {},
   "source": [
    "#### Elegir `min_samples` para DBSCAN\n",
    "\n",
    "Seleccionar el parámetro `min_samples` correcto en DBSCAN es crucial para definir el umbral de densidad que determina qué constituye un cluster. A diferencia del parámetro `eps`, que puede estimarse utilizando el gráfico de distancia k, `min_samples` a menudo requiere un enfoque más experimental, guiado por las características de su conjunto de datos y sus objetivos específicos de clustering.\n",
    "\n",
    "##### Directrices para `min_samples`:\n",
    "\n",
    "1. **Basado en la dimensionalidad**:\n",
    "   - Una regla general es establecer `min_samples` a `D + 1`, donde `D` es la dimensionalidad de su conjunto de datos. Este es el número mínimo necesario para formar una región densa en un espacio de dimensiones «D».\n",
    "   - Para conjuntos de datos con mayor dimensionalidad, considere el uso de `2 * D` o superior para mitigar la maldición de la dimensionalidad.\n",
    "\n",
    "2. **Consideraciones sobre la densidad y el tamaño de los datos**:\n",
    "   - Para conjuntos de datos más grandes con conglomerados densos esperados, un `min_samples` más alto podría ser apropiado para evitar identificar demasiados conglomerados pequeños, posiblemente insignificantes.\n",
    "   - Por el contrario, para conjuntos de datos más pequeños o en los que las distinciones de conglomerados más finas son valiosas, un `min_samples` más bajo puede ayudar a identificar estructuras matizadas.\n",
    "\n",
    "3. **Sensibilidad al ruido**:\n",
    "   - Aumentar `min_samples` mejora la resistencia del algoritmo al ruido al exigir más puntos para constituir un conglomerado, clasificando así más puntos como valores atípicos.\n",
    "   - Ajuste `min_muestras` en función de su tolerancia al ruido y a los valores atípicos en sus resultados de clustering.\n",
    "\n",
    "##### Enfoque práctico de la selección:\n",
    "\n",
    "- **La experimentación es la clave**: Comience con la directriz basada en la dimensionalidad y ajústela en función de la calidad y relevancia de la agrupación observada.\n",
    "- **Evaluar los resultados de la agrupación**: Utilice el conocimiento del dominio y, si procede, métricas internas como las puntuaciones de silueta para evaluar la validez de los clusters. Sin embargo, hay que tener cuidado con las puntuaciones de silueta, ya que pueden no captar completamente la calidad de los clusters no esféricos.\n",
    "- **Refinamiento iterativo**: Ajuste `min_samples` en función de los resultados iniciales. Un número excesivo de valores atípicos puede sugerir la reducción de `min_samples`, mientras que un número excesivo de conglomerados pequeños o una fragmentación excesiva pueden indicar la necesidad de aumentarlo.\n",
    "\n",
    "##### Ejemplo:\n",
    "\n",
    "Si se trata de un conjunto de datos de 10 dimensiones, comience con `min_samples` fijado en 11 (`D + 1`). Si los conglomerados parecen demasiado fragmentados, considere la posibilidad de aumentar `min_samples` a 20 (`2 * D`) y vuelva a evaluar los resultados.\n",
    "\n",
    "##### Conclusión:\n",
    "\n",
    "Determinar el valor óptimo de `min_muestras` implica equilibrar la sensibilidad del algoritmo al ruido, la granularidad deseada de los conglomerados y las propiedades inherentes del conjunto de datos. Empezar con directrices informadas y refinarlas a través de pruebas le conducirá a resultados de clustering DBSCAN más significativos y robustos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca85cb17",
   "metadata": {},
   "source": [
    "## Ejercicio: Implementación de DBSCAN Clustering en el conjunto de datos Wine\n",
    "\n",
    "### Objetivo:\n",
    "En este ejercicio, aplicará el algoritmo de clustering DBSCAN al conjunto de datos Wine, explorando cómo ajustar sus parámetros eficazmente e interpretar los resultados. El conjunto de datos Wine presenta un desafío de clustering más complejo, lo que lo convierte en una excelente oportunidad para practicar DBSCAN.\n",
    "\n",
    "### Conjunto de datos:\n",
    "**Conjunto de datos Vino**: Contiene los resultados de un análisis químico de vinos cultivados en la misma región de Italia pero derivados de tres cultivares diferentes. El conjunto de datos incluye 13 medidas diferentes tomadas para diferentes constituyentes encontrados en los tres tipos de vino.\n",
    "\n",
    "### Pasos:\n",
    "\n",
    "1. **Cargar el conjunto de datos**:\n",
    "   - Utilice `sklearn.datasets.load_wine()` para importar el conjunto de datos Wine.\n",
    "\n",
    "2. **Preprocesar los datos**:\n",
    "   - Estandarizar el conjunto de datos utilizando `StandardScaler` para asegurar que cada característica contribuye por igual a los cálculos de distancia.\n",
    "\n",
    "3. **Aplicar DBSCAN**:\n",
    "   - Implementar DBSCAN desde `sklearn.cluster` con valores iniciales de `eps=0.5` y `min_samples=5`. Ajuste estos parámetros según sea necesario en función de los resultados del clustering.\n",
    "\n",
    "4. **Ajuste de parámetros**:\n",
    "   - Experimente variando `eps` y `min_samples` para ver su impacto en el clustering. \n",
    "   - Considere emplear el gráfico de distancia k para determinar un valor `eps` más apropiado.\n",
    "\n",
    "5. **Evaluar y analizar los resultados**:\n",
    "   - Determinar el número de clusters y puntos de ruido identificados por DBSCAN.\n",
    "   - Visualice la agrupación utilizando un gráfico de dispersión u otras técnicas de visualización adecuadas. Considere reducir la dimensionalidad con PCA para la visualización si es necesario.\n",
    "   - Opcionalmente, compare los clusters derivados de DBSCAN con los cultivares de vino reales para evaluar la calidad del clustering.\n",
    "\n",
    "6. **Reflexión y Discusión**:\n",
    "   - Reflexione sobre los ajustes de los parámetros DBSCAN que produjeron los resultados de clustering más significativos.\n",
    "   - Discuta la eficacia de DBSCAN para el conjunto de datos del vino y cualquier idea obtenida de la agrupación.\n",
    "\n",
    "### Resultados esperados:\n",
    "\n",
    "Al completar este ejercicio, deberías:\n",
    "\n",
    "- Adquirir experiencia práctica en la aplicación de DBSCAN a un conjunto de datos complejo.\n",
    "- Comprender la importancia de los parámetros `eps` y `min_samples` y cómo ajustarlos.\n",
    "- Aprender a evaluar la calidad de los conglomerados formados por DBSCAN e interpretar la capacidad del algoritmo para manejar el ruido y los valores atípicos.\n",
    "- Obtener información sobre la composición química de diferentes variedades de vino a gracias a los resultados de la agrupación no supervisada.\n",
    "\n",
    "Este ejercicio práctico profundizará sus conocimientos sobre el clustering basado en la densidad, sus aplicaciones en el análisis de conjuntos de datos del mundo real y cultura enóloga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a81e668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa84e04",
   "metadata": {},
   "source": [
    "## 3. Perfilado de clusters\n",
    "\n",
    "Después de identificar los clusters dentro de un conjunto de datos utilizando algoritmos de clustering como K-means, DBSCAN o Clustering Jerárquico, el siguiente paso crítico es el **Perfilado de Clusters**. Este proceso consiste en analizar cada cluster para comprender sus características definitorias y en qué se diferencia de otros clusters. La creación de perfiles nos permite interpretar los clústeres de una manera significativa, lo que a menudo conduce a ideas útiles.\n",
    "\n",
    "[Artículo sobre creación de clusters y algoritmos de cluster](https://medium.com/analytics-vidhya/clustering-and-profiling-customers-using-k-means-9afa4277427)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a8374",
   "metadata": {},
   "source": [
    "### 3.1 Profundización en el perfilado de clústeres\n",
    "\n",
    "#### La importancia del perfilado de clústeres\n",
    "\n",
    "La creación de perfiles de conglomerados sirve para múltiples propósitos críticos en el análisis de datos:\n",
    "\n",
    "- Caracterización perspicaz**: Transforma los clusters abstractos en grupos comprensibles con características específicas, haciendo que el análisis de datos sea procesable.\n",
    "- Estrategias específicas**: Las empresas y organizaciones pueden desarrollar estrategias a medida para los distintos segmentos identificados mediante la agrupación, optimizando recursos y esfuerzos.\n",
    "- Mejora de la comunicación**: Los perfiles claramente definidos permiten comunicar mejor los resultados a las partes interesadas, lo que facilita la toma de decisiones estratégicas.\n",
    "\n",
    "#### Estrategias para un perfilado eficaz de los clusters\n",
    "\n",
    "Para perfilar eficazmente los clusters, se suele emplear una combinación de técnicas de análisis estadístico y visualización. He aquí algunas estrategias clave:\n",
    "\n",
    "1. **Resúmenes estadísticos**:\n",
    "   - **Estadística descriptiva**: Calcule la media, la mediana, la desviación estándar y otras estadísticas para las características numéricas dentro de cada conglomerado. Esto ayuda a comprender la tendencia central y la dispersión de los puntos de datos.\n",
    "   - **Análisis de la distribución**: Evaluar la distribución de los datos dentro de los conglomerados para identificar asimetrías, valores atípicos o patrones peculiares que definen cada conglomerado.\n",
    "\n",
    "2. **Análisis de características**:\n",
    "   - **Características significativas**: Identificar qué características contribuyen significativamente a definir un conglomerado. Esto puede hacerse mediante pruebas de hipótesis o examinando la importancia de las características en los árboles de decisión.\n",
    "   - **Importancia relativa**: Comparar la importancia de las características entre clusters para entender qué hace que cada cluster sea distinto.\n",
    "\n",
    "3. **Técnicas de visualización\n",
    "   - **Cuadros e histogramas**: Visualizar la distribución de las características dentro de cada conglomerado para comprender su dispersión y sus valores centrales.\n",
    "   - **Gráficos de radar**: Útiles para datos multidimensionales, permiten comparar los promedios de los clusters a través de varias características.\n",
    "   - **Mapas de calor**: Compara clusters contra características clave para visualizar cómo cada cluster difiere en términos de valores de características.\n",
    "\n",
    "4. **Reducción de la dimensionalidad**:\n",
    "   - **PCA (Análisis de Componentes Principales)**: Utilice PCA para reducir la dimensionalidad de sus datos, facilitando la visualización e interpretación de los clusters en un espacio 2D o 3D.\n",
    "   - **t-SNE o UMAP**: Para conjuntos de datos más complejos, t-SNE o UMAP pueden proporcionar una visualización matizada de los clusters en un espacio de dimensiones reducidas.\n",
    "\n",
    "5. **Tabulación cruzada para datos categóricos**:\n",
    "   - Analice cómo se distribuyen las variables categóricas en los conglomerados para encontrar patrones o anomalías. Esto puede revelar si ciertas categorías predominan en conglomerados específicos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75705e",
   "metadata": {},
   "source": [
    "### 3.2 Técnicas Avanzadas para Perfilar Clusters\n",
    "\n",
    "Una vez identificados los clusters en el conjunto de datos, el siguiente paso consiste en comprenderlos y perfilarlos en profundidad. La creación de perfiles consiste en examinar las características y los rasgos clave de cada cluster para descubrir nuevos puntos de vista. Esta sección explora técnicas avanzadas para perfilar clusters, centrándose en resúmenes estadísticos y métodos de visualización.\n",
    "\n",
    "#### Resúmenes estadísticos para el perfilado de clusters\n",
    "\n",
    "Los resúmenes estadísticos proporcionan una visión cuantitativa de las propiedades de cada cluster. He aquí cómo enfocar esto:\n",
    "\n",
    "1. **Tendencia central y dispersión**: Para cada cluster, calcule medidas de tendencia central (media, mediana) y dispersión (varianza, desviación estándar) para todas las características. Esto ayuda a comprender los valores típicos y la dispersión de los datos dentro de cada conglomerado.\n",
    "\n",
    "2. **Distribución**: Evalúe la distribución de las características dentro de los conglomerados mediante histogramas o estimaciones de la densidad del núcleo. Esto puede poner de relieve la asimetría o la presencia de valores atípicos dentro de los conglomerados.\n",
    "\n",
    "3. **Comparación**: Compare estas medidas estadísticas entre clusters para identificar rasgos distintivos. Por ejemplo, si un conglomerado tiene valores medios significativamente más altos para una característica particular, esta característica podría ser clave para definir ese conglomerado.\n",
    "\n",
    "#### Técnicas avanzadas de visualización\n",
    "\n",
    "Las herramientas visuales pueden dar vida a los datos, haciendo que las relaciones complejas sean más comprensibles. A continuación se presentan algunas técnicas avanzadas de visualización para el perfilado de clusters:\n",
    "\n",
    "1. **Gráficos de caja**: Utilice gráficos de caja para visualizar la distribución de características dentro de cada cluster. Esto puede resaltar rápidamente las diferencias en los valores medios y la variabilidad entre clusters.\n",
    "\n",
    "2. **Gráficos de radar**: Los gráficos de radar son eficaces para los datos multidimensionales, ya que permiten trazar los valores medios de las características de cada conglomerado en un gráfico circular. Esto facilita ver qué características definen cada conglomerado.\n",
    "\n",
    "3. **Mapas de calor**: Los mapas de calor pueden utilizarse para visualizar los valores medios de las características en los distintos conglomerados. Esto puede ayudar a identificar qué rasgos son los más característicos de cada cluster.\n",
    "\n",
    "4. **Reducción de la dimensionalidad**: Aplique PCA (Análisis de Componentes Principales) o t-SNE para una visualización 2D o 3D de los clusters. Esto puede ayudar a visualizar lo bien separados que están los clusters en un espacio de dimensiones reducidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e002e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load a dataset\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target  # Assuming 'y' are cluster labels from a previous clustering\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to a DataFrame for easier manipulation\n",
    "df = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
    "df['Cluster'] = y  # Add cluster labels to the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ab406",
   "metadata": {},
   "source": [
    "#### Perspectivas de los perfiles de clúster\n",
    "\n",
    "##### 1. Resúmenes estadísticos\n",
    "\n",
    "Los resúmenes estadísticos proporcionan una comprensión básica de cada cluster calculando la media y la desviación estándar de cada característica dentro de cada cluster. Este análisis revela\n",
    "\n",
    "- **Tendencias centrales**: Los valores medios dan una idea de las características «medias» de cada conglomerado, ayudando a identificar las tendencias centrales de las características que definen cada grupo.\n",
    "- **Variabilidad**: Los valores de la desviación estándar iluminan la variabilidad o dispersión de las características dentro de cada conglomerado. Una desviación típica elevada indica que los puntos de datos de ese grupo están repartidos en una amplia gama de valores, mientras que una desviación típica baja sugiere que los puntos están agrupados en torno a la media.\n",
    "\n",
    "Estos resúmenes son cruciales para comprender el comportamiento general de cada conglomerado e identificar los rasgos distintivos que diferencian a cada conglomerado de los demás.\n",
    "\n",
    "##### 2. Técnicas de visualización: Gráficos de caja\n",
    "\n",
    "Los gráficos de caja son una herramienta inestimable para resumir visualmente la distribución de cada característica dentro de los conglomerados. A partir de los gráficos de caja, puede obtener información sobre:\n",
    "\n",
    "- **Dispersión de la distribución**: La amplitud con la que los puntos de datos de cada conglomerado se distribuyen en el rango de la característica. Esto puede resaltar clusters con amplias variaciones en ciertas características.\n",
    "- **Outliers**: Puntos que caen fuera del rango típico de los datos, mostrados como puntos más allá de los 'bigotes' de los gráficos de caja. Los valores atípicos pueden indicar anomalías dentro de los conglomerados o características con alta variabilidad.\n",
    "- **Valores medios**: La línea dentro de cada recuadro representa el valor medio de la característica en ese conglomerado, ofreciendo una indicación visual de la tendencia central que complementa los valores medios de los resúmenes estadísticos.\n",
    "- **Rango intercuartílico (IQR)**: La longitud de la caja muestra el IQR, el 50% medio de los valores de la característica. Un recuadro más largo indica una mayor dispersión de los valores medios.\n",
    "\n",
    "Al examinar los diagramas de caja de cada característica en los conglomerados, podrá comparar visualmente los conglomerados y comprender qué características contribuyen más a las diferencias entre los conglomerados. Este método de visualización da vida a los resúmenes estadísticos, proporcionando una forma clara e inmediata de comprender las características que definen cada conglomerado.\n",
    "\n",
    "Juntos, los resúmenes estadísticos y los gráficos de caja forman una imagen completa de sus conglomerados, combinando el análisis cuantitativo con la exploración visual para profundizar en su comprensión de la estructura de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db9646f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Statistical Summaries\n",
    "# Compute mean and standard deviation for each cluster and feature\n",
    "cluster_summary = df.groupby('Cluster').agg(['mean', 'std']).transpose()\n",
    "\n",
    "# 2. Visualization Techniques\n",
    "# Box plots for each feature across clusters\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(data.feature_names):\n",
    "    plt.subplot(4, 4, i + 1)  # Adjust grid size based on the number of features\n",
    "    sns.boxplot(x='Cluster', y=feature, data=df)\n",
    "    plt.title(feature)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198d7fea",
   "metadata": {},
   "source": [
    "#### Visualización de perfiles de conglomerados con gráficos de radar\n",
    "\n",
    "En el ámbito del análisis de conglomerados, la comprensión de las características únicas de cada grupo identificado es crucial para obtener información útil. Una de las formas más eficaces de visualizar y comparar los perfiles multidimensionales de los clusters es a través de los gráficos de radar. Estos gráficos ofrecen una visión completa de las diferencias entre los clusters en función de sus características, lo que permite comprender mejor la estructura subyacente de los datos.\n",
    "\n",
    "##### Objetivo del ejercicio\n",
    "\n",
    "El objetivo de este ejercicio es emplear gráficos de radar para visualizar los valores medios de las características a través de diferentes conglomerados. Al trazar el perfil de cada conglomerado en un gráfico de radar, podemos evaluar visualmente las características definitorias de cada grupo, destacando sus similitudes y diferencias en un único gráfico intuitivo.\n",
    "\n",
    "##### ¿Por qué gráficos de radar?\n",
    "\n",
    "- **Comparación multidimensional**: Los gráficos de radar nos permiten comparar simultáneamente varias variables cuantitativas, lo que los hace ideales para la elaboración de perfiles de conglomerados.\n",
    "- **Identificar atributos únicos**: Al observar la forma que forma cada conglomerado en el gráfico, podemos identificar rápidamente qué características son las más distintivas de cada conglomerado.\n",
    "- **Mayor interpretabilidad**: Estos gráficos facilitan la comunicación de datos multidimensionales complejos de una forma accesible tanto para los interesados técnicos como para los no técnicos.\n",
    "\n",
    "En el siguiente ejemplo de Python, mostraremos cómo crear un gráfico de radar para los clústeres obtenidos del conjunto de datos Wine mediante DBSCAN. Esta visualización no sólo mejorará nuestra comprensión de los clusters, sino que también mostrará la aplicación práctica de técnicas avanzadas de visualización de datos en Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8a5914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Load and scale the Wine dataset\n",
    "wine_data = load_wine()\n",
    "X = wine_data.data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Assume DBSCAN clustering has been applied to obtain cluster labels\n",
    "# For demonstration, applying DBSCAN directly here\n",
    "dbscan = DBSCAN(eps=2, min_samples=5)\n",
    "clusters = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "# Adding cluster labels to the dataframe\n",
    "df = pd.DataFrame(X_scaled, columns=wine_data.feature_names)\n",
    "df['Cluster'] = clusters\n",
    "\n",
    "# Prepare data for the radar chart\n",
    "features = wine_data.feature_names\n",
    "means = df.groupby('Cluster').mean().reset_index()\n",
    "\n",
    "# Setup for radar chart\n",
    "labels = np.array(features)\n",
    "num_vars = len(labels)\n",
    "\n",
    "# Create angles for each axis in the plot (with one extra for the closing loop)\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "angles += angles[:1]  # Close the loop\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
    "\n",
    "# Plot each cluster's average feature values on the radar chart\n",
    "for i, row in means.iterrows():\n",
    "    data = row.drop('Cluster').values.tolist()\n",
    "    data += data[:1]  # Repeat the first value to close the loop\n",
    "    ax.plot(angles, data, label=f'Cluster {row.Cluster}')\n",
    "    ax.fill(angles, data, alpha=0.25)\n",
    "\n",
    "# Add feature names to each axis\n",
    "ax.set_xticks(angles[:-1])  # Set ticks to the number of features\n",
    "ax.set_xticklabels(labels, rotation='vertical')  # Ensure the labels match feature names\n",
    "\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a310282",
   "metadata": {},
   "source": [
    "#### Introducción al PCA para la reducción de la dimensionalidad y la visualización\n",
    "\n",
    "En nuestra exploración del perfilado y visualización de clusters, hemos encontrado conjuntos de datos con múltiples características que pueden hacer que el análisis sea complejo, especialmente cuando se intenta visualizar o encontrar patrones en un espacio de alta dimensionalidad. Para hacer frente a este desafío, introducimos una potente técnica conocida como **Análisis de Componentes Principales (PCA)**.\n",
    "\n",
    "##### ¿Qué es el PCA?\n",
    "\n",
    "El PCA es un procedimiento estadístico que transforma un conjunto de observaciones de variables posiblemente correlacionadas en un conjunto de valores de variables linealmente no correlacionadas denominadas componentes principales. Esta transformación se define de forma que el primer componente principal tenga la mayor varianza posible (que represente la mayor parte posible de la variabilidad de los datos), y cada componente sucesivo, a su vez, tenga la mayor varianza posible bajo la restricción de que sea ortogonal a los componentes precedentes.\n",
    "\n",
    "##### ¿Por qué utilizar el PCA?\n",
    "\n",
    "- **Simplificación**: El PCA reduce la dimensionalidad de los datos, simplificando la complejidad de los conjuntos de datos de alta dimensionalidad y conservando al mismo tiempo la variación presente en el conjunto de datos.\n",
    "- **Visualización**: Al reducir el conjunto de datos a 2 ó 3 componentes principales, el PCA nos permite visualizar los datos en un espacio 2D o 3D, lo que facilita la observación de patrones, agrupaciones y relaciones.\n",
    "- **Eficacia**: La reducción del número de variables puede mejorar la eficiencia de otros análisis computacionales, como los algoritmos de agrupamiento.\n",
    "\n",
    "##### PCA en Cluster Profiling:\n",
    "\n",
    "En el contexto del perfilado de conglomerados, el PCA es particularmente útil para visualizar cómo los conglomerados formados en un espacio de alta dimensión se distribuyen en dimensiones reducidas. Esto puede proporcionar información valiosa sobre la separación y las características de los clusters que podrían no ser evidentes en el espacio original.\n",
    "\n",
    "##### Próxima exploración detallada:\n",
    "\n",
    "Aunque aquí hemos introducido brevemente el PCA y demostrado su aplicación en la visualización de conglomerados, en la próxima sección se tratará una exploración más detallada del PCA. Profundizaremos en las matemáticas que hay detrás del PCA, cómo interpretar sus componentes y discutiremos las mejores prácticas para su aplicación en el análisis de datos.\n",
    "\n",
    "Manténgase en sintonía para una comprensión más profunda de PCA y cómo se puede aprovechar para descubrir patrones ocultos en sus datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c5f60d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4. PCA for Dimensionality Reduction and Visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=df['Cluster'], palette='viridis', alpha=0.7)\n",
    "plt.title('Clusters in PCA-reduced Space')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# This script demonstrates statistical summaries, box plots for feature distributions,\n",
    "# a radar chart for comparing cluster averages, and PCA for visualizing clusters in reduced dimensionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc55dd56",
   "metadata": {},
   "source": [
    "## 4. Reducción de la dimensionalidad con PCA\n",
    "\n",
    "Cuando trabajamos con datos, a veces tenemos demasiados detalles (o «características») para analizarlos todos a la vez. Imagínese intentar comprender a una persona basándose en miles de datos sobre ella. Puede resultar abrumador. Ahí es donde entra en juego el Análisis de Componentes Principales (ACP). Es como encontrar las historias más importantes que resumen todos esos hechos sin perder la esencia.\n",
    "\n",
    "**¿Qué es el PCA?**\n",
    "\n",
    "PCA es una forma de simplificar tus datos. Si tus datos son una sala enorme y abarrotada de información, PCA te ayuda a encontrar los pocos interlocutores clave que te dicen casi todo lo que necesitas saber. Lo hace combinando muchos detalles en unas pocas grandes ideas, que llamamos «componentes principales».\n",
    "\n",
    "**Cómo funciona el PCA**\n",
    "\n",
    "- **Haciendo que todo sea comparable**: En primer lugar, nos aseguramos de que ningún detalle domine por error, escalando todos los datos. Es como asegurarnos de que todo el mundo habla a un volumen que podamos comparar.\n",
    "- **Encontrar relaciones**: A continuación, el ACP examina cómo se relacionan estos detalles entre sí: qué hechos van de la mano y cuáles no.\n",
    "- **Elegir a los líderes**: A partir de esto, PCA encuentra los «líderes» - los componentes principales que representan los patrones más significativos en los datos.\n",
    "- **Elegir las historias importantes**: Por último, elegimos los principales líderes. Estos líderes nos cuentan las historias más importantes de nuestros datos.\n",
    "\n",
    "**¿Por qué utilizar el PCA?**\n",
    "\n",
    "- **Menos ruido**: El PCA nos ayuda a centrarnos en el panorama general e ignorar los detalles ruidosos y sin importancia.\n",
    "- **Más fácil de ver y entender**: Con menos detalles, es más fácil visualizar y comprender los datos.\n",
    "- **Procesamiento más rápido**: Menos información significa que nuestro análisis de datos puede ejecutarse más rápido.\n",
    "\n",
    "**¿Dónde podemos utilizar el PCA?**\n",
    "\n",
    "- **Simplificación de datos**: Cuando tenemos demasiada información, PCA nos ayuda a mantener sólo lo que es realmente importante.\n",
    "- **Encontrar patrones**: El ACP puede ayudar a descubrir patrones en los datos que antes no eran obvios.\n",
    "- **Mejorar la velocidad**: Antes de ejecutar análisis complejos o aprendizaje automático, el uso de PCA puede agilizar las cosas simplificando primero los datos.\n",
    "\n",
    "[PCA link](https://www.youtube.com/watch?v=FgakZw6K1QQ&ab_channel=StatQuestwithJoshStarmer)\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*ba0XpZtJrgh7UpzWcIgZ1Q.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79fc856",
   "metadata": {},
   "source": [
    "## 4.1 Dominar el PCA\n",
    "\n",
    "Imagina que tienes una caja enorme llena de juguetes. Para entender lo que hay dentro, decides ordenarlos. Pero en lugar de mirar cada juguete individualmente, encuentra una forma más sencilla de organizarlos por sus características más notables, como el tamaño o el color. Esto es en cierto modo lo que hace el ACP (Análisis de Componentes Principales) con los datos. Encuentra lo más importante en una gran cantidad de información y nos ayuda a comprender el panorama general sin perdernos en los detalles.\n",
    "\n",
    "### ¿De qué trata el PCA?\n",
    "\n",
    "El PCA es una herramienta que nos ayuda a ver patrones en los datos cuando hay demasiado que ver. Es como utilizar un mapa para navegar por una ciudad en lugar de deambular por cada calle. Funciona de la siguiente manera\n",
    "\n",
    "1. **Poner todo en la misma página**: Al igual que es más fácil comparar las cosas cuando están en una escala similar, PCA comienza por asegurarse de que todas las partes de nuestros datos se pueden comparar de manera justa. Este paso se denomina estandarización.\n",
    "   \n",
    "2. **Encontrar lo que destaca**: Imagine que puede estirar una banda elástica alrededor de todos sus puntos de datos. El lado más largo de la banda señala lo que es más diferente entre ellos. El ACP busca estas diferencias, que llamamos «componentes principales». El primer componente principal muestra la mayor diferencia, el segundo encuentra la siguiente gran diferencia, y así sucesivamente.\n",
    "\n",
    "3. **Elegir la mejor vista**: A menudo, sólo necesitamos los primeros componentes principales para hacernos una buena idea de qué tratan nuestros datos. Es como elegir las mejores fotos de un viaje para contar su historia sin mostrar todas y cada una de las imágenes.\n",
    "\n",
    "### ¿Por qué usamos PCA?\n",
    "\n",
    "- **Para simplificar**: El ACP nos ayuda a centrarnos en las grandes ideas de nuestros datos, facilitando su exploración y comprensión.\n",
    "- **Para ver patrones**: Al reducir el desorden, el ACP puede revelar patrones que de otro modo pasarían desapercibidos.\n",
    "- **Para acelerar las cosas**: Los análisis que llevarían mucho tiempo con muchos datos pueden ejecutarse más rápido después de que el PCA simplifique los datos.\n",
    "\n",
    "### ¿Adónde puede llevarnos el PCA?\n",
    "\n",
    "- **Detectar grupos**: PCA puede mostrarnos agrupaciones naturales en nuestros datos que podríamos querer explorar más a fondo.\n",
    "- **Reducir el ruido**: A veces, los datos tienen mucha variación aleatoria que puede distraernos. PCA ayuda a centrarse en los patrones que importan.\n",
    "- **Preparación para otros análisis**: Con datos más sencillos, estamos mejor preparados para utilizar otras herramientas y técnicas para obtener información más profunda.\n",
    "\n",
    "### Llevar el PCA a la práctica\n",
    "\n",
    "Ahora que sabemos por qué PCA es útil, nos sumergiremos en cómo usarlo con Python en nuestros próximos pasos. Tomaremos nuestros datos complejos, aplicaremos PCA para resaltar las características más reveladoras, y visualizaremos la esencia de nuestros datos de una manera que sea fácil de entender y compartir.\n",
    "\n",
    "Manténgase en sintonía a medida que desglosamos estos conceptos en ejercicios prácticos, paso a paso, que le permitirán dominar PCA en sus proyectos de datos.\n",
    "\n",
    "[Useful matherial on PCA](https://medium.com/mlearning-ai/principal-component-analysis-pca-simplified-22ef97b0e1dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd46c8e",
   "metadata": {},
   "source": [
    "## PCA práctico con el conjunto de datos Iris\n",
    "\n",
    "El análisis de componentes principales (PCA) es una potente técnica de reducción dimensional que mejora la visualización de datos y la eficiencia computacional. He aquí una guía paso a paso para realizar PCA en el conjunto de datos Iris, un popular conjunto de datos para tareas de aprendizaje automático que implican la clasificación de especies florales basada en medidas físicas.\n",
    "\n",
    "### Pasos para realizar el PCA\n",
    "\n",
    "1. **Estandarización**: Asegúrese de que cada característica contribuye por igual mediante la normalización de los datos.\n",
    "\n",
    "2. **Cálculo de la matriz de covarianza**: Comprender cómo varían las características entre sí.\n",
    "\n",
    "3. **Descomposición propia**: Identificar las direcciones (componentes principales) que maximizan la varianza en los datos.\n",
    "\n",
    "4. **Ordenar por valores propios**: Ordenar los componentes principales por su importancia.\n",
    "\n",
    "5. **Elegir componentes principales**: Seleccione un subconjunto de componentes principales para su posterior análisis.\n",
    "\n",
    "### PCA paso a paso en el conjunto de datos Iris\n",
    "\n",
    "**1. 1. Cargar y preparar los datos:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f3b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Visualize the data before standardization\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.kdeplot(data=pd.DataFrame(X, columns=feature_names))\n",
    "plt.title('Before Standardization')\n",
    "plt.xlabel('Feature Value')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(feature_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1e2df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Normalización\n",
    "# Al analizar datos, a menudo encontramos características medidas en unidades diferentes (por ejemplo, kilogramos, kilómetros).\n",
    "# Comparar directamente estas características sin estandarización puede llevar a suposiciones y resultados incorrectos.\n",
    "# Por ejemplo, sin normalización, un algoritmo podría interpretar erróneamente que 100 gramos es mayor que 1 kilogramo.\n",
    "# La estandarización ajusta las características para que tengan una media de 0 y una varianza de 1, lo que hace que diferentes unidades sean directamente comparables.\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Scale the data to mean=0 and variance=1\n",
    "\n",
    "# Visualizar después de estandarizar\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.kdeplot(data=pd.DataFrame(X_scaled, columns=feature_names))\n",
    "plt.title('After Standardization')\n",
    "plt.xlabel('Feature Value')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(feature_names)\n",
    "plt.show()\n",
    "\n",
    "# Nota: La estandarización puede no ser necesaria si sus variables ya están en una escala similar. Sin embargo, para PCA,\n",
    "# la normalización garantiza que cada característica contribuya por igual al análisis, evitando que las características con mayores\n",
    "# escalas mayores dominen los componentes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5af35e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2: Calcular la matriz de covarianza\n",
    "# La matriz de covarianza es como una herramienta matemática que nos ayuda a entender\n",
    "# cómo se mueven juntas las diferentes características de nuestro conjunto de datos. ¿Aumenta una característica\n",
    "# cuando lo hace otra? ¿O disminuye? ¿O tal vez no hay un patrón claro?\n",
    "# Eso es lo que queremos averiguar con la matriz de covarianza.\n",
    "\n",
    "# Varianza - Piensa en la varianza como una forma de medir la dispersión de un conjunto de datos.\n",
    "# Imagina que estás mirando las alturas de un grupo de personas. Si todos tienen\n",
    "# la misma altura, la varianza es baja. Pero si hay algunas personas muy altas y otras\n",
    "# personas muy bajas, la varianza es alta. Es una manera de cuantificar cuánto los datos\n",
    "# varían de la media.\n",
    "\n",
    "# Covarianza - Ahora, digamos que estamos viendo tanto la altura como el peso de las personas.\n",
    "# La covarianza nos dice si las personas más altas tienden a ser más pesadas (covarianza positiva), si\n",
    "# las personas más altas tienden a ser más ligeras (covarianza negativa), o si no hay una tendencia clara\n",
    "# entre la altura y el peso (covarianza cero).\n",
    "\n",
    "# Cálculo de la matriz de covarianza\n",
    "matriz_cov = np.cov(X_escalada.T) # Aquí, estamos calculando la matriz de covarianza para nuestros datos estandarizados.\n",
    "# Esta matriz nos dirá la covarianza entre pares de características. Por ejemplo,\n",
    "# cov_matrix[i, j] nos dice la covarianza entre la característica i y la característica j. Si miramos\n",
    "# en la diagonal de esta matriz (donde i es igual a j), encontramos la varianza de cada característica.\n",
    "\n",
    "# Visualizando el concepto de varianza y covarianza (visualización opcional no proporcionada en el código)\n",
    "# Imagina que pintas tus datos en un gráfico, con una característica en el eje x y otra en el eje y.\n",
    "# La dispersión de los datos en la dirección de cada eje representa la varianza, y cómo los datos\n",
    "# se extiende diagonalmente a través del gráfico puede darte una idea de la covarianza. Una pendiente positiva\n",
    "# indica covarianza positiva, una pendiente negativa indica covarianza negativa, y una nube de\n",
    "# puntos sin pendiente discernible indica covarianza cero.\n",
    "\n",
    "# Nota: El código de trazado para la visualización de la varianza y la covarianza no se incluye aquí.\n",
    "# Para visualizar estos conceptos, puede trazar características individuales entre sí y observar\n",
    "# la dispersión y direccionalidad de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33806475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: Descomposición Eigen\n",
    "# Después de averiguar cómo las características en nuestro conjunto de datos se mueven entre sí a través de la matriz de covarianza,\n",
    "# el siguiente gran paso es la descomposición eigen. Esto puede sonar complicado, pero se trata de encontrar\n",
    "# valores especiales y vectores de nuestra matriz de covarianza que nos dicen mucho acerca de nuestros datos.\n",
    "\n",
    "# Valores propios y vectores propios - ¿Qué son?\n",
    "# Imagina que tienes una forma hecha de un material flexible, y puedes estirarla en diferentes direcciones.\n",
    "# Los vectores propios son direcciones en las que esta forma se puede estirar o aplastar, mientras que los valores propios te dicen\n",
    "# cuánto se estira o aplasta en esas direcciones. En nuestros datos, los vectores propios apuntan a direcciones de\n",
    "# varianza máxima (donde los datos están más dispersos), y los valores propios nos dicen cómo se extienden los datos es\n",
    "# en estas direcciones.\n",
    "\n",
    "# ¿Por qué son importantes?\n",
    "# Los vectores propios nos guían para entender los componentes principales (los nuevos ejes de nuestro conjunto de datos) tras el ACP.\n",
    "# Estos componentes son nuevas formas de ver nuestros datos, donde cada paso hacia abajo en componentes nos muestra el siguiente\n",
    "# mejor ángulo para ver nuestra dispersión de datos. Los valores propios nos ayudan a decidir qué componentes principales son los más\n",
    "# importantes mostrando la cantidad de varianza capturada por cada componente.\n",
    "\n",
    "eigen_values, eigen_vectors = np.linalg.eig(cov_matrix) # Descomponer la matriz de covarianza para encontrar eigenvalores y eigenvectores.\n",
    "\n",
    "# Sugerencia de visualización:\n",
    "# Para visualizar eigenvectores y eigenvalores, imagine que traza sus datos estandarizados en un gráfico. Dibuje una\n",
    "# flecha desde el centro de este gráfico en la dirección donde los datos se extienden más. Esta flecha es\n",
    "# su primer vector propio. La longitud de la flecha representa el valor # propio - una flecha más larga significa más # varianza en esa dirección.\n",
    "# varianza en esa dirección. La segunda flecha (vector propio) se dibuja perpendicular a la primera, de nuevo\n",
    "# mostrando otra dirección de dispersión de los datos, pero menor que la primera. Esto nos ayuda a visualizar la\n",
    "# datos multidimensionales en dos dimensiones, simplificando nuestra comprensión de su estructura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aebfbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la matriz de covarianza y realizar la descomposición eigen\n",
    "cov_matrix = np.cov(X_scaled.T)\n",
    "eigen_values, eigen_vectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Trazar los datos estandarizados\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], alpha=0.7)\n",
    "\n",
    "# Calcular la media de cada característica para el origen de los vectores propios\n",
    "mean_vec = np.mean(X_scaled, axis=0)\n",
    "\n",
    "# Trazar los vectores propios\n",
    "for i in range(len(eigen_values)):\n",
    "    # Los valores propios escalan los vectores propios (para propósitos de visualización)\n",
    "    vec_end = mean_vec + eigen_vectors[:, i] * 2 * np.sqrt(eigen_values[i])\n",
    "    \n",
    "    plt.annotate('', xy=vec_end, xytext=mean_vec,\n",
    "                 arrowprops=dict(facecolor='red', width=1.5, headwidth=7))\n",
    "    plt.text(vec_end[0], vec_end[1], f'PC{i+1}', color='red', fontsize=12)\n",
    "\n",
    "plt.xlabel('Feature 1 (standardized)')\n",
    "plt.ylabel('Feature 2 (standardized)')\n",
    "plt.title('Iris dataset with Eigenvectors')\n",
    "plt.grid(True)\n",
    "plt.axis('equal')  # Escala igual para ambos ejes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2494192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 4: Ordenar los valores propios y los vectores propios\n",
    "# Después de obtener los valores y vectores propios de la descomposición propia,\n",
    "# es crucial ordenarlos para identificar qué componentes principales (direcciones de\n",
    "# (direcciones de máxima varianza en los datos) contienen la mayor información.\n",
    "\n",
    "# Ordenar los valores propios y sus correspondientes vectores propios por valores propios decrecientes\n",
    "# nos ayuda a jerarquizar los componentes por importancia. Esta clasificación es esencial porque\n",
    "# nos dice qué direcciones (vectores propios) de los datos capturan la mayor varianza,\n",
    "# y, por tanto, la mayor información sobre cómo se distribuyen los datos.\n",
    "\n",
    "eigen_values_sorted_indices = np.argsort(eigen_values)[::-1]   # Obtener índices de valores propios ordenados en orden descendente\n",
    "eigen_values_sorted = eigen_values[eigen_values_sorted_indices]  # Ordena los valores propios por estos índices\n",
    "eigen_vectors_sorted = eigen_vectors[:, eigen_values_sorted_indices]  # Ordenar los vectores propios por los mismos índices\n",
    "\n",
    "# ¿Por qué ordenar y elegir componentes principales?\n",
    "# Los componentes principales son nuevas variables creadas a partir de combinaciones lineales de las variables originales.\n",
    "# El objetivo es reducir las dimensiones (características) preservando tanta información como sea posible.\n",
    "\n",
    "# Los vectores propios con los valores propios más altos son los más significativos. Capturan la mayor\n",
    "# varianza (información) de los datos. Por el contrario, los vectores propios con los valores propios más bajos contienen\n",
    "# la menor información sobre la distribución de los datos y pueden eliminarse para reducir la dimensionalidad.\n",
    "\n",
    "# Por ejemplo, si tenemos tres características que contribuyen a un conjunto de datos y calculamos tres valores propios,\n",
    "# eliminando la que tiene el valor propio más bajo reducimos la dimensionalidad de nuestro conjunto de datos. Este proceso\n",
    "# simplifica el conjunto de datos conservando la información más importante.\n",
    "\n",
    "#Multiplicando los datos estandarizados originales por los vectores propios elegidos (correspondientes a los valores propios # más altos), obtenemos los valores propios más altos.\n",
    "# valores propios) nos da los componentes principales. Estos componentes principales son las nuevas características\n",
    "# de nuestro conjunto de datos de dimensionalidad reducida, ofreciendo una visión simplificada pero informativa de nuestros datos.\n",
    "\n",
    "# Ejemplo: Si nuestro conjunto de datos incluye características como edad, altura y peso, y después de PCA, encontramos que\n",
    "# la altura y el peso tienen los valores propios más altos, podríamos eliminar la edad de nuestro análisis. Utilizando los\n",
    "# vectores propios asociados con la altura y el peso, podemos construir componentes principales que mejor\n",
    "# representen mejor la varianza de nuestro conjunto de datos, centrándose en los aspectos más informativos de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0100a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_values_sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a175e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_values_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9411992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_vectors_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd9e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 5: Seleccionar los componentes principales más importantes\n",
    "# Después de ordenar los valores y vectores propios, el siguiente paso crucial es decidir\n",
    "# cuántos componentes principales queremos conservar. Esta decisión afecta directamente a la\n",
    "# dimensionalidad de los datos transformados y la cantidad de varianza (información) retenida.\n",
    "\n",
    "# Elección del número de componentes:\n",
    "# La elección de N (el número de componentes principales) suele estar guiada por el objetivo de\n",
    "# conservar una parte sustancial de la varianza original de los datos. Seleccionando los\n",
    "# N asociados con los mayores valores propios, nos aseguramos de capturar los\n",
    "# patrones y estructuras más significativos de los datos.\n",
    "\n",
    "n_components = 3  # Aquí, elegimos 2 por una razón específica.\n",
    "\n",
    "# La elección de 2 componentes principales está especialmente motivada por la facilidad de visualización.\n",
    "# Reducir los datos a 2 dimensiones nos permite trazarlos en un gráfico 2D estándar, haciendo posible\n",
    "# evaluar visualmente patrones, agrupaciones y relaciones entre puntos de datos. Esta inspección visual\n",
    "# Esta inspección visual puede proporcionar ideas intuitivas que no son fáciles de obtener a partir de datos de alta dimensión.\n",
    "\n",
    "top_eigen_vectors = eigen_vectors_sorted[:, :n_components]  # Seleccione los 2 mejores vectores propios\n",
    "\n",
    "# ¿Por qué dos componentes?\n",
    "# Seleccionar dos componentes es una práctica común cuando nuestro interés principal es visualizar los datos.\n",
    "# Sin embargo, para otras aplicaciones, como maximizar la varianza retenida o simplificar conjuntos de datos para\n",
    "# modelos de aprendizaje automático, podríamos elegir más componentes basados en la varianza acumulada explicada.\n",
    "\n",
    "# Visualización de la elección:\n",
    "# A menudo, la decisión sobre el número de componentes a retener es apoyada por un scree plot - un gráfico\n",
    "# que muestra la fracción de la varianza total retenida frente al número de componentes. Un 'codo' en\n",
    "# Un «codo» en este gráfico puede sugerir un buen equilibrio entre la retención de la varianza y la reducción de las dimensiones.\n",
    "\n",
    "#La implementación de PCA con dos componentes nos permite transformar nuestros datos de alta dimensión en un espacio \n",
    "# 2D donde podemos explorar y reducir las dimensiones.\n",
    "# espacio 2D donde podemos explorar e interpretar la estructura de los datos visualmente. Simplifica\n",
    "# simplifica los conjuntos de datos complejos conservando la esencia de la información, lo que facilita tanto el análisis como la narración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bfb825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 6: Proyectar los datos en un espacio de dimensiones inferiores\n",
    "# Después de identificar los componentes principales superiores (vectores propios), el siguiente paso es utilizarlos\n",
    "# para transformar nuestros datos originales, de alta dimensión, en un nuevo espacio con menos dimensiones. Este nuevo espacio #\n",
    "# nuevo espacio está definido por los componentes principales que hemos elegido como los más significativos.\n",
    "\n",
    "# ¿Por qué proyectar los datos?\n",
    "# Proyectar los datos en un espacio de menor dimensión nos ayuda a lograr varios objetivos clave:\n",
    "# 1. **Simplificación**: Simplifica el conjunto de datos, haciendo que sea más fácil trabajar con ellos, especialmente para su visualización y análisis.\n",
    "# 2. **Preservación de la información**: A pesar de reducir el número de dimensiones, este proceso conserva los aspectos más críticos de los datos originales, capturados por los componentes principales.\n",
    "# 3. **Reducción del ruido**: Al centrarnos en las direcciones de máxima varianza, también tendemos a filtrar el ruido y la información menos relevante.\n",
    "\n",
    "# ¿Cómo funciona la proyección?\n",
    "# La proyección se realiza multiplicando los datos estandarizados originales (X_escalados) por la matriz de vectores propios seleccionados (vectores_propios).\n",
    "# Esta operación esencialmente reorienta los datos de los ejes originales a los nuevos ejes definidos por los componentes principales, dando como resultado un conjunto de datos transformado que resalta las relaciones y patrones más significativos.\n",
    "X_pca = np.dot(X_scaled, top_eigen_vectors)  # Realizar la proyección\n",
    "\n",
    "# El resultado - ¿Qué obtenemos?\n",
    "# El resultado de esta proyección es una nueva versión de nuestro conjunto de datos (X_pca) donde cada punto de datos ha sido reposicionado en un espacio bidimensional (si elegimos 2 componentes).\n",
    "# Este espacio 2D está definido por las dos direcciones más significativas de varianza en los datos originales, tal y como fueron identificadas por PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0595e2d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Paso 7: Visualización de los datos reducidos por PCA\n",
    "# Visualizar los datos en el nuevo espacio 2D reducido por PCA\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=iris.target, palette='viridis')\n",
    "plt.title('PCA of Iris Dataset')\n",
    "plt.xlabel('Principal Component 1')  # The first principal component\n",
    "plt.ylabel('Principal Component 2')  # The second principal component\n",
    "plt.legend(title='Species', labels=iris.target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c29ff",
   "metadata": {},
   "source": [
    "## 5. Minería de reglas de asociación\n",
    "\n",
    "En el mundo de la minería de datos, la minería de reglas de asociación es una potente técnica utilizada para encontrar patrones y relaciones ocultas en grandes conjuntos de datos. Imagine que está en una tienda de comestibles analizando las cestas de la compra para encontrar artículos que se compran juntos con frecuencia. Esta información puede ayudarle a tomar decisiones como la colocación de productos, las promociones y la gestión de inventarios. La minería de reglas de asociación hace exactamente esto, pero a una escala mucho mayor y a través de diversos conjuntos de datos.\n",
    "\n",
    "[Reglas de asociación](https://www.youtube.com/watch?v=WGlMlS_Yydk&ab_channel=AugmentedAI)\n",
    "\n",
    "### 5.1 Fundamentos de las reglas de asociación\n",
    "\n",
    "La minería de reglas de asociación se basa en el descubrimiento de relaciones interesantes entre variables de grandes bases de datos. Estas relaciones no se basan en propiedades inherentes a los propios elementos de datos, sino que se derivan de los datos a medida que se agrupan en operaciones.\n",
    "\n",
    "#### Conceptos y métricas clave:\n",
    "\n",
    "1. **Soporte**: \n",
    "   - El soporte mide la frecuencia con la que un elemento o conjunto de elementos aparece en el conjunto de datos. En nuestro ejemplo de la tienda de comestibles, si 100 personas compraron leche en 1.000 transacciones, el apoyo a la leche es del 10%.\n",
    "   - ¿Por qué es importante? El apoyo nos ayuda a filtrar los conjuntos de elementos menos frecuentes para su posterior análisis, centrándonos en los patrones más comunes y potencialmente significativos.\n",
    "\n",
    "2. **Confianza**:\n",
    "   - La confianza mide la frecuencia con la que los artículos de Y aparecen en transacciones que contienen X. Si de los que compraron leche, 30 también compraron galletas, la confianza de la regla {Leche -> Galletas} es del 30%.\n",
    "   - La confianza indica la fiabilidad de la inferencia realizada por la regla. Las reglas de alta confianza tienen más probabilidades de ser de interés porque representan asociaciones fuertes.\n",
    "\n",
    "3. **Elevación**\n",
    "   - ¿Qué es? La elevación mide la frecuencia con la que X e Y aparecen juntos, más de lo que cabría esperar si fueran estadísticamente independientes. Si la leche y las galletas se compran juntas tres veces más de lo que sugeriría la hipótesis de independencia, la elevación es de 3.\n",
    "   - La elevación ayuda a identificar la fuerza de una regla sobre la probabilidad base de la ocurrencia de elementos. Un valor de elevación superior a 1 indica una asociación positiva entre X e Y.\n",
    "\n",
    "#### Importancia en la minería de datos:\n",
    "\n",
    "Las reglas de asociación son importantes en la minería de datos porque revelan información que puede conducir a un conocimiento práctico. Por ejemplo, entender que la leche y las galletas se compran juntas con frecuencia puede conducir a estrategias de marketing específicas, mejores diseños de tienda y políticas de almacenamiento optimizadas.\n",
    "\n",
    "Estas reglas no se limitan al comercio minorista, sino que pueden aplicarse a diversos ámbitos, como la minería de usos de Internet, la detección de intrusos y la bioinformática, lo que demuestra la utilidad versátil de las reglas de asociación para descubrir patrones significativos ocultos en grandes conjuntos de datos.\n",
    "\n",
    "[Association Rule link](https://medium.com/analytics-vidhya/association-rule-mining-concept-and-implementation-28443d16f611)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4918a3c",
   "metadata": {},
   "source": [
    "## 5.2 El algoritmo Apriori en acción\n",
    "\n",
    "Imagina que intentas encontrar patrones en lo que la gente compra junta en un supermercado. Por ejemplo, la gente que compra pan suele comprar también leche. El algoritmo Apriori nos ayuda a encontrar estos patrones pero sin tener que comprobar cada combinación a mano. Es como tener un asistente inteligente que te dice las combinaciones de compra más comunes.\n",
    "\n",
    "### ¿Qué es el algoritmo Apriori?\n",
    "\n",
    "El algoritmo Apriori es un método clásico utilizado en minería de datos para descubrir asociaciones entre elementos. Es como jugar a un juego de detectives en el que se buscan artículos que suelen aparecer juntos en las cestas de la compra.\n",
    "\n",
    "#### ¿Cómo funciona?\n",
    "\n",
    "1. **Preparación del escenario**: En primer lugar, imagine que cada compra en una tienda es una «cesta» con varios artículos. El algoritmo Apriori busca en estas cestas combinaciones de artículos que aparecen juntos con frecuencia.\n",
    "\n",
    "2. **Buscar coincidencias**: Empieza con artículos individuales y los empareja gradualmente con otros para ver con qué frecuencia se compran juntos. Por ejemplo, primero se da cuenta de que la leche se compra a menudo, luego el pan, y empieza a buscar la frecuencia con la que la leche y el pan se compran juntos.\n",
    "\n",
    "3. **Aplicación de reglas**: El algoritmo utiliza dos reglas principales: \n",
    "   - **La regla de frecuencia (apoyo)**: Si mucha gente compra leche, se considera que la leche es importante.\n",
    "   - **La regla de confianza**: Si cada vez que alguien compra leche, también compra pan, entonces la combinación de leche y pan es significativa.\n",
    "\n",
    "4. **Aumentar**: A partir de ahí, sigue añadiendo más elementos a la mezcla, como añadir huevos a nuestra combinación de leche y pan, siempre comprobando que estas nuevas combinaciones siguen siendo lo suficientemente comunes.\n",
    "\n",
    "### ¿Por qué utilizar el algoritmo Apriori?\n",
    "\n",
    "- **Eficiencia**: Reduce inteligentemente el número de combinaciones que necesita comprobar, haciéndolo más rápido que buscar en todas las combinaciones posibles.\n",
    "- **Simplicidad**: Sus pasos son sencillos, por lo que es fácil de entender y aplicar, incluso para los principiantes.\n",
    "- **Perspicaz**: Puede revelar patrones sorprendentes en los datos que podrían no ser obvios a primera vista.\n",
    "\n",
    "### Aplicación en el mundo real: Conjunto de datos de venta al por menor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfb8fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Step 1: Prepare the Dataset\n",
    "# Let's simulate a small dataset of transactions in a retail setting\n",
    "transactions = [\n",
    "    ['Milk', 'Bread', 'Eggs'],\n",
    "    ['Milk', 'Bread'],\n",
    "    ['Bread', 'Eggs'],\n",
    "    ['Milk', 'Eggs'],\n",
    "    ['Bread', 'Eggs', 'Beer'],\n",
    "    ['Milk', 'Bread', 'Eggs', 'Beer']\n",
    "]\n",
    "\n",
    "# The TransactionEncoder transforms the dataset into a one-hot encoded DataFrame\n",
    "# Each column represents an item, and each row represents a transaction\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Step 2: Applying the Apriori Algorithm\n",
    "# Use the apriori function to find frequent itemsets\n",
    "# min_support is set to 0.5, meaning only itemsets appearing in at least 50% of transactions will be considered\n",
    "frequent_itemsets = apriori(df, min_support=0.5, use_colnames=True)\n",
    "\n",
    "# Step 3: Generating Association Rules\n",
    "# From the frequent itemsets, generate association rules using the association_rules function\n",
    "# Here, we're interested in rules with a minimum confidence of 0.7\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "\n",
    "# Step 4: Analyzing the Results\n",
    "# Display the rules found by the algorithm\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
    "\n",
    "# Explanation:\n",
    "# The output includes frequent itemsets and association rules with their respective support, confidence, and lift.\n",
    "# - Support: The proportion of transactions that contain the itemset.\n",
    "# - Confidence: The likelihood that buying the antecedents leads to the consequents.\n",
    "# - Lift: The increase in the ratio of the sale of consequents when the antecedents are sold.\n",
    "\n",
    "# Real-World Application Insight:\n",
    "# For instance, if 'Milk' and 'Bread' appear frequently together in the rules with high confidence,\n",
    "# a retail manager might consider placing these items closer in the store or promoting them together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d780a1c",
   "metadata": {},
   "source": [
    "## 6. Detección de anomalías"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5880ab37",
   "metadata": {},
   "source": [
    "#### ¿Qué es la detección de anomalías?\n",
    "\n",
    "La detección de anomalías se refiere a la identificación de elementos, eventos u observaciones poco comunes que se desvían significativamente de la mayoría de los datos. Estos valores atípicos pueden indicar incidentes críticos como fraudes, intrusiones en la red, equipos que funcionan mal o simplemente puntos de datos raros pero significativos.\n",
    "\n",
    "#### ¿Por qué es importante la detección de anomalías?\n",
    "\n",
    "* Detección de fraudes: Identificación de transacciones inusuales que puedan indicar actividad fraudulenta.\n",
    "\n",
    "* Seguridad de la red: Detección de accesos no autorizados o patrones inusuales en el tráfico de red.\n",
    "\n",
    "* Control de calidad: Identificación de defectos o irregularidades en los procesos de fabricación.\n",
    "\n",
    "* Diagnóstico médico: Detección de enfermedades raras o anomalías en imágenes médicas.\n",
    "\n",
    "#### Técnicas de detección de anomalías\n",
    "\n",
    "**Métodos estadísticos**\n",
    "\n",
    "- Puntuación Z: Mide cuántas desviaciones estándar tiene un elemento respecto a la media.\n",
    "\n",
    "- IQR (Rango Intercuartílico): Identifica los valores atípicos basándose en la dispersión del 50% medio de los datos.\n",
    "\n",
    "- Modelos de mezclas gaussianas (GMM): Utiliza modelos probabilísticos para identificar anomalías.\n",
    "\n",
    "**Métodos basados en la proximidad**\n",
    "\n",
    "- Vecinos más próximos (k-NN): Las anomalías son puntos de datos que tienen una vecindad dispersa.\n",
    "\n",
    "- Agrupación espacial de aplicaciones con ruido basada en la densidad (DBSCAN): Identifica puntos en regiones de baja densidad como anomalías.\n",
    "\n",
    "**Métodos basados en clustering**\n",
    "\n",
    "- k-Means: Los puntos alejados de cualquier centroide de cluster se consideran anomalías.\n",
    "\n",
    "- Agrupación jerárquica: El análisis de dendrogramas puede ayudar a detectar valores atípicos.\n",
    "\n",
    "**Métodos de aprendizaje automático**\n",
    "\n",
    "- Bosque de aislamiento: Construye árboles de decisión para aislar anomalías.\n",
    "\n",
    "- SVM de una clase: Aprende una función de decisión para una sola clase e identifica los puntos que no se ajustan a ella.\n",
    "\n",
    "**Redes neuronales**\n",
    "\n",
    "- Autocodificadores: Aprenden a comprimir y reconstruir datos; un error de reconstrucción elevado indica anomalías.\n",
    "\n",
    "- Redes neuronales recurrentes (RNN): Útiles para datos secuenciales en los que las anomalías alteran los patrones normales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aaa19c",
   "metadata": {},
   "source": [
    "### 6.1 Técnicas avanzadas de detección de anomalías\n",
    "\n",
    "#### Isolation Forest:\n",
    "\n",
    "* Principio: Las anomalías son pocas y diferentes. Isolation Forest aísla las observaciones seleccionando aleatoriamente una característica y, a continuación, seleccionando aleatoriamente un valor de división entre los valores máximo y mínimo de la característica seleccionada.\n",
    "\n",
    "* Proceso:\n",
    "    1. Construir múltiples árboles de decisión aleatorios.\n",
    "\n",
    "    2. Calcular la longitud media del camino de cada observación a la raíz.\n",
    "\n",
    "    3. Los caminos más cortos indican anomalías.\n",
    "\n",
    "* Ventajas: Eficiente para grandes conjuntos de datos, funciona bien con datos de alta dimensión.\n",
    "\n",
    "#### One-class SVM :\n",
    "\n",
    "* Principio: SVM (Support Vector Machine) construye una frontera que abarca la mayoría de los puntos de datos en el espacio de características.\n",
    "\n",
    "* Proceso:\n",
    "    1. Mapear los datos en un espacio de características de alta dimensión.\n",
    "\n",
    "    2. Encontrar el hiperplano de margen máximo que mejor separa los datos del origen.\n",
    "\n",
    "    3. Los puntos de datos situados a un lado del hiperplano se consideran normales, y los situados al otro lado, anomalías.\n",
    "\n",
    "* Ventajas: Eficaz en espacios de alta dimensionalidad, robusto a la maldición de la dimensionalidad.\n",
    "\n",
    "### Consideraciones prácticas\n",
    "\n",
    "- Elección del método: Depende de la naturaleza de los datos, del tipo de anomalías esperadas y de los recursos computacionales disponibles.\n",
    "\n",
    "- Ajuste de parámetros: Métodos como Isolation Forest y One-Class SVM requieren un ajuste cuidadoso de parámetros como el número de árboles o los parámetros del kernel.\n",
    "\n",
    "- Evaluación: Utilice métricas como Precision, Recall, F1-Score y AUC-ROC para evaluar el rendimiento de los modelos de detección de anomalías.\n",
    "\n",
    "#### Ejemplo de flujo de trabajo\n",
    "1. Preprocesamiento de datos: Limpiar y normalizar los datos.\n",
    "\n",
    "2. Selección del modelo: Elegir el método de detección de anomalías adecuado.\n",
    "\n",
    "3. Entrenamiento: Ajustar el modelo a los datos de entrenamiento.\n",
    "\n",
    "4. Evaluación: Evaluar el rendimiento del modelo utilizando datos de validación.\n",
    "\n",
    "5. Despliegue: Aplicar el modelo para detectar anomalías en nuevos datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28768b84",
   "metadata": {},
   "source": [
    "### 6.2 Detección de anomalías en Python\n",
    "\n",
    "#### Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd31d9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "# Generate sample data\n",
    "X = np.random.rand(100, 2)\n",
    "\n",
    "# Introduce anomalies\n",
    "X = np.concatenate([X, np.array([[1.5, 1.5], [1.6, 1.6]])])\n",
    "\n",
    "# Train Isolation Forest\n",
    "model = IsolationForest(contamination=0.1)\n",
    "model.fit(X)\n",
    "\n",
    "# Predict anomalies\n",
    "predictions = model.predict(X)\n",
    "# -1 for anomalies, 1 for normal data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b32d28",
   "metadata": {},
   "source": [
    "#### One-Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb637b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "import numpy as np\n",
    "\n",
    "# Generate sample data\n",
    "X = np.random.rand(100, 2)\n",
    "\n",
    "# Introduce anomalies\n",
    "X = np.concatenate([X, np.array([[1.5, 1.5], [1.6, 1.6]])])\n",
    "\n",
    "# Train One-Class SVM\n",
    "model = OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n",
    "model.fit(X)\n",
    "\n",
    "# Predict anomalies\n",
    "predictions = model.predict(X)\n",
    "# -1 for anomalies, 1 for normal data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea60d454",
   "metadata": {},
   "source": [
    "**En resumen**: <p>\n",
    "La detección de anomalías es un aspecto crítico del análisis de datos, especialmente en campos donde la identificación de eventos raros es crucial. Comprender y aplicar diferentes técnicas de detección de anomalías, como Isolation Forest y One-Class SVM, equipa a los científicos de datos con las herramientas necesarias para descubrir valores atípicos significativos en sus datos. Mediante una cuidadosa selección del modelo, el ajuste de los parámetros y una evaluación rigurosa, se pueden desarrollar sistemas eficaces de detección de anomalías para hacer frente a diversos retos del mundo real."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
