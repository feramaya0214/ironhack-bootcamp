{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWzt_ckQEAFH"
   },
   "source": [
    "# Bayesian Statistics\n",
    "\n",
    "> Bayesian statistics is a theory in the field of statistics based on the **Bayesian interpretation of probability** where probability expresses a degree of belief in an event. \n",
    "> The degree of belief may be based on prior knowledge about the event, such as the results of previous experiments, or on personal beliefs about the event. \n",
    "> This differs from a number of other interpretations of probability, such as the frequentist interpretation that views probability as the limit of the relative frequency of an event after many trials. \n",
    "\n",
    ">(Source: *Wikipedia*) \n",
    "\n",
    "## The Bayes \"formula\"\n",
    "\n",
    "Bayes' theorem is a fundamental theorem in Bayesian statistics. \n",
    "\n",
    "$$ P(H | E) = \\frac{P(E | H)P(H)}{P(E)}$$\n",
    "\n",
    "Bayes' formula can be used in frequentist statistics for computing **conditional probabilities**, but in Bayesian statistics is used to compute **posterior probabilities** (as opposed to **prior probabilities**), given observations. \n",
    "\n",
    "> For example, a patient is observed to have a certain symptom ($E$), and Bayes' formula can be used to compute the probability that a diagnosis ($H$) is correct, given that observation. \n",
    "\n",
    "Bayesian statistics **interprets probabilities as measures of believability** (how confident we are) in an event, **not as the long-run frequency of events**.\n",
    "\n",
    "Beliefs mesaures are applied to individuals, not to nature, so there is room for conflicting belives among individuals. Different beliefs are not intepreted as errors but as **different states of knowledge about an event**. \n",
    "\n",
    "The formula is interpreted as an **updating of belief after observing data**.\n",
    "\n",
    "### A note about Bayesian subjectivity\n",
    "\n",
    "> Bayesian methods are often characterized as “subjective” because the user must choose a prior distribution, that is, a mathematical expression of prior information. \n",
    "\n",
    "> The prior distribution requires information and user input, that’s for sure, but I don’t see this as being any more “subjective” than other aspects of a statistical procedure, such as the choice of model for the data (for example, logistic regression) or the choice of which variables to include in a prediction, the choice of which coefficients should vary over time or across situations, the choice of statistical test, and so forth. \n",
    "\n",
    "> Indeed, Bayesian methods can in many ways be more “objective” than conventional approaches in that Bayesian inference, with its smoothing and partial pooling, is well adapted to including diverse sources of information and thus can reduce the number of data coding or data exclusion choice points in an analysis. \n",
    "\n",
    "> *Andrew Gelman, Professor of statistics and political science and director of the Applied Statistics Center at Columbia University*. \n",
    "\n",
    "# Bayesian Data Analysis\n",
    "\n",
    "Let's now imagine the following scenario: We want to know the *probability* of getting the Oscar for the last film an actor has starred. \n",
    "\n",
    "In this case, the *frequentist* notion of **series of trials** is not well defined: every year the situation is different, there are no series of identical trials to consider. We can conclude that the classical notion of probability does not apply to this situations.\n",
    "\n",
    "But **Bayesianism** defines probability in a different way: **the degree of belief that an event will occur**.\n",
    "\n",
    "What is the probability that Bin Laden is dead? For a frequentist there is no probability for this event because there are no possible trials (Bin Laden is dead or is not, it's not a question of probability). A Bayesianist would assign a probability to this event based on her **state of knowledge**. The state of knowledge changes when new information is available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BF5Mz81EAFM"
   },
   "source": [
    "## The Bayes rule\n",
    "\n",
    "The main tool of Bayesian Analysis is the Bayes theorem, presented in 1763:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bml-o1SoEAFM"
   },
   "source": [
    "<b> Bayes Theorem </b> <br>\n",
    "$$ P(A | B) = \\frac{P(B | A)P(A)}{P(B)}$$\n",
    "</div>   \n",
    "\n",
    "\n",
    "This theorem describes the relationship between the conditional probabilities of two events.\n",
    "\n",
    "It is easy to show that this is true. It's only basic arithmetic based on probability rules (chain rule):\n",
    "\n",
    "+ We know that $ P(A \\mbox{ and } B) = P(A)P(B | A) $.\n",
    "+ But it also true that $ P(A \\mbox{ and } B) = P(B)P(A | B)$.\n",
    "+ So, $ P(A)P(B | A) = P(B)P(A | B)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8m-jqZmEAFM"
   },
   "source": [
    "Although this is called Bayes’ theorem, the **general form** of it as stated here was actually first written down not by Thomas Bayes, but by Pierre-Simon Laplace. What Bayes did was derive the special case of this formula for “inverting” the binomial distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTMo3RViEAFM"
   },
   "source": [
    "### Hypotheses and evidences.\n",
    "\n",
    "The most common interpretation of Bayes's Theorem is based in considering that $A$ is an hypothesis $H$ and $B$ a new evidence $E$ that should modify our belief in $H$:\n",
    "\n",
    "$$P(H | E) = P(H) \\frac{P(E|H)}{P(E)}$$\n",
    "\n",
    "This is called the **diachronic interpretation** because it describes how *an hypothesis must be updated over time every time a new evidence is found*. \n",
    "\n",
    "\n",
    "+ $P(H | E)$ is called the **posterior**.\n",
    "+ $P(H)$ is called the **prior probability** of the hypothesis.\n",
    "+ $P(E | H)$ is called the **likelihood** of the evidence.\n",
    "+ $P(E)$ is a normalizing constant. If there are $n$ hypotheses that are *mutually exclusive* and *collectivelly exahustive*, we can compute $P(E)$ as:\n",
    "\n",
    "$$ P(E) = P(H_1)P(E|H_1) + \\dots + P(H_n)P(E|H_n)$$\n",
    "\n",
    "\n",
    "In general, $P(H | E), P(H), P(E|H), P(E)$ are functions! \n",
    "\n",
    "We can extract point estimates, set estimates and probabilistic propositions from $P(H | E)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJXD9SqUEAFM"
   },
   "source": [
    "### Example: Visualization of the Bayes Rule<br>\n",
    "\n",
    "Let's suppose we are living a pandemic. Say $P(H_{yes})=5\\%$ is the **prevalence** of the disease and that  \n",
    "each individual of the population is given a test with accuracy $P(E_{yes}|H_{yes})=P(E_{no}|H_{no}) = 90\\%$.  \n",
    "\n",
    "We want to know the **probability of having the disease if you tested positive**: \n",
    "$$Pr(H_{yes}|E_{yes})$$. \n",
    "\n",
    "We can use the Bayes rule to compute this posterior:\n",
    "\n",
    "$$ P(H_{yes}|E_{yes}) = \\frac{P(H_{yes}) P(E_{yes}|H_{yes}) }{P(E_{yes})} = $$\n",
    "\n",
    "$$ = \\frac{P(H_{yes})P(E_{yes}|H_{yes}) }{P(H_{yes})P(E_{yes}|H_{yes}) + P(H_{no})P(E_{yes}|H_{no})} $$\n",
    "\n",
    "That is\n",
    "\n",
    "$$ = \\frac{0.05 \\times 0.9}{0.05 \\times 0.9 + 0.95 \\times 0.1} \\approx 0.32 $$\n",
    "\n",
    "Many find it counterintuitive that this probability is much lower than $90\\%$; this animated gif is meant to help.\n",
    "\n",
    "The `O` in the middle turns into an `X` when the test fails. The rate of `X`s is $1-Pr(E_{yes}|H_{yes})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VnmorIULkH4"
   },
   "source": [
    "![ChessUrl](https://raw.githubusercontent.com/simplystats/simplystats.github.io/master/_images/bayes.gif \"chess\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19bSZkZrEAFN"
   },
   "source": [
    "### Example: Monty Hall Problem\n",
    "\n",
    "> \"*Let's Make a Deal*\" is a television game show which originated in the United States and has since been produced in many countries throughout the world. The show is based around deals offered to members of the audience by the host. The traders usually have to weigh the possibility of an offer for valuable prizes, or undesirable items, referred to as \"Zonks\". \n",
    "\n",
    ">*Source: Wikipedia*.\n",
    "\n",
    "Monty Hall was the original host of the game. The Monty Hall problem is based on one of the regular games of the show. It is a stick or switch problem:\n",
    "+ Suppose you're on the game show, and you're given the choice of three doors: Behind one door is a car; behind the others, goats.\n",
    "+ You pick a door, say Door A (the door is not open), and the host, who knows what's behind the doors, opens Door B, which has a goat.\n",
    "+ He then says to you, \"Do you want to pick Door C?\" Is it to your advantage to switch your choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebq8cShfPk0H"
   },
   "source": [
    "<img src=\"./images/lets.jpg\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rv0Lp36oQHxY"
   },
   "source": [
    "<img src=\"./images/monty.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAoXa7TPEAFO"
   },
   "source": [
    "Most people intuitively thinks that it makes no difference to stick or to switch, but this is wrong!\n",
    "\n",
    "The truth is that if you stick the probability of winning is 1/3; if you switch your chances are 2/3.\n",
    "\n",
    "We can use the Bayesian point of view to solve this problem. At the beginning, there are different hypotheses $H$ with their corresponding **prior** probabilities:\n",
    "\n",
    "+ A: the car is behind Door A; $P(H=\\mbox{'A'}) = 1/3$\n",
    "+ B: the car is behind Door B; $P(H=\\mbox{'B'}) = 1/3$\n",
    "+ C: the car is behind Door C; $P(H=\\mbox{'C'}) = 1/3$\n",
    "\n",
    "You choose A randomly. If you stick to A after Monty opens the door B (this a our evidence *E*). We can compute  $P(H=\\mbox{'A'}|E)$:\n",
    "\n",
    "$$ P(H=\\mbox{'A'}|E) = \\frac{P(H=\\mbox{'A'})P(E|H=\\mbox{'A'})}{P(E)} $$\n",
    "$$= \\frac{1/3 \\times 1/2}{1/3 \\times 1/2 + 1/3 \\times 0 + 1/3 \\times 1} = 1/3$$ \n",
    "\n",
    "The denominator can be understood in this way: We are assuming we initially chose A. It follows that if the car is behind A, Monty will show us a goat behind B half the time. If the car is behind B, Monty never shows us a goat behind B. Finally, if the car is behind C, Monty shows us a goat behind B every time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uscZI6LTEAFO"
   },
   "source": [
    "**What is the probability if we switch?**\n",
    "\n",
    "To know $P(H=\\mbox{'C'}|E)$ you could also apply Bayes's Theorem directly, but there is a simpler way to compute it:  Since the probability that it's behind A is 1/3 and the sum of the two probabilities must equal 1, the probability the car is behind C is 1−1/3=2/3. \n",
    "\n",
    "\n",
    "Samuel Arbesman, Wired, 11.26.14: \n",
    "\n",
    "> In fact, Paul Erdős, one of the most prolific and foremost mathematicians involved in probability, when initially told of the Monty Hall problem also fell victim to not understanding why opening a door should make any difference. Even when given the mathematical explanation multiple times, he wasn’t really convinced. It took several days before he finally understood the correct solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTpJnBhdEAFO"
   },
   "source": [
    "Let's make a **simulation** of the game to compute $P(H=\\mbox{'C'}|E)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K9ePwTNPEAFO",
    "outputId": "e2014d4e-cd79-4f3e-f83e-55036d93682f"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "iterations = 100000\n",
    "doors = [\"goat\"] * 2 + [\"car\"]\n",
    "change_wins = 0\n",
    "change_loses = 0\n",
    "\n",
    "for i in range(iterations):\n",
    "    random.shuffle(doors)\n",
    "    \n",
    "    # you pick door n:\n",
    "    n = random.randrange(3)\n",
    "    \n",
    "    # monty picks door k, k!=n and doors[k]!=\"car\"\n",
    "    sequence = list(range(3))\n",
    "    random.shuffle(sequence)\n",
    "    for k in sequence:\n",
    "        if k == n or doors[k] == \"car\":\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # now if you change, you lose iff doors[n]==\"car\"\n",
    "    if doors[n] == \"car\":\n",
    "        change_loses += 1\n",
    "    else:\n",
    "        change_wins += 1\n",
    "\n",
    "perc = (100.0 * change_wins) / (change_wins + change_loses)\n",
    "print(\"Switching has %s wins and %s losses: you win %.1f%% of the time\" % (change_wins, change_loses, perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ak2GyEGiEAFO"
   },
   "source": [
    "### Exercise: Bayes Rule\n",
    "\n",
    "Compute $P(H=\\mbox{'C'}|E)$ by applying Bayes' Rule and check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L95Ax2P7EAFP",
    "outputId": "d0930360-906c-49a1-cd68-50003fab16e2"
   },
   "outputs": [],
   "source": [
    "# your solution here\n",
    "\n",
    "P = 0\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zj7UeDLfEAFP"
   },
   "source": [
    "### Exercise: Clinical Test\n",
    "\n",
    "**What is the evidence of a disease after a clinical test?**\n",
    "\n",
    "Data:\n",
    "\n",
    "+ 1% of women at age forty who participate in routine screening have breast cancer. \n",
    "+ 80% of women with breast cancer will get positive mammographies. \n",
    "+ 9.6% of women without breast cancer will also get positive mammographies. \n",
    "\n",
    "A woman in this age group had a positive mammography in a routine screening.\n",
    "\n",
    "- **What is the probability that she actually has breast cancer?**\n",
    "\n",
    "- **Is this a \"good\" test for detecting breast cancer?**\n",
    "\n",
    "This simple puzzle is not all that simple in practice. Only 15% of doctors, when presented with this situation, come up with the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DvLIq3TEAFP"
   },
   "source": [
    "The data:\n",
    "\n",
    "+ $P(\\mbox{cancer}) = 0.01$ and $P(\\mbox{no cancer}) = 0.99$.\n",
    "+ If you get $+$, then $P(\\mbox{ + } | \\mbox{ cancer}) = 0.8$ and $P(\\mbox{ + } | \\mbox{ no cancer}) = 0.096$.\n",
    "+ If you get $-$, then $P(\\mbox{ - } | \\mbox{ cancer}) = 0.2$ and $P(\\mbox{ - } | \\mbox{ no cancer}) = 0.904$.\n",
    "\n",
    "Let's give an answer to the second question. The answer can be given by comparing $P(\\mbox{cancer }|\\mbox{ + })$ and $P(\\mbox{no cancer }|\\mbox{ + })$:\n",
    "\n",
    "$$ P(\\mbox{cancer }|\\mbox{ + }) \\propto P(\\mbox{cancer}) P(\\mbox{ + } | \\mbox{ cancer}) = 0.008$$\n",
    "\n",
    "$$ P(\\mbox{no cancer }|\\mbox{ + }) \\propto P(\\mbox{no cancer}) P(\\mbox{ + } | \\mbox{ no cancer}) = 0.09504$$\n",
    "\n",
    "Obviously, this is not a good test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onFcoloXEAFP"
   },
   "source": [
    "**What is the probability that she actually has breast cancer?** Remember that:\n",
    "\n",
    "$$P(E) = \\sum_i P(H_i) p (E | H_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvjgbhk3EAFP"
   },
   "outputs": [],
   "source": [
    "# Your solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6B6kTVQUEAFP"
   },
   "source": [
    "### Exercise: Second clinical test\n",
    "Let's suppose that the result of a second test for a given woman *is independent* from the first one (this is clearly a wrong assumption in the real world, but let's pretend it is true!). **What is the probability of cancer after a positive in a second test?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVQFNI_DEAFQ",
    "outputId": "b40259f6-bbff-4863-eba4-6aff49b22d58"
   },
   "outputs": [],
   "source": [
    "# Your solution here.\n",
    "\n",
    "P2 = 0\n",
    "print(P2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0-NXiANEAFQ"
   },
   "source": [
    "## The locomotive problem.\n",
    "\n",
    "A railroad company numbers its locomotives in order $1 \\dots N$. One day you see a locomotive with the number 60. Estimate how many locomotives the railroad company has.\n",
    "\n",
    "> During World War II, the *Economic Warfare Division* of the American Embassy in London used statistical analysis to estimate German production of tanks and other equipment.\n",
    "The Western Allies had captured log books, inventories, and repair records that included chassis and engine serial numbers for individual tanks.\n",
    "\n",
    "> Analysis of these records indicated that serial numbers were allocated by manufacturer and tank type in blocks of 100 numbers, that numbers in each block were used sequentially, and that not all numbers in each block were used. So the problem of estimating German tank production could be reduced, within each block of 100 numbers, to a form of the locomotive problem.\n",
    "\n",
    "> Based on this insight, American and British analysts produced estimates substantially lower than estimates from other forms of intelligence. And after the war, records indicated that they were substantially more accurate.\n",
    "\n",
    "> *(Source: http://greenteapress.com/thinkbayes/html/thinkbayes004.html#toc24)*\n",
    "\n",
    "Based on the observation, we know the railroad has 60 or more locomotives. But how many more? \n",
    "\n",
    "To apply Bayesian reasoning, we can break this problem into two steps:\n",
    "\n",
    "+ What did we know about $N$ before we saw the data?\n",
    "+ For any given value of $N$, what is the likelihood of seeing the data (a locomotive with number 60)?\n",
    "\n",
    "The answer to the first question is the **prior**, $P(H)$. The answer to the second is the **likelihood**, $P(E|H)$.\n",
    "\n",
    "We don’t have much basis to choose a prior, but we can start with something simple and then consider alternatives. Let’s assume that $N$ can be any value from 1 to 1000 and a flat prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "feiJqEpeEAFQ",
    "outputId": "3e7d2d6f-3f17-4a82-e785-bea666805611"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hypos = np.array(range(1, 1001))\n",
    "priors = np.array([1.0/len(hypos) for hypo in hypos])\n",
    "\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    plt.figure(figsize=(10,2))\n",
    "    plt.plot(hypos, priors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-ZwE0w8EAFQ"
   },
   "source": [
    "Now all we need is a **likelihood function**, $P(E|H)$. \n",
    "\n",
    "In a hypothetical fleet of $N$ locomotives, what is the probability that we would see number 60? \n",
    "\n",
    "If we assume that there is only one train-operating company (or only one we care about) and that we are equally likely to see any of its locomotives, then the chance of seeing any particular locomotive is $1/N$ and that there are at least $N$ locomotives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "my86tvinEAFQ"
   },
   "outputs": [],
   "source": [
    "def Likelihood(data, hypo):\n",
    "    if hypo < data:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1.0/hypo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPoptm3sEAFQ"
   },
   "source": [
    "Let's plug our data into the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "id": "MjzmS2baEAFQ",
    "outputId": "47e74c95-e521-4be2-e9c8-1d25e1728300"
   },
   "outputs": [],
   "source": [
    "def Posterior(data, hypos, priors):\n",
    "    import numpy as np\n",
    "    posterior = np.array([Likelihood(data, hypo) for hypo in hypos]) * priors\n",
    "    return posterior\n",
    "\n",
    "# After an update, the distribution is no longer normalized, \n",
    "# but because these hypotheses are mutually exclusive and \n",
    "# collectively exhaustive, we can renormalize.\n",
    "\n",
    "def Normalize(d):\n",
    "    total = d.sum()\n",
    "    factor = 1.0 / total\n",
    "    for i in range(len(d)):\n",
    "        d[i] *= factor\n",
    "    return d\n",
    "\n",
    "posterior = Normalize(Posterior(60, hypos, priors))\n",
    "\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    plt.figure(figsize=(10,2))\n",
    "    plt.ylim([-0.001,0.006])\n",
    "    plt.plot(hypos, posterior)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTq021-nEAFQ"
   },
   "source": [
    "The most likely value, if you had to guess, is 60. That might not seem like a very good guess; after all, what are the chances that you just happened to see the train with the highest number?\n",
    "\n",
    "Nevertheless, if you want to maximize the chance of getting the answer exactly right, you should guess 60.\n",
    "\n",
    "But maybe that’s not the right goal. An alternative is to compute the Bayes estimate, the **hypothesys that corresponds to the mean value of the posterior distribution**: \n",
    "\n",
    "$\\hat H (E) = \\mathbb{E}[H | E] = \\int H p(H | E) dH$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2raCGdR4EAFQ",
    "outputId": "7415eff3-3d13-4cf9-bf08-c4dfaa76d069"
   },
   "outputs": [],
   "source": [
    "def Meanp(hypos, posterior):\n",
    "    total = 0.0\n",
    "    s = hypos * posterior\n",
    "    return s.mean()*len(hypos)\n",
    "\n",
    "print(int(Meanp(hypos, posterior)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pc-gdaLHEAFR"
   },
   "source": [
    "If we want to increse our knowledge, there are two ways to proceed:\n",
    "\n",
    "+ Get more data.\n",
    "+ Get more background information.\n",
    "\n",
    "For example, suppose that in addition to train 60 we also see trains 30 and 90. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "id": "G2Iox1ibEAFR",
    "outputId": "ee94a426-4232-46b0-ac9a-cfe8988f5d25"
   },
   "outputs": [],
   "source": [
    "hypos = range(1, 1001)\n",
    "posterior =  Normalize(Posterior(60, hypos, priors))\n",
    "posterior2 = Normalize(Posterior(30, hypos, posterior))\n",
    "posterior3 = Normalize(Posterior(90, hypos, posterior2))\n",
    "\n",
    "print(int(Meanp(hypos, posterior3)))\n",
    "\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    plt.figure(figsize=(10,2))\n",
    "    plt.ylim([-0.001,0.025])\n",
    "    plt.plot(hypos, posterior3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MS9ZoA5aEAFR"
   },
   "source": [
    "We can refactor our functions in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "id": "2bNzq1fgEAFR",
    "outputId": "109c3330-6a18-49f7-8df7-faa83517f0ed"
   },
   "outputs": [],
   "source": [
    "def Normalize(d):\n",
    "    total = d.sum()\n",
    "    factor = 1.0 / total\n",
    "    for i in range(len(d)):\n",
    "        d[i] *= factor\n",
    "    return d\n",
    "\n",
    "def Likelihood1(datum, hypo):\n",
    "    if hypo < datum:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1.0/hypo\n",
    "    \n",
    "def Posterior(datum, hypos, priors, likelihood):\n",
    "    import numpy as np\n",
    "    posterior = np.array([likelihood(datum, hypo) for hypo in hypos]) * priors\n",
    "    return posterior\n",
    "\n",
    "def Posterior_n(data, hypos, priors, likelihood):\n",
    "    p = priors\n",
    "    for d in data:\n",
    "        posterior =  Normalize(Posterior(d, hypos, p, likelihood))\n",
    "        p = posterior\n",
    "    return posterior\n",
    "\n",
    "def Meanp(hypos, posterior):\n",
    "    total = 0.0\n",
    "    s = hypos * posterior\n",
    "    return s.mean()*len(hypos)\n",
    "\n",
    "hypos = np.array(range(1, 1001))\n",
    "priors = np.array([1.0/len(hypos) for hypo in hypos])\n",
    "posteriors = Posterior_n([60,30,90], hypos, priors, Likelihood1)\n",
    "\n",
    "print('Mean of the posterior distribution with 1000 hypotheses: ', \\\n",
    "        int(Meanp(hypos, posteriors)))\n",
    "\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    plt.figure(figsize=(10,2))\n",
    "    plt.ylim([-0.001,0.025])\n",
    "    plt.plot(hypos, posteriors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gY2y505EEAFR"
   },
   "source": [
    "With more data, **posterior distributions based on different priors tend to converge**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tGAABkKKEAFR",
    "outputId": "09553c1c-ff0b-418d-8083-1cfe26385b86"
   },
   "outputs": [],
   "source": [
    "hypos = np.array(range(1, 501))\n",
    "priors = np.array([1.0/len(hypos) for hypo in hypos])\n",
    "posteriors = Posterior_n([60,60,90], hypos, priors, Likelihood1)\n",
    "print('Mean of the posterior distribution with 500 hypotheses: ', \n",
    "      int(Meanp(hypos, posteriors)))\n",
    "\n",
    "hypos = np.array(range(1, 2001))\n",
    "priors = np.array([1.0/len(hypos) for hypo in hypos])\n",
    "posteriors = Posterior_n([60,60,90], hypos, priors, Likelihood1)\n",
    "print('Mean of the posterior distribution with 2000 hypotheses: ', \n",
    "      int(Meanp(hypos, posteriors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWg2r8A_EAFR"
   },
   "source": [
    "If more data are not available, another option is to improve the priors by gathering more background information. It is probably not reasonable to assume that a train-operating company with 1000 locomotives is just as likely as a company with only 1.\n",
    "\n",
    "Let's suppose that the distribution of the total number of locomotives of a company tends to follow a **power law**. \n",
    "\n",
    "This law suggests that if there are 1000 companies with fewer than 10 locomotives, there might be 100 companies with 100 locomotives, 10 companies with 1000, and possibly one company with 10,000 locomotives.\n",
    "\n",
    "Mathematically, a power law means that the number of companies with a given size is inversely proportional to size:\n",
    "\n",
    "$$ f(x) = \\left( \\frac{1}{x} \\right)^\\alpha $$\n",
    "\n",
    "where $f(x)$ is the probability mass function of $x$ and $\\alpha$ is a parameter (that is often near 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "id": "Pn-ZcRWCEAFR",
    "outputId": "0971c1fd-80f5-4958-fa37-a3154609e3b8"
   },
   "outputs": [],
   "source": [
    "def Likelihood2(data, hypo, alpha=1.0):\n",
    "    if hypo < data:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return hypo**(-alpha)\n",
    "\n",
    "alpha = 1.0\n",
    "hypos = np.array(range(1, 1001))\n",
    "priors = Normalize(np.array([Likelihood2(0, hypo, alpha) for hypo in hypos]))\n",
    "\n",
    "print(int(Meanp(hypos, priors)))\n",
    "    \n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    plt.figure(figsize=(10,2))\n",
    "    plt.ylim([-0.01,0.14])\n",
    "    plt.xlim([-10,500])\n",
    "    plt.plot(hypos, priors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "id": "7JuaDX8eEAFR",
    "outputId": "0ca2f90c-e846-4217-d5f2-9ebbf7d12879"
   },
   "outputs": [],
   "source": [
    "alpha = 1.0\n",
    "\n",
    "hypos = np.array(range(1, 1001))\n",
    "priors = Normalize(np.array([Likelihood2(0, hypo, alpha) for hypo in hypos]))\n",
    "\n",
    "posteriors = Posterior_n([30,60,90], hypos, priors, Likelihood2)\n",
    "\n",
    "print('Mean of the posterior distribution with 1000 hypotheses: ', int(Meanp(hypos, posteriors)))\n",
    "\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    plt.figure(figsize=(10,2))\n",
    "    plt.ylim([-0.001,0.035])\n",
    "    plt.plot(hypos, posteriors)\n",
    "    plt.axvline(x=int(Meanp(hypos, posteriors)), ymin=0.0, ymax = 0.9, \\\n",
    "                linewidth=1, color='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "ra2d51xEEAFR",
    "outputId": "1e0a66bc-24f4-4ae3-8534-b865fa1568a7"
   },
   "outputs": [],
   "source": [
    "hypos = np.array(range(1, 501))\n",
    "priors = Normalize(np.array([Likelihood2(0, hypo, alpha) for hypo in hypos]))\n",
    "\n",
    "posteriors = Posterior_n([30,60,90], hypos, priors, Likelihood2)\n",
    "\n",
    "print('Mean of the posterior distribution with 500 hypotheses: ',int(Meanp(hypos, posteriors)))\n",
    "\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    plt.figure(figsize=(6,2))\n",
    "    plt.ylim([-0.001,0.035])\n",
    "    plt.xlim([0,2000])\n",
    "    plt.plot(hypos, posteriors)\n",
    "    plt.axvline(x=int(Meanp(hypos, posteriors)), ymin=0.0, ymax = 0.9, \\\n",
    "                linewidth=1, color='r')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "hypos = np.array(range(1, 2001))\n",
    "priors = Normalize(np.array([Likelihood2(0, hypo, alpha) for hypo in hypos]))\n",
    "\n",
    "posteriors = Posterior_n([30,60,90], hypos, priors, Likelihood2)\n",
    "\n",
    "print('Mean of the posterior distribution with 2000 hypotheses: ',int(Meanp(hypos, posteriors)))\n",
    "\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    plt.figure(figsize=(6,2))\n",
    "    plt.ylim([-0.001,0.035])\n",
    "    plt.plot(hypos, posteriors)\n",
    "    plt.axvline(x=int(Meanp(hypos, posteriors)), ymin=0.0, ymax = 0.9, \\\n",
    "                linewidth=1, color='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J571-GltEAFS"
   },
   "source": [
    "### MAP and credible intervals\n",
    "\n",
    "Once we have the **posterior distribution** we could be interested in summarizing the result with a **point estimate** or a **credible interval**.\n",
    "\n",
    "For point estimates we can use, the mean, the median or the **mode**. This last estimate is called **maximum a posteriori estimate (MAP)**.\n",
    "\n",
    "For intervals we usually report two values computed so that there is a 95% chance that the unknown value falls between them (or any other probability). These values define a **credible interval**.\n",
    "\n",
    "A simple way to compute the 95% credible interval is to add up the probabilities in the posterior distribution and record the values that correspond to probabilities 2.5% and 97.5%. In other words, the 2.5th and 97.5th percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "id": "mUqUEYW3EAFS",
    "outputId": "3da69352-b1ad-4ee3-ac6c-bd56e569e24e"
   },
   "outputs": [],
   "source": [
    "def Percentile(hypos, posterior, percentage):\n",
    "    import numpy as np\n",
    "    p = percentage / 100.0\n",
    "    cdf = np.cumsum(np.array(posterior))\n",
    "    total = 0\n",
    "    for i in range(len(hypos)):\n",
    "        total += posterior[i]\n",
    "        if total >= p:\n",
    "            return hypos[i] \n",
    "\n",
    "alpha = 1.0\n",
    "\n",
    "hypos = np.array(range(1, 1001))\n",
    "priors = Normalize(np.array([Likelihood2(0, hypo, alpha) for hypo in hypos]))\n",
    "posteriors = Posterior_n([30,60,90], hypos, priors, Likelihood2)      \n",
    "\n",
    "print('The credible interval is [', Percentile(hypos, posteriors, 2.5),',', \\\n",
    "            Percentile(hypos, posteriors, 97.5), ']')\n",
    "\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    fig = plt.figure(figsize=(10,2))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.ylim([-0.001,0.035])\n",
    "    plt.plot(hypos, posteriors)\n",
    "    plt.axvspan(Percentile(hypos, posteriors, 2.5), Percentile(hypos, posteriors, 97.5), facecolor='0.5', alpha=0.2)\n",
    "    plt.axvline(x=int(Meanp(hypos, posteriors)), ymin=0.0, ymax = 1, \\\n",
    "                linewidth=1, color='r')\n",
    "    ax.set_xlabel('The credible interval is [ 90 , 303 ]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxvizQ8QEAFS"
   },
   "source": [
    "For the previous example—the locomotive problem with a power law prior and three trains—the 90% credible interval is (90, 303). The width of this range suggests, correctly, that we are still quite uncertain about how many locomotives there are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WSpzzi7EAFS"
   },
   "source": [
    "## Bayesian hypotheses comparison.\n",
    "\n",
    "From a frequentist point of view, we say that an effect $H_A$ is statistically significant (or not) by computing the chances (likelihood) of the effect under a null hypothesis $P(E|H_0)$, but you can't conclude it is real.\n",
    "\n",
    "From a Bayesian point of view what we directly compute $P(H_A | E)$, where $H_A$ is the hypothesis the effect is real. \n",
    "\n",
    "By Bayes's Theorem:\n",
    "\n",
    "$$ P(H_A | E) = \\frac{P(E|H_A) P(H_A)}{P(E)} = \\frac{P(E|H_A) P(H_A)}{P(E | H_A)P(H_A) + P(E | H_0)P(H_0)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJAYCPu6EAFS"
   },
   "source": [
    "To compute $P(E | H_A)$, the likelihood term, we can follow a similar approach to the one employed to compute $P(E | H_0)$, by generating 1000 sample pairs, one from each distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SGlAzS2ZEAFS",
    "outputId": "b41df86b-48f4-4fbe-87da-4342898e8dba"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "file = open('2002FemPreg.dat', 'r')\n",
    "seed = 41\n",
    "np.random.seed(seed)\n",
    "\n",
    "def chr_int(a):\n",
    "    if a == '  ':\n",
    "        return 0\n",
    "    else:\n",
    "        return int(a)\n",
    "        \n",
    "preg=[]\n",
    "for line in file:\n",
    "    lst  = [int(line[:12]), int(line[274:276]), int(line[276]), \\\n",
    "                 chr_int(line[277:279]), float(line[422:440])]\n",
    "    preg.append(lst)\n",
    "    \n",
    "\n",
    "df = pd.DataFrame(preg)\n",
    "df.columns = ['caseid', 'prglength', 'outcome', 'birthord', 'finalwgt']\n",
    "\n",
    "#data cleaning\n",
    "df2 = df.drop(df.index[(df.outcome == 1) & (df['prglength'] > df['prglength'].median() + 6)])\n",
    "df2[(df2.outcome == 1) & \n",
    "    (df2['prglength'] > df2['prglength'].median() + 6)]\n",
    "df3 = df2.drop(df2.index[(df2.outcome == 1) &\n",
    "                         (df2['prglength'] < df2['prglength'].median() -10)])\n",
    "df3[(df3.outcome == 1) & \n",
    "    (df3['prglength'] < df3['prglength'].median() - 10)]\n",
    "\n",
    "firstbirth = df3[(df3.outcome == 1) & (df3.birthord == 1)]\n",
    "othersbirth = df3[(df3.outcome == 1) & (df3.birthord >= 2)]\n",
    "\n",
    "x = firstbirth['prglength']\n",
    "y = othersbirth['prglength']\n",
    "m= len(x)\n",
    "n= len(y)\n",
    "p = abs(x.mean() - y.mean())\n",
    "N = 1000\n",
    "\n",
    "# Compute p(E|H_0): all come from same population --> bootstrap from the total pool\n",
    "pool = np.concatenate([x,y])\n",
    "np.random.shuffle(pool)\n",
    "diff_0 = np.zeros(shape=(N))\n",
    "for i in range(N):\n",
    "    p1 = [random.choice(pool) for j in range(m)]\n",
    "    p2 = [random.choice(pool) for j in range(n)]\n",
    "    diff_0[i] = abs(np.mean(p1)-np.mean(p2))          # two sided test\n",
    "\n",
    "w1_0 = np.where(diff_0 > p)[0]      # counting how many differences are larger than the observed one\n",
    "p_0 = len(w1_0)/float(N)\n",
    "print('p(E|H_0) =', p_0, '(', p_0*100 ,'%)')\n",
    "\n",
    "# Compute p(E|H_A): firstbirth and othersbirth come from diferent populations --> bootstrap from each group\n",
    "diff_A = np.zeros(shape=(N))\n",
    "for i in range(N):\n",
    "    p1 = [random.choice(x.values) for j in range(m)]\n",
    "    p2 = [random.choice(y.values) for j in range(n)]\n",
    "    diff_A[i] = abs(np.mean(p1)-np.mean(p2))          # two sided test\n",
    "\n",
    "w1_A = np.where(diff_A > p)[0]      # counting how many differences are larger than the observed one\n",
    "p_A = len(w1_A)/float(N)\n",
    "print('p(E|H_A) =', p_A, '(', p_A*100 ,'%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPMDxRZmEAFS"
   },
   "source": [
    "Then, the probability of $P(E | H_A)$ is around 0.5. \n",
    "\n",
    "In absence of knowledge we will assume that $P(H_A) = P(H_0) = 0.5$. \n",
    "\n",
    "Then, we can compute the posterior probabilities of $H_A$ and $H_0$:\n",
    "\n",
    "$$ P(H_A | E) = \\frac{P(E|H_A) P(H_A)}{P(E)}$$\n",
    "\n",
    "$$ P(H_0 | E) = \\frac{P(E|H_0) P(H_0)}{P(E)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fDDujF-nEAFS",
    "outputId": "48cf06d6-1b89-478d-c329-41b0363ca2b2"
   },
   "outputs": [],
   "source": [
    "print('P(H_A|E):', 0.5 * 0.5 / (0.5 * 0.5 + 0.02 * 0.5))\n",
    "print('P(H_0|E):', 0.02 * 0.5 / (0.5 * 0.5 + 0.02 * 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kneOataaEAFS"
   },
   "source": [
    "So, by taking into account a new evidence we have increased our belief in the effect $H_A$ from 50% to 96%. This is not a decision rule but a change in our knowledge. It makes sense: the evidence supports the hypothesis!\n",
    "\n",
    "In our problem, based on expert knowledge, it could also make sense to consider that $P(H_A) = 0.01$ and $P(H_0) = 0.99$. In this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K6MdpDucEAFS",
    "outputId": "f7a5515c-6673-4b45-a7b4-ee2855a01934"
   },
   "outputs": [],
   "source": [
    "print('H_A:', 0.5 * 0.01 / (0.5 * 0.01 + 0.02 * 0.99))\n",
    "print('H_0:', 0.02 * 0.99 / (0.5 * 0.01 + 0.02 * 0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5OhnCx0SEAFT"
   },
   "source": [
    "We have increased our belief in the effect $H_A$ from 1% to 20%, but we still can believe in $H_0$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llifUnDVEAFT"
   },
   "source": [
    "The change strongly depends on our prior belief: <br><br>\n",
    "\n",
    "<center><small>Image from: Sellke et al. \"Calibration of ρ values for testing precise null hypotheses.\" Am. Stat. 55.1 (2001): 62-71.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AS5t0k41azPD"
   },
   "source": [
    "<img src=\"./images/p-graphic.jpg\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZaAz-Y4EAFT"
   },
   "source": [
    "# Bayesian Inference\n",
    "\n",
    "(from [*Bayesian Methods for Hackers, Cam Davidson-Pilon, 2013*](https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/))\n",
    "\n",
    "Suppose, naively, that you are unsure about the probability of heads in a coin flip. You believe there is some true underlying ratio, call it $θ \\in [0,1]$, but have no prior opinion on what $θ$ might be.\n",
    "\n",
    "We begin to flip a coin, and record the observations: either ``H`` or ``T``. This is our observed data. \n",
    "\n",
    "An interesting question to ask is how our inference changes as we observe more and more data? More specifically, **what do our posterior probabilities look like when we have little data, versus when we have lots of data**?\n",
    "\n",
    "> **Assumption**: our (parametric) distributions are generative models of the world.\n",
    "\n",
    "In probability theory and statistics, the **Bernoulli distribution**, named after Swiss scientist Jacob Bernoulli, is the probability distribution of a random variable which takes value 1 with success probability $θ$ and value 0 with failure probability $1-θ$. \n",
    "\n",
    "The probability distribution, over possible outcomes $k$, is:\n",
    "$$P(k|θ)=θ^k (1−θ)^{1−k}$$\n",
    "where $k∈\\{0,1\\}$ and $θ∈[0,1]$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuvtmB_uEAFT"
   },
   "source": [
    "We can use the Bernoulli likelihood function to determine the probability of seeing a particular sequence of $N$ flips, given by the set ${k_1,...,k_N}$.\n",
    "\n",
    "Since each of these flips is independent of any other, the probability of the sequence occuring is simply the product of the probability of each flip occuring (Binomial distribution).\n",
    "\n",
    "If we have a particular fairness parameter $θ$, then the probability of seeing this particular stream of flips, given $θ$, is given by:\n",
    "\n",
    "$$ P(\\{k_1,...,k_N\\} | θ ) =  \\prod_i P(k_i|θ) = \\prod_i θ^{k_i} (1−θ)^{1−k_i} $$\n",
    "\n",
    "What about if we are interested in the number of heads, say, in $N$ flips? \n",
    "\n",
    "If we denote by $z$ the number of heads appearing, then the formula above becomes:\n",
    "\n",
    "$$ P(z,N|θ) = θ^{z} (1−θ)^{N-z} $$\n",
    "That is, the probability of seeing $z$ heads, in $N$ flips, assuming a fairness parameter $θ$. \n",
    "\n",
    "We will use this formula when we come to determine our posterior belief distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfH8Si_KEAFT"
   },
   "source": [
    "## Bayes' Rule for Bayesian Inference\n",
    "\n",
    "$$P(θ|D)=\\frac{P(D|θ)P(θ)}{P(D)}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "+ $P(θ)$ is the prior. Our prior view on the probability of how fair the coin is.\n",
    "\n",
    "+ $P(θ|D)$ is the posterior. \n",
    "\n",
    "+ $P(D|θ)$ is the likelihood. If we knew the coin was fair, this tells us the probability of seeing a number of heads in a particular number of flips.\n",
    "\n",
    "+ $P(D)$ is the evidence. If we had multiple views of what the fairness of the coin is (but didn't know for sure), then this tells us the probability of seeing a certain sequence of flips for all possibilities of our belief in the coin's fairness.\n",
    "\n",
    "We are interested in the probability of the coin coming up heads as a function of the underlying fairness parameter $θ$.\n",
    "\n",
    "Regarding the likelihood: \n",
    "\n",
    "$$ P(D|θ) = P(z,N|θ) = θ^{z} (1−θ)^{N-z} $$\n",
    "\n",
    "Regarding the prior, we are going to choose the beta distribution:\n",
    "\n",
    "$$ P(θ | \\alpha, \\beta)=  \\frac{θ^{\\alpha-1} (1−θ)^{\\beta -1}}{B(\\alpha, \\beta)}  $$ \n",
    "\n",
    "where the term in the denominator is present to act as a normalising constant so that the area under the PDF actually sums to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "DVrc6amCflBB",
    "outputId": "89553eba-9ac6-4e6e-d998-81501dc6efab"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import beta\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define the distribution parameters to be plotted\n",
    "alpha_values = [0.5, 1.0, 3.0, 0.5]\n",
    "beta_values = [0.5, 1.5, 3.0, 1.5]\n",
    "linestyles = ['-', '--', ':', '-.']\n",
    "x = np.linspace(0, 1, 1002)[1:-1]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the distributions\n",
    "fig, ax = plt.subplots(figsize=(5, 3.75))\n",
    "\n",
    "for a, b, ls in zip(alpha_values, beta_values, linestyles):\n",
    "    dist = beta(a, b)\n",
    "\n",
    "    plt.plot(x, dist.pdf(x), ls=ls, c='black',\n",
    "             label=r'$\\alpha=%.1f,\\ \\beta=%.1f$' % (a, b))\n",
    "\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 3)\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(r'$p(x|\\alpha,\\beta)$')\n",
    "plt.title('Beta Distribution')\n",
    "\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_u4tKqEPEAFT"
   },
   "source": [
    "The most important reason for choosing a beta distribution is because it is a **conjugate prior** for the Bernoulli distribution.\n",
    "\n",
    "In Bayes' rule above we can see that the posterior distribution is proportional to the product of the prior distribution and the likelihood function:\n",
    "\n",
    "$$ P(θ|D) \\propto P(D|θ)P(θ) $$\n",
    "\n",
    "A **conjugate prior** is a choice of prior distribution, that when coupled with a specific type of likelihood function, provides a posterior distribution that is of the same family as the prior distribution.\n",
    "\n",
    "> The prior and posterior both have the same probability distribution family, but with differing parameters.\n",
    "\n",
    "If our prior is given by $beta(θ|α,β)$ and we observe $z$ heads in $N$ flips subsequently, then the posterior is given by $beta(θ|z+α,N−z+β)$.\n",
    "\n",
    "Below we plot a sequence of updating posterior probabilities as we observe increasing amounts of data (coin flips)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9V-9S12MEAFT",
    "outputId": "7f02652a-afe1-40f4-cfd7-fb012e7c9065"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "from IPython.core.pylabtools import figsize\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats as stats\n",
    "figsize(10, 9)\n",
    "\n",
    "# we use a (continous) prior for p by using the beta function\n",
    "dist = stats.beta\n",
    "\n",
    "n_trials = [0,1,2,3,4,5,8,15, 50, 500]\n",
    "\n",
    "# synthetic data generation\n",
    "# random number generation with a Bernoulli distribution with p=0.5\n",
    "# the probability of this sequence is modelled by the binomial distribution\n",
    "data = stats.bernoulli.rvs(0.5, size = n_trials[-1])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 681
    },
    "id": "Lt0yNrdWEAFT",
    "inputHidden": false,
    "outputHidden": false,
    "outputId": "9bdcec11-5370-4e48-b3c5-0b8edba6402d"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "x = np.linspace(0,1,100)\n",
    "\n",
    "for k, N in enumerate(n_trials):\n",
    "    sx = plt.subplot( len(n_trials)/2, 2, k+1)\n",
    "    plt.xlabel(\"$p$, probability of heads\") if k in [0,len(n_trials)-1] else None\n",
    "    plt.setp(sx.get_yticklabels(), visible=False)\n",
    "    \n",
    "    # counting the number of heads\n",
    "    heads = data[:N].sum()\n",
    "    \n",
    "    # in this case the posterior can be analitically computed\n",
    "    # because the conjugate prior of the Binomial distribution\n",
    "    # is a Beta distribution \n",
    "    y = dist.pdf(x, 1 + heads, 1 + N - heads )\n",
    "    \n",
    "    plt.plot( x, y, label= \"observe %d tosses,\\n %d heads\"%(N,heads) )\n",
    "    plt.fill_between( x, 0, y, color=\"#348ABD\", alpha = 0.4 )\n",
    "    plt.vlines( 0.5, 0, 4, color = \"k\", linestyles = \"--\", lw=1 )\n",
    "    leg = plt.legend()\n",
    "    leg.get_frame().set_alpha(0.4)\n",
    "    plt.autoscale(tight = True)\n",
    "\n",
    "plt.suptitle( \"Bayesian updating of posterior probabilities\", y = 1.02, fontsize = 14);\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjcswZklEAFU"
   },
   "source": [
    "## Reminder: Popular PDF's\n",
    "*(Source: https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers*)\n",
    "\n",
    "Let $Z$ be some random variable. Then associated with $Z$ is a *probability distribution function* that assigns probabilities to the different outcomes $Z$ can take. Graphically, a probability distribution is a curve where the probability of an outcome is proportional to the height of the curve.\n",
    "\n",
    "### Discrete Case\n",
    "\n",
    "If $Z$ is discrete, then its distribution is called a *probability mass function*, which measures the probability $Z$ takes on the value $k$, denoted $P(Z=k)$. \n",
    "\n",
    "There are a lot of popular probability mass functions, but let's introduce the first very useful probability mass function. \n",
    "\n",
    "We say $Z$ is *Poisson*-distributed if:\n",
    "\n",
    "$$P(Z = k) =\\frac{ \\lambda^k e^{-\\lambda} }{k!}, \\; \\; k=0,1,2, \\dots $$\n",
    "\n",
    "It can express the **probability of a given number of events occurring in a fixed interval of time and/or space if these events occur with a known average rate and independently of the time since the last event**.\n",
    "\n",
    "$\\lambda$ is called a parameter of the distribution, and it controls the distribution's shape. For the Poisson distribution, $\\lambda$ can be any positive number. By increasing $\\lambda$, we add more probability to larger values, and conversely by decreasing $\\lambda$ we add more probability to smaller values. One can describe $\\lambda$ as the *intensity* of the Poisson distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgNAvPw9EAFU"
   },
   "source": [
    "Unlike $\\lambda$, which can be any positive number, the value $k$ in the above formula must be a non-negative integer, i.e., $k$ must take on values 0,1,2, and so on. This is very important, because if you wanted to model a population you could not make sense of populations with 4.25 or 5.612 members. \n",
    "\n",
    "One useful property of the Poisson distribution is that its expected value is equal to its parameter, i.e.:\n",
    "\n",
    "$$E\\large[ \\;Z\\; | \\; \\lambda \\;\\large] = \\lambda $$\n",
    "\n",
    "This property is often used, so it's useful to remember. Below, we plot the probability mass distribution for different $\\lambda$ values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "p_Fa2L5hEAFU",
    "outputId": "33e9fe76-18dd-4fe7-dabb-7549a3efb959"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12.5, 4))\n",
    "\n",
    "import scipy.stats as stats\n",
    "a = np.arange(16)\n",
    "poi = stats.poisson\n",
    "lambda_ = [1.5, 4.25]\n",
    "colours = [\"#348ABD\", \"#A60628\",]\n",
    "\n",
    "plt.bar(a, poi.pmf(a, lambda_[0]), color=colours[0],\n",
    "        label=\"$\\lambda = %.1f$\" % lambda_[0], alpha=0.60,\n",
    "        edgecolor=colours[0], lw=\"3\")\n",
    "\n",
    "plt.bar(a, poi.pmf(a, lambda_[1]), color=colours[1],\n",
    "        label=\"$\\lambda = %.1f$\" % lambda_[1], alpha=0.60,\n",
    "        edgecolor=colours[1], lw=\"3\")\n",
    "\n",
    "\n",
    "plt.xticks(a + 0.4, a)\n",
    "plt.legend()\n",
    "plt.ylabel(\"probability of $k$\")\n",
    "plt.xlabel(\"$k$\")\n",
    "plt.title(\"Probability mass function of a Poisson random variable; differing $\\lambda$ values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEOChydAEAFU"
   },
   "source": [
    "### Continuous Case\n",
    "Instead of a probability mass function, a continuous random variable has a *probability density function*. \n",
    "\n",
    "An example of continuous random variable is a random variable with **exponential density**. The density function for an exponential random variable looks like this:\n",
    "\n",
    "$$f_Z(z | \\lambda) = \\lambda e^{-\\lambda z }, \\;\\; z\\ge 0$$\n",
    "\n",
    "Like a Poisson random variable, an exponential random variable can take on only non-negative values. But unlike a Poisson variable, the exponential can take on *any* non-negative values, including non-integral values such as 4.25 or 5.612401. \n",
    "\n",
    "This property makes it a poor choice for count data, which must be an integer, but a great choice for time data, temperature data, or any other *precise and positive* variables. The graph below shows two probability density functions with different $\\lambda$ values. \n",
    "\n",
    "Given a specific $\\lambda$, the expected value of an exponential random variable is equal to the inverse of $\\lambda$, that is:\n",
    "\n",
    "$$E[\\; Z \\;|\\; \\lambda \\;] = \\frac{1}{\\lambda}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569
    },
    "id": "z5ARpTcEEAFU",
    "outputId": "26d43443-86bf-4a6e-887d-fd278df6946d"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "a = np.linspace(0, 4, 100)\n",
    "expo = stats.expon\n",
    "\n",
    "mean = [0.5, 1]\n",
    "\n",
    "for l, c in zip(mean, colours):\n",
    "    plt.plot(a, expo.pdf(a, scale=1. / l), lw=3,\n",
    "             color=c, label=\"$\\lambda = %.1f$\" % l)\n",
    "    plt.fill_between(a, expo.pdf(a, scale=1. / l), color=c, alpha=.33)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel(\"PDF at $z$\")\n",
    "plt.xlabel(\"$z$\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.title(\"Probability density function of an Exponential random variable;\\\n",
    " differing $\\lambda$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZHdC8PvEAFU"
   },
   "source": [
    "### But what is $\\lambda \\;$?\n",
    "\n",
    "**This question is what motivates statistics**. In the real world, $\\lambda$ is hidden from us. \n",
    "\n",
    "We see only $Z$, and must go backwards to try and determine $\\lambda$. The problem is difficult because there is no one-to-one mapping from $Z$ to $\\lambda$. Many different methods have been created to solve the problem of estimating $\\lambda$, but since $\\lambda$ is never actually observed, no one can say for certain which method is best! \n",
    "\n",
    "Bayesian inference is concerned with *beliefs* about what $\\lambda$ might be. Rather than try to guess $\\lambda$ exactly, we can only talk about what $\\lambda$ is likely to be by assigning a probability distribution to $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dke8F1hrQ-l-"
   },
   "source": [
    "## Probabilistic Programming\n",
    "\n",
    "Probabilistic programming  allows for flexible specification and fitting of Bayesian statistical models. \n",
    "\n",
    "PyMC3 is a new, open-source PP framework with an intuitive and readable, yet powerful, syntax that is close to the natural syntax statisticians use to describe models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "829QDPudRyx5",
    "outputId": "a926d4a8-218f-41f7-e8a5-8e0d9abf3e62"
   },
   "outputs": [],
   "source": [
    "!apt-get install python3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rhi_xE9rQ-l-",
    "outputId": "e0fa0f81-18dd-4c31-b828-94eab416b084"
   },
   "outputs": [],
   "source": [
    "!pip install pymc3\n",
    "!pip install patsy\n",
    "import pymc3 as pm  \n",
    "import numpy as np  \n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1WEG5C6Q-l_"
   },
   "source": [
    "To introduce model definition, fitting and posterior analysis, we first consider a simple **Bayesian linear regression model** with normal priors on the parameters. \n",
    "\n",
    "We are interestedin predicting outcomes $Y$ as normally-distributed observations with $\\mu$ that is a linear function of two predictor variables, $X_1$ and $X_2$:\n",
    "\n",
    "$$ Y \\sim \\mathcal N (\\mu, \\sigma^2)$$\n",
    "$$ \\mu = \\alpha + \\beta_1 X_1 + \\beta_2 X_2 $$\n",
    "\n",
    "We will apply zero-mean normal priors with variance of 10 to both regression coefficients, which corresponds to weak information regarding the true parameter values. \n",
    "\n",
    "Since variances must be positive, we will also choose a half-normal distribution (normal distribution bounded below at zero) as the prior for $\\sigma$:\n",
    "\n",
    "$$ \\alpha \\sim \\mathcal N(0,10)$$\n",
    "$$ \\beta_i \\sim \\mathcal N(0,10)$$\n",
    "$$ \\sigma \\sim \\mathcal |N(0,10)|$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "ZgwTiCW1Q-l_",
    "outputId": "1efc236a-61a1-4c32-ac3c-a694e4d41f56"
   },
   "outputs": [],
   "source": [
    "# generating data\n",
    "\n",
    "# Intialize random number generator\n",
    "from numpy import random\n",
    "np.random.seed(123)\n",
    "\n",
    "# True parameter values\n",
    "\n",
    "alpha, sigma = 1, 1\n",
    "beta = [1, 2.5]\n",
    "size = 1000\n",
    "X1 = random.rand(size)\n",
    "X2 = random.rand(size)/5.0\n",
    "\n",
    "# Simulate outcome variable\n",
    "Y = alpha + beta[0]*X1 + beta[1]*X2 + np.random.randn(size)*sigma\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, sharex=True, figsize=(10,4))\n",
    "axes[0].scatter(X1, Y)\n",
    "axes[1].scatter(X2, Y)\n",
    "axes[0].set_ylabel('Y'); axes[0].set_xlabel('X1'); axes[1].set_xlabel('X2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "xF5KMTp0Q-l_",
    "outputId": "24e5bb67-1009-41c0-f12c-d6714b5ec34b"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(X1,X2,Y) # plot the point (2,3,4) on the figure\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulC1PINgQ-l_"
   },
   "source": [
    "### Model specification\n",
    "\n",
    "Specifying this model in PyMC3 is straightforward because the syntax is as close to the statistical notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OB_y5PICQ-l_"
   },
   "outputs": [],
   "source": [
    "from pymc3 import Model, Normal, HalfNormal\n",
    "\n",
    "# container for the model random variables\n",
    "basic_model = Model()\n",
    "\n",
    "# all PyMC3 objects introduced in the indented \n",
    "# code block below the with statement are added to the model behind the scenes\n",
    "with basic_model:\n",
    "    \n",
    "    # Priors (stochastic random variables) for unknown model parameters\n",
    "    alpha = Normal('alpha', mu=0, sd=10)\n",
    "    beta = Normal('beta', mu=0, sd=10, shape=2)\n",
    "    sigma = HalfNormal('sigma', sd=1)\n",
    "    \n",
    "    # Expected value of outcome\n",
    "    mu = alpha + beta[0]*X1 + beta[1]*X2\n",
    "    \n",
    "    # Likelihood (sampling distribution) of observations\n",
    "    Y_obs = Normal('Y_obs', mu=mu, sd=sigma, observed=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5SH5UsGQ-l_"
   },
   "source": [
    "### Model fitting\n",
    "\n",
    "Having completely specified our model, the next step is to obtain **posterior estimates for the unknown variables in the model**. \n",
    "\n",
    "**Ideally, we could calculate the posterior estimates analytically, but for most non-trivial models, this is not feasible.**\n",
    "\n",
    "We will consider two approaches, whose appropriateness depends on the structure of the model and the goals of the analysis: \n",
    "+ finding the maximum a posteriori (MAP) point using optimization methods, and \n",
    "Maximum a posteriori method+ computing summaries based on samples drawn from the posterior distribution using Markov Chain Monte Carlo (MCMC) sampling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "bs1r_GuCQ-mA",
    "outputId": "73e8df5b-ca54-4c15-b676-22f81bc931d4"
   },
   "outputs": [],
   "source": [
    "# Maximum a posteriori method (maximum of the log-posterior)\n",
    "\n",
    "from pymc3 import find_MAP\n",
    "\n",
    "map_estimate = find_MAP(model=basic_model)\n",
    "print(map_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjMbUTA-Q-mA"
   },
   "source": [
    "PyMC3 has the standard sampling algorithms like adaptive Metropolis-Hastings and adaptive slice sampling, but PyMC3's most capable step method is the No-U-Turn Sampler. \n",
    "\n",
    "NUTS is especially useful on models that have many continuous parameters,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "id": "uw1ec27rQ-mA",
    "outputId": "4c98dbe6-dd8b-4997-de86-f4ae4785bfcb"
   },
   "outputs": [],
   "source": [
    "# sampling method\n",
    "\n",
    "from pymc3 import sample\n",
    "\n",
    "with basic_model:\n",
    "    \n",
    "    # obtain starting values via MAP\n",
    "    start = find_MAP(model=basic_model)\n",
    "    \n",
    "    # draw 2000 posterior samples\n",
    "    trace = sample(2000, start=start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tL6eFpcUQ-mD"
   },
   "source": [
    "The sample function runs the `step` method(s) assigned (or passed) to it for the given number of iterations and returns a `Trace` object containing the samples collected, in the order they were collected. \n",
    "\n",
    "The trace object can be queried in a similar way to a dict containing a map from variable names to numpy.arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fA5GnbuhQ-mD",
    "outputId": "f7b540b1-ba8b-4249-d838-c2c33c58c36d"
   },
   "outputs": [],
   "source": [
    "trace['alpha'][-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cQowvLJQ-mE"
   },
   "source": [
    "### Posterior analysis\n",
    "\n",
    "PyMC3 provides plotting and summarization functions for inspecting the sampling output. A simple posterior plot can be created using `traceplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "oEFmUnJOQ-mE",
    "outputId": "8b1b7146-8540-456a-dafd-9d2f8ad8e5ff"
   },
   "outputs": [],
   "source": [
    "from pymc3 import traceplot\n",
    "\n",
    "traceplot(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192
    },
    "id": "VEUQdC6LQ-mE",
    "outputId": "4b450842-f889-4b84-8367-0544016ed846"
   },
   "outputs": [],
   "source": [
    "from pymc3 import summary\n",
    "\n",
    "summary(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7R89BjBEAFY"
   },
   "source": [
    "## Example: Inferring behaviour from text-message data.\n",
    "\n",
    "> You are given a series of daily text-message counts from a user of your system. The data, plotted over time, appears in the chart below. You are curious to know if the user's text-messaging habits have changed over time, either gradually or suddenly. How can you model this? (This is in fact my own text-message data. Judge my popularity as you wish.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567
    },
    "id": "-rw65b8sEAFY",
    "outputId": "f884c007-3f39-41e7-d364-a75f4849fa49"
   },
   "outputs": [],
   "source": [
    "\n",
    "file = open('txtdata.csv', 'r')\n",
    "\n",
    "count_data = np.loadtxt(file)\n",
    "n_count_data = len(count_data)\n",
    "plt.bar(np.arange(n_count_data), count_data, color=\"#348ABD\")\n",
    "plt.xlabel(\"Time (days)\")\n",
    "plt.ylabel(\"count of text-msgs received\")\n",
    "plt.title(\"Did the user's texting habits change over time?\")\n",
    "plt.xlim(0, n_count_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 803
    },
    "id": "5Gk3ycoOEAFY",
    "outputId": "ef52e5fc-d285-40a4-ae35-e660dd0bad2c"
   },
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "\n",
    "with pm.Model() as model:\n",
    "    alpha = 1.0/count_data.mean()  # Recall count_data is the\n",
    "                                   # variable that holds our txt counts\n",
    "                                   \n",
    "    lambda_1 = pm.Exponential(\"lambda_1\", alpha)\n",
    "    lambda_2 = pm.Exponential(\"lambda_2\", alpha)\n",
    "    \n",
    "    tau = pm.DiscreteUniform(\"tau\", lower=0, upper=n_count_data - 1)\n",
    "\n",
    "with model:\n",
    "    idx = np.arange(n_count_data) # Index\n",
    "    lambda_ = pm.math.switch(tau > idx, lambda_1, lambda_2)\n",
    "\n",
    "with model:\n",
    "    observation = pm.Poisson(\"obs\", lambda_, observed=count_data)\n",
    "    \n",
    "with model:\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(10000, tune=5000, step=step, return_inferencedata=False)\n",
    "\n",
    "lambda_1_samples = trace['lambda_1']\n",
    "lambda_2_samples = trace['lambda_2']\n",
    "tau_samples = trace['tau']\n",
    "\n",
    "figsize(12.5, 10)\n",
    "#histogram of the samples:\n",
    "\n",
    "ax = plt.subplot(311)\n",
    "ax.set_autoscaley_on(False)\n",
    "\n",
    "plt.hist(lambda_1_samples, histtype='stepfilled', bins=30, alpha=0.85,\n",
    "         label=\"posterior of $\\lambda_1$\", color=\"#A60628\", density=True)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(r\"\"\"Posterior distributions of the variables\n",
    "    $\\lambda_1,\\;\\lambda_2,\\;\\tau$\"\"\")\n",
    "plt.xlim([15, 30])\n",
    "plt.xlabel(\"$\\lambda_1$ value\")\n",
    "\n",
    "ax = plt.subplot(312)\n",
    "ax.set_autoscaley_on(False)\n",
    "plt.hist(lambda_2_samples, histtype='stepfilled', bins=30, alpha=0.85,\n",
    "         label=\"posterior of $\\lambda_2$\", color=\"#7A68A6\", density=True)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlim([15, 30])\n",
    "plt.xlabel(\"$\\lambda_2$ value\")\n",
    "\n",
    "plt.subplot(313)\n",
    "w = 1.0 / tau_samples.shape[0] * np.ones_like(tau_samples)\n",
    "plt.hist(tau_samples, bins=n_count_data, alpha=1,\n",
    "         label=r\"posterior of $\\tau$\",\n",
    "         color=\"#467821\", weights=w, rwidth=2.)\n",
    "plt.xticks(np.arange(n_count_data))\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.ylim([0, .75])\n",
    "plt.xlim([35, len(count_data)-20])\n",
    "plt.xlabel(r\"$\\tau$ (in days)\")\n",
    "plt.ylabel(\"probability\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "QWoaCIWnQ-mF",
    "outputId": "77aa1d2f-fa0b-4687-d829-a88953b5da2c"
   },
   "outputs": [],
   "source": [
    "figsize(12.5, 5)\n",
    "# tau_samples, lambda_1_samples, lambda_2_samples contain\n",
    "# N samples from the corresponding posterior distribution\n",
    "N = tau_samples.shape[0]\n",
    "expected_texts_per_day = np.zeros(n_count_data)\n",
    "for day in range(0, n_count_data):\n",
    "    # ix is a bool index of all tau samples corresponding to\n",
    "    # the switchpoint occurring prior to value of 'day'\n",
    "    ix = day < tau_samples\n",
    "    # Each posterior sample corresponds to a value for tau.\n",
    "    # for each day, that value of tau indicates whether we're \"before\"\n",
    "    # (in the lambda1 \"regime\") or\n",
    "    #  \"after\" (in the lambda2 \"regime\") the switchpoint.\n",
    "    # by taking the posterior sample of lambda1/2 accordingly, we can average\n",
    "    # over all samples to get an expected value for lambda on that day.\n",
    "    # As explained, the \"message count\" random variable is Poisson distributed,\n",
    "    # and therefore lambda (the poisson parameter) is the expected value of\n",
    "    # \"message count\".\n",
    "    expected_texts_per_day[day] = (lambda_1_samples[ix].sum()\n",
    "                                   + lambda_2_samples[~ix].sum()) / N\n",
    "\n",
    "\n",
    "plt.plot(range(n_count_data), expected_texts_per_day, lw=4, color=\"#E24A33\",\n",
    "         label=\"expected number of text-messages received\")\n",
    "plt.xlim(0, n_count_data)\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Expected # text-messages\")\n",
    "plt.title(\"Expected number of text-messages received\")\n",
    "plt.ylim(0, 60)\n",
    "plt.bar(np.arange(len(count_data)), count_data, color=\"#348ABD\", alpha=0.65,\n",
    "        label=\"observed texts per day\")\n",
    "\n",
    "plt.legend(loc=\"upper left\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUU_fB0gEAFY"
   },
   "source": [
    "More information: [Bayesian Methods for Hackers](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [
    "ak2GyEGiEAFO",
    "zj7UeDLfEAFP",
    "6B6kTVQUEAFP",
    "p0-NXiANEAFQ",
    "J571-GltEAFS",
    "bZHdC8PvEAFU"
   ],
   "name": "12. Bayesian Data Analysis.ipynb",
   "provenance": []
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
