{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data hunting and gathering (part 2)\n",
    "\n",
    "<img style = \"border-radius:20px;\" src = \"http://unadocenade.com/wp-content/uploads/2012/09/cavalls-de-valltorta.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sesión 2: Creando Nuestra Propia API Web - Scraping\n",
    "\n",
    "En esta sesión, nos adentramos en el mundo del web scraping, enfocándonos en técnicas y herramientas avanzadas para extraer datos de páginas web dinámicas.\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- **Entender HTML y CSS**: Comprender los fundamentos de la estructura HTML y el estilo CSS para navegar efectivamente por las páginas web.\n",
    "- **Selectores XPath**: Aprender a usar selectores XPath para localizar y extraer contenido específico de las páginas web.\n",
    "- **Scraping de Contenido Dinámico con Selenium**: Entender cómo raspar contenido generado dinámicamente, el cual las herramientas de scraping estándar no siempre pueden manejar.\n",
    "\n",
    "## Bibliotecas Adicionales de Python\n",
    "Necesitarás instalar estas bibliotecas de Python para las tareas de scraping:\n",
    "\n",
    "- **lxml**: Una poderosa biblioteca para procesar XML y HTML en Python.\n",
    "  - Instalar vía pip: `pip install lxml`\n",
    "- **selenium**: Una herramienta automatizada de navegador web para probar aplicaciones web, también útil para tareas complejas de scraping.\n",
    "  - Instalar vía pip: `pip install selenium`\n",
    "\n",
    "### Nota\n",
    "Asegúrate de que todo el software y las bibliotecas estén instalados antes de la sesión. Esto te permitirá participar activamente en los ejercicios y seguir los ejemplos de scraping.\n",
    "\n",
    "## 1. \"Creando Tu Propia API\": Web Scraping\n",
    "\n",
    "### Entendiendo el Web Scraping\n",
    "El web scraping se vuelve esencial cuando los datos están disponibles en la web pero no son accesibles a través de una API, o la API existente carece de ciertas funcionalidades o tiene términos de servicio restrictivos. En tales escenarios, el **Web Scraping** es la técnica que permite la extracción automatizada de estos datos, replicando el acceso que tendría visualmente un humano.\n",
    "\n",
    "### ¿Por Qué Web Scraping?\n",
    "- **Accesibilidad de Datos**: A veces, la única forma de acceder a ciertos datos es directamente desde las páginas web donde se muestran.\n",
    "- **Flexibilidad**: El web scraping permite personalizar la extracción de datos para necesidades específicas, eludiendo las limitaciones de las APIs existentes.\n",
    "\n",
    "### Preparándose para el Web Scraping: Entendiendo la Estructura de las Páginas Web\n",
    "Antes de adentrarse en el scraping, es crucial tener un conocimiento básico de la estructura de las páginas web y cómo se almacenan y presentan los datos. Esta sesión cubre:\n",
    "\n",
    "#### Páginas Estáticas Básicas de HTML y CSS\n",
    "- **HTML (Lenguaje de Marcado de Hipertexto)**: El lenguaje de marcado estándar utilizado para crear páginas web. Entender HTML es clave para identificar los datos que quieres raspar.\n",
    "- **CSS (Hojas de Estilo en Cascada)**: Se utiliza para describir la presentación de un documento escrito en HTML. Conocer CSS ayuda a localizar elementos específicos en una página.\n",
    "\n",
    "#### HTML Dinámico\n",
    "- **Ejemplo Básico de JavaScript Usando JQuery**: Los sitios web a menudo usan JavaScript para cargar datos dinámicamente. Entender cómo funciona esto es crucial para raspar datos de dichas páginas dinámicas.\n",
    "\n",
    "### 1.1 Fundamentos de HTML + CSS 101\n",
    "\n",
    "#### Entendiendo los Fundamentos de las Páginas Web\n",
    "\n",
    "Las páginas web más fundamentales están construidas usando HTML y CSS. Estas tecnologías cumplen dos propósitos primarios: **HTML (Lenguaje de Marcado de Hipertexto)** estructura y almacena el contenido, siendo el objetivo principal para el web scraping, mientras que **CSS (Hojas de Estilo en Cascada)** formatea y estiliza el contenido, destacando elementos visuales como fuentes, colores, bordes y diseño.\n",
    "\n",
    "#### HTML: La Estructura de la Web\n",
    "HTML es un lenguaje de marcado típicamente renderizado por navegadores web. Usa 'etiquetas' para definir elementos en una página web. Un formato de etiqueta típico incluye un nombre de etiqueta, atributos (si los hay) y el contenido entre etiquetas de apertura y cierre.\n",
    "\n",
    "#### Componentes Clave de un Archivo HTML\n",
    "\n",
    "- **Declaración DOCTYPE**: \n",
    "  - Comienza con `<!DOCTYPE html>`, indicando el uso de HTML5.\n",
    "  - Versiones anteriores de HTML tenían diferentes DOCTYPEs.\n",
    "\n",
    "- **Etiqueta HTML**: \n",
    "  - La etiqueta `html` (y su etiqueta de cierre `/html`) encierra todo el contenido de la página web.\n",
    "\n",
    "- **Cabeza y Cuerpo**: \n",
    "  - La sección `head` a menudo incluye la etiqueta `title`, definiendo el nombre de la página web, enlaces a hojas de estilo CSS y archivos JavaScript para comportamiento dinámico.\n",
    "  - El `body` contiene el contenido visible de la página web.\n",
    "\n",
    "- **Elementos Comunes de HTML**:\n",
    "  - **Encabezados y Párrafos**: Usa `h#` (donde # es un número) para encabezados y `p` para párrafos.\n",
    "  - **Hipervínculos**: Definidos con el atributo `href` en etiquetas `a` (ancla).\n",
    "  - **Imágenes**: Incrustadas usando etiquetas `img` con el atributo `src`. Nota: `img` se cierra por sí misma.\n",
    "\n",
    "#### Ejercicio: Construir una Página Web Básica en HTML\n",
    "\n",
    "Pongamos en práctica tu conocimiento de HTML:\n",
    "\n",
    "- Crea un archivo llamado 'ejemplo.html' en tu editor de texto favorito.\n",
    "- Construye una página web básica en HTML que contenga elementos como etiquetas `title`, `h1`, `p`, `img` y `a`. Recuerda que casi todas las etiquetas necesitan cerrarse con una `/tag`.\n",
    "\n",
    "Este ejercicio tiene como objetivo familiarizarte con la estructura básica de HTML y cómo varios elementos se unen para formar una página web.\n",
    "\n",
    "Si eres perezoso, ve a la carpeta de archivos y haz doble clic en \"ejemplo.html\". Puedes verificar el código html ejecutando la siguiente línea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<!-- Start of the HTML head section -->\n",
       "<head>\n",
       "    <!-- Title of the webpage -->\n",
       "    <title>\n",
       "        Basic knowledge for web scraping.\n",
       "    </title>\t\n",
       "</head>\n",
       "<!-- Start of the HTML body section -->\n",
       "<body>\n",
       "    <!-- Header 1 indicating the subject of the content -->\n",
       "    <h1>About HTML\n",
       "    </h1>\n",
       "    <!-- Paragraph explaining what HTML is and providing a link for further information -->\n",
       "    <p>Html (Hypertext markdown language) is the basic language to provide contents in the web. It is a tagged language. You can check more about it in <a href=\"http://www.w3.org/community/webed/wiki/HTML\">World Wide Web Consortium.</a></p>\n",
       "    \n",
       "    <!-- Paragraph indicating that one of the following images is clickable -->\n",
       "    <p> One of the following rubberduckies is clickable\n",
       "    </p>\n",
       "    <!-- Image of a rubber ducky; this one is not clickable -->\n",
       "    <p>\n",
       "        <img src = \"files/rubberduck.jpg\"/>\n",
       "    \n",
       "        <!-- Clickable image (hyperlinked) of a rubber ducky -->\n",
       "        <a href=\"http://www.pinterest.com/misscannabliss/rubber-duck-mania/\"><img src = \"files/rubberduck.jpg\"/></a>\n",
       "    </p>\n",
       "</body>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<!-- Start of the HTML head section -->\n",
    "<head>\n",
    "    <!-- Title of the webpage -->\n",
    "    <title>\n",
    "        Basic knowledge for web scraping.\n",
    "    </title>\t\n",
    "</head>\n",
    "<!-- Start of the HTML body section -->\n",
    "<body>\n",
    "    <!-- Header 1 indicating the subject of the content -->\n",
    "    <h1>About HTML\n",
    "    </h1>\n",
    "    <!-- Paragraph explaining what HTML is and providing a link for further information -->\n",
    "    <p>Html (Hypertext markdown language) is the basic language to provide contents in the web. It is a tagged language. You can check more about it in <a href=\"http://www.w3.org/community/webed/wiki/HTML\">World Wide Web Consortium.</a></p>\n",
    "    \n",
    "    <!-- Paragraph indicating that one of the following images is clickable -->\n",
    "    <p> One of the following rubberduckies is clickable\n",
    "    </p>\n",
    "    <!-- Image of a rubber ducky; this one is not clickable -->\n",
    "    <p>\n",
    "        <img src = \"files/rubberduck.jpg\"/>\n",
    "    \n",
    "        <!-- Clickable image (hyperlinked) of a rubber ducky -->\n",
    "        <a href=\"http://www.pinterest.com/misscannabliss/rubber-duck-mania/\"><img src = \"files/rubberduck.jpg\"/></a>\n",
    "    </p>\n",
    "</body>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entendiendo Páginas Estáticas HTML Antiguas vs. Actuales\n",
    "\n",
    "#### Páginas Estáticas HTML al Estilo Antiguo\n",
    "Las páginas HTML antiguas a menudo dependían de tablas y listas para estructurar el contenido.\n",
    "\n",
    "- **Listas**:\n",
    "  - **Listas Ordenadas (`ol`)**: Se utilizan para crear listas donde el orden importa, con cada elemento representado por `li` (list item).\n",
    "  - **Listas Desordenadas (`ul`)**: Se utilizan para listas donde el orden no es importante, nuevamente usando `li` para cada elemento.\n",
    "\n",
    "- **Tablas**:\n",
    "  - La etiqueta `table` se utiliza para crear una tabla.\n",
    "  - Cada fila de la tabla se marca con una etiqueta `tr`.\n",
    "  - Las columnas de la tabla se definen por elementos `td` (table data) dentro de cada fila.\n",
    "  - Las tablas pueden incluir un encabezado (`thead`) y un cuerpo (`tbody`).\n",
    "  - Los elementos `th` se utilizan de manera similar a `td` pero para encabezados.\n",
    "  - Para extender una celda a través de múltiples columnas, se utiliza `colspan` con el número de celdas a cubrir.\n",
    "\n",
    "#### Páginas Estáticas HTML Actuales\n",
    "Las páginas HTML modernas se centran más en el uso de contenedores y CSS para el diseño y estilo.\n",
    "\n",
    "- **Divisiones (`div`)**: \n",
    "  - La etiqueta `div` señala una división y se utiliza para definir un bloque de contenido. Es un contenedor versátil utilizado en el diseño web moderno.\n",
    "\n",
    "- **Span (`span`)**: \n",
    "  - La etiqueta `span` se utiliza para resaltar o estilizar una parte específica de un bloque de contenido. Es un contenedor en línea y se utiliza a menudo para modificaciones a pequeña escala de texto u otros elementos.\n",
    "\n",
    "Tanto los estilos antiguos como los actuales de HTML tienen sus usos, pero las prácticas modernas favorecen el uso de `div` y `span` junto con CSS para un diseño más flexible y adaptable.\n",
    "\n",
    "### Entendiendo CSS para el Web Scraping\n",
    "\n",
    "CSS, que significa Hojas de Estilo en Cascada, es un lenguaje de hojas de estilo utilizado para describir la presentación y formato de documentos HTML. En el web scraping, entender CSS es crucial para navegar y extraer datos de las páginas web eficazmente.\n",
    "\n",
    "#### ¿Qué es CSS?\n",
    "- **CSS** es un lenguaje diseñado para estilizar el contenido de archivos HTML. Mediante CSS, los desarrolladores web definen cómo deben aparecer varios elementos HTML en una página web.\n",
    "- El término **\"cascada\"** se refiere a la prioridad que se da a ciertas reglas de estilo sobre otras más genéricas. Esta jerarquía es un aspecto fundamental de CSS.\n",
    "\n",
    "#### El Papel de CSS en el Web Scraping\n",
    "- **Separación de Preocupaciones**: CSS permite una clara separación entre la estructura de HTML (contenido) y el estilo de la página web (apariencia). Esta separación hace que las páginas web sean más fáciles de diseñar y mantener, y también más fáciles de raspar.\n",
    "- **Selectores y Propiedades**:\n",
    "  - **Selectores** son patrones utilizados para seleccionar el(los) elemento(s) que se desea estilizar, o en el caso del scraping, los elementos que se desean extraer.\n",
    "  - **Propiedades** son los aspectos de los elementos que se desea estilizar, como color, fuente, ancho, altura y más.\n",
    "- **Orden de Cascada**:\n",
    "  - Los estilos se aplican en orden de especificidad, con selectores más específicos anulando los más generales. Los estilos en línea (directamente dentro de un elemento HTML) tienen la mayor especificidad.\n",
    "\n",
    "#### Ejemplo en Web Scraping\n",
    "Considera una página web con el siguiente HTML y CSS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```html\n",
    "<!-- HTML Example -->\n",
    "<div class=\"product-description\">\n",
    "    <p>Awesome product</p>\n",
    "</div>\n",
    "```\n",
    "\n",
    "Now, css example:\n",
    "\n",
    "```css\n",
    "/* CSS Example */\n",
    ".product-description p {\n",
    "    color: blue;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, el CSS se dirige a un elemento `p` dentro de un `div` de la clase `product-description` y cambia el color de su texto a azul. Entender cómo se aplica esta regla de CSS ayuda en el scraping de datos de manera precisa.\n",
    "\n",
    "### Conclusión\n",
    "Para el web scraping, CSS no se trata solo de entender la estética de la página web; se trata de comprender de manera integral la estructura de la página web. Este entendimiento es crucial para una extracción de datos efectiva.\n",
    "\n",
    "### Ejemplo en Python: Web Scraping Usando Selectores CSS\n",
    "\n",
    "Este ejemplo demuestra cómo raspar una página web (el blog oficial de Python) y extraer contenido específico utilizando selectores CSS en Python. Utilizamos las bibliotecas `requests` y `BeautifulSoup` para lograr esto.\n",
    "\n",
    "#### Explicación del Script\n",
    "\n",
    "1. **Import Libraries**:\n",
    "   - `requests` for sending HTTP requests.\n",
    "   - `BeautifulSoup` from `bs4` for parsing HTML content.\n",
    "\n",
    "   ```python\n",
    "   import requests\n",
    "   from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the website we want to scrape\n",
    "# In this case, we are targeting the Python.org blog page\n",
    "url = \"https://www.python.org/blogs/\"\n",
    "\n",
    "# Send a GET request to the specified URL\n",
    "# This request fetches the HTML content of the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "# 'BeautifulSoup' is a Python library for parsing HTML documents\n",
    "# It creates a parse tree from page source code that can be used to extract data easily\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is soup if you print it:\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a CSS selector to extract specific elements\n",
    "# Here, we select all 'h2' elements (commonly used for titles/headings in HTML)\n",
    "# 'select' is a method that finds all instances of a tag with the specified CSS path\n",
    "titles = soup.select('h2')\n",
    "\n",
    "# Iterate through the extracted titles and print them\n",
    "# 'get_text()' extracts the text part of the HTML element, and 'strip()' removes leading/trailing whitespaces\n",
    "for title in titles:\n",
    "    print(title.get_text().strip())\n",
    "\n",
    "# This script prints out all the text content of 'h2' tags found on the Python blog page\n",
    "# It provides an example of how to extract and print specific parts of a webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio de Web Scraping: Extracción de Titulares de Noticias de Tecnología de la BBC\n",
    "\n",
    "#### Objetivo\n",
    "Escribir un script en Python para raspar titulares de la sección de noticias de tecnología de la BBC y categorizarlos basándose en palabras clave.\n",
    "\n",
    "#### Detalles de la Tarea\n",
    "\n",
    "1. **Sitio Web para Raspar**:\n",
    "   - Dirigirse a la sección 'Tecnología' de la BBC: [Noticias de Tecnología de la BBC](https://www.bbc.co.uk/news/technology).\n",
    "\n",
    "2. **Requisito de Scraping**:\n",
    "   - Raspar los titulares principales de la página, que se encuentran típicamente en etiquetas `h3` o en una clase específica.\n",
    "\n",
    "3. **Categorización**:\n",
    "   - Categorizar los titulares basándose en palabras clave predefinidas como 'Apple', 'Microsoft', 'Google', etc.\n",
    "   - Contar el número de titulares que caen en cada categoría.\n",
    "\n",
    "4. **Salida**:\n",
    "   - Imprimir cada titular junto con su respectiva categoría.\n",
    "   - Resumir con el recuento de titulares en cada categoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the BBC technology news section\n",
    "url = \"https://www.bbc.co.uk/news/technology\"\n",
    "\n",
    "# Send a GET request and parse the HTML content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Define categories and associated keywords\n",
    "categories = {\n",
    "    'Apple': ['Apple', 'iPhone', 'iPad'],\n",
    "    'Microsoft': ['Microsoft', 'Windows', 'Bill Gates'],\n",
    "    'Google': ['Google', 'Android', 'Alphabet']\n",
    "    # Add more categories as needed\n",
    "}\n",
    "\n",
    "# Function to determine the category of a headline\n",
    "def categorize_headline(headline):\n",
    "    # Logic to determine the category based on keywords\n",
    "    # Return the category name if a keyword is found, else return 'Other'\n",
    "    pass\n",
    "\n",
    "# Scrape and process the headlines\n",
    "# Look for 'h3' tags or other relevant tags\n",
    "# Use the categorize_headline function to categorize each headline\n",
    "# Print each headline and its category\n",
    "\n",
    "# Print the count of headlines in each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Seleccionando Elementos con XPath\n",
    "\n",
    "XPath, o XML Path Language, es una herramienta versátil y robusta para navegar y seleccionar elementos dentro de documentos HTML. Aunque las bibliotecas Beautiful Soup y requests son comúnmente usadas para el web scraping, XPath ofrece un enfoque único y poderoso para extraer datos de páginas web.\n",
    "\n",
    "### ¿Qué es XPath?\n",
    "\n",
    "XPath fue diseñado originalmente para navegar documentos XML, pero es igualmente aplicable a HTML, que comparte una similitud estructural con XML. XPath te permite especificar la ubicación precisa de elementos o datos dentro de un documento HTML usando una sintaxis concisa y expresiva.\n",
    "\n",
    "### Diferenciadores Clave:\n",
    "\n",
    "Aquí hay algunos diferenciadores clave que distinguen a XPath de otros enfoques de web scraping:\n",
    "\n",
    "1. **Selección Granular**: XPath proporciona un control granular sobre la selección de elementos. A diferencia de Beautiful Soup, que a menudo requiere múltiples iteraciones y filtrado, XPath te permite apuntar directamente a elementos basados en sus atributos, etiquetas o posiciones dentro del documento.\n",
    "\n",
    "2. **Navegación Jerárquica**: XPath sobresale en navegar la estructura jerárquica de documentos HTML. Te permite atravesar el árbol del documento, moviéndote hacia arriba, abajo o a través de ramas con facilidad.\n",
    "\n",
    "3. **Consultas Precisas**: Con XPath, puedes crear consultas precisas para extraer datos específicos. Por ejemplo, puedes apuntar a elementos con atributos específicos, como seleccionar todos los elementos `<a>` con una clase particular o localizar elementos dentro de elementos padres específicos.\n",
    "\n",
    "4. **Extracción de Texto**: La función `text()` de XPath simplifica la extracción del contenido de texto de los elementos. Esto es particularmente útil para raspar datos de texto, como titulares, párrafos o descripciones de productos.\n",
    "\n",
    "### Cómo Usar XPath:\n",
    "\n",
    "Para utilizar XPath para el web scraping, típicamente sigues estos pasos:\n",
    "\n",
    "1. **Enviar una Solicitud HTTP**: Usa una biblioteca como requests para enviar una solicitud GET HTTP a la página web que deseas raspar. Esto recupera el contenido HTML de la página.\n",
    "\n",
    "2. **Parsear el HTML**: Una vez que tienes el contenido HTML, pásalo por una biblioteca como lxml o lxml.html. Este paso construye una representación estructurada de la página web que puedes navegar con XPath.\n",
    "\n",
    "3. **Construir Expresiones XPath**: Formula expresiones XPath que apunten a los elementos o datos específicos que deseas extraer. Las expresiones XPath pueden variar en complejidad, permitiéndote adaptarte a diferentes estructuras de páginas web.\n",
    "\n",
    "4. **Aplicar Expresiones XPath**: Aplica tus expresiones XPath al documento HTML parseado para seleccionar los elementos o datos deseados. Este proceso filtra efectivamente el contenido HTML para capturar solo lo que necesitas.\n",
    "\n",
    "5. **Recuperar y Procesar Datos**: Recupera los elementos o datos seleccionados usando las consultas XPath y procésalos según sea necesario para tu tarea de scraping.\n",
    "\n",
    "En resumen, XPath es una herramienta poderosa para el web scraping que ofrece una selección precisa y eficiente de elementos dentro de documentos HTML. Mientras que bibliotecas como Beautiful Soup y requests son valiosas, XPath proporciona una capa adicional de control y flexibilidad, haciéndolo una elección valiosa para proyectos de scraping avanzados.\n",
    "\n",
    "### Entendiendo la Sintaxis de XPath\n",
    "\n",
    "- **Ruta Absoluta (`/`)**: \n",
    "  - Usar una barra inclinada indica una ruta absoluta desde el elemento raíz.\n",
    "  - Ejemplo: `xpath('/html/body/p')` selecciona todos los elementos de párrafo (`<p>`) directamente bajo el `<body>` dentro del elemento raíz `<html>`.\n",
    "\n",
    "- **Ruta Relativa (`//`)**:\n",
    "  - Las dobles barras inclinadas indican una ruta relativa, lo que significa que la selección puede comenzar en cualquier lugar de la jerarquía del documento.\n",
    "  - Ejemplo: `xpath('//a/div')` encuentra todos los elementos `<div>` que son descendientes de etiquetas `<a>`, independientemente de su ubicación específica en el documento.\n",
    "\n",
    "- **Comodines (`*`)**:\n",
    "  - El asterisco actúa como un comodín, representando cualquier elemento.\n",
    "  - Ejemplo: `xpath('//a/div/*')` selecciona todos los elementos que son hijos de etiquetas `<div>` bajo etiquetas `<a>`, en cualquier lugar del documento.\n",
    "  - Otro ejemplo: `xpath('/*/*/div')` encuentra elementos `<div>` que están en el segundo nivel de la jerarquía desde la raíz.\n",
    "\n",
    "- **Seleccionando Elementos Específicos (Usando Corchetes)**:\n",
    "  - Si una selección devuelve múltiples elementos, puedes especificar cuál seleccionar usando corchetes.\n",
    "  - Ejemplo: `xpath('//a/div[1]')` selecciona el primer `<div>` en el conjunto; `xpath('//a/div[last()]')` selecciona el último `<div>`.\n",
    "\n",
    "### Trabajando con Atributos\n",
    "\n",
    "- **Seleccionando Atributos (`@`)**:\n",
    "  - El símbolo `@` se utiliza para trabajar con atributos de elementos.\n",
    "  - Ejemplo: `xpath('//@name')` selecciona todos los atributos llamados 'name' en el documento.\n",
    "  - Para seleccionar elementos `<div>` con un atributo 'name': `xpath('//div[@name]')`.\n",
    "  - Para seleccionar elementos `<div>` sin ningún atributo: `xpath('//div[not(@*)]')`.\n",
    "  - Para encontrar elementos `<div>` con un valor de atributo 'name' específico: `xpath('//div[@name=\"chachiname\"]')`.\n",
    "\n",
    "### Utilizando Funciones Incorporadas\n",
    "\n",
    "- XPath viene con varias funciones incorporadas para ayudar en la selección de elementos.\n",
    "  - `contains()`: Selecciona elementos que contienen una subcadena específica. Ejemplo: `xpath('//*[contains(name(),'iv')]')`.\n",
    "  - `count()`: Se utiliza para selección condicional basada en el conteo de hijos. Ejemplo: `xpath('//*[count(div)=2]')`.\n",
    "\n",
    "### Combinando Rutas y Seleccionando Parientes\n",
    "\n",
    "- **Combinando Rutas (`|`)**:\n",
    "  - Usa el símbolo de tubería para combinar rutas, funcionando como un operador OR.\n",
    "  - Ejemplo: `xpath('/div/p|/div/a')` selecciona elementos que coinciden con `div/p` o `div/a`.\n",
    "\n",
    "- **Seleccionando Parientes**:\n",
    "  - Puedes referirte a varios aspectos relacionales como padre, ancestros, hijos o descendientes.\n",
    "  - Ejemplo: `xpath('//div/div/parent::*')` selecciona los elementos padres de las rutas `div/div`.\n",
    "\n",
    "Entender XPath es esencial para el web scraping efectivo, ya que permite un direccionamiento y extracción precisos de datos basados en la estructura de una página web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "\n",
    "# URL of the website we want to scrape\n",
    "# For this example, we'll use a news website like the BBC technology page\n",
    "url = \"https://www.bbc.co.uk/news/technology\"\n",
    "\n",
    "# Send a GET request to the URL to fetch the webpage's HTML content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the webpage\n",
    "# The html.fromstring method constructs an lxml HTML document from the response text\n",
    "tree = html.fromstring(response.content)\n",
    "\n",
    "# Use XPath to select specific elements\n",
    "# In this case, we'll attempt to extract text content from the page\n",
    "# The XPath expression here captures all text content within the HTML structure\n",
    "# Note: The actual XPath may vary depending on the webpage's HTML structure\n",
    "headlines = tree.xpath('//text()')\n",
    "\n",
    "\n",
    "# Print each extracted headline\n",
    "# The text() function in XPath extracts the text content of the selected elements\n",
    "for headline in headlines:\n",
    "    print(headline.strip())\n",
    "\n",
    "# This script prints all the text content of <h3> tags found on the BBC Technology page\n",
    "# It demonstrates how to use XPath for extracting specific information from a webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0. Comenzando con Selenium\n",
    "\n",
    "Selenium es una herramienta poderosa utilizada principalmente para automatizar navegadores web. Se utiliza ampliamente en áreas como el web scraping, pruebas automatizadas y automatización de tareas administrativas basadas en web.\n",
    "\n",
    "### Introducción a Selenium Sin Geckodriver\n",
    "\n",
    "Tradicionalmente, Selenium funciona en conjunto con un controlador específico para cada navegador, como geckodriver para Firefox o chromedriver para Chrome. Sin embargo, desarrollos recientes han permitido que ciertos navegadores sean controlados directamente por Selenium sin la necesidad de un controlador adicional:\n",
    "\n",
    "- **Chrome**: Versiones recientes de Google Chrome pueden ser controladas directamente por Selenium a través del Protocolo Chrome DevTools. Esto simplifica el proceso de configuración ya que no necesitas descargar y configurar chromedriver por separado.\n",
    "\n",
    "- **Microsoft Edge**: Similar a Chrome, el navegador Edge (versión Chromium) también puede ser automatizado directamente usando Selenium con sus capacidades de controlador integradas.\n",
    "\n",
    "Este enfoque de usar Selenium sin un controlador adicional agiliza las tareas de automatización del navegador, haciéndolo más accesible y fácil de configurar, especialmente para principiantes y aquellos que buscan configurar rápidamente interacciones automatizadas del navegador.\n",
    "\n",
    "## 2.1 Conceptos Básicos de Selenium WebDriver\n",
    "\n",
    "### Entendiendo WebDriver\n",
    "\n",
    "WebDriver es un componente clave del conjunto de herramientas Selenium. Actúa como una interfaz para interactuar con el navegador web, permitiéndote controlarlo programáticamente. WebDriver puede realizar operaciones como abrir páginas web, hacer clic en botones, ingresar texto en formularios y extraer datos de páginas web.\n",
    "\n",
    "#### Funciones Clave de WebDriver\n",
    "- **Abrir una Página Web**: WebDriver puede navegar a una URL específica.\n",
    "- **Localizar Elementos**: Puede encontrar elementos en una página web basados en sus atributos (como ID, nombre, XPath).\n",
    "- **Interactuar con Elementos**: WebDriver puede simular acciones como hacer clic en botones, escribir texto y enviar formularios.\n",
    "\n",
    "### Interactuando con Elementos Web\n",
    "\n",
    "Puedes localizar e interactuar con elementos en una página web usando varios métodos proporcionados por WebDriver. La elección del método depende de los atributos de los elementos HTML que estás apuntando.\n",
    "\n",
    "- **find_element_by_id**: Localiza un elemento por su ID único.\n",
    "- **find_element_by_name**: Encuentra un elemento por su atributo de nombre.\n",
    "- **find_element_by_xpath**: Utiliza consultas XPath para localizar elementos, proporcionando una forma poderosa de navegar el DOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Selenium WebDriver Python Examples\n",
    "#### Example 1: Opening a Web Page\n",
    "\n",
    "#This example demonstrates how to open a web page using Selenium WebDriver.\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open a web page\n",
    "driver.get(\"https://www.python.org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo 2: Web Scraping de Salarios de Jugadores de la NBA**\n",
    "\n",
    "Este ejemplo demuestra cómo raspar datos de salarios de jugadores de la NBA de un sitio web usando Selenium en Python. Es una ilustración práctica de cómo Selenium puede ser utilizado para automatizar la navegación web y extraer datos específicos de páginas web. El script navega a través de diferentes páginas para cada temporada de la NBA, recopila los nombres de los jugadores y sus correspondientes salarios, y organiza estos datos en un DataFrame de pandas para cada año desde 1990 hasta 2018. Este es un ejemplo útil para aprender cómo manejar elementos web, extraer texto y manejar datos usando pandas en Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "from selenium import webdriver  # Used to automate web browser interaction\n",
    "from selenium.webdriver.common.by import By  # Helps in locating elements on web pages\n",
    "import pandas as pd  # Pandas library for data manipulation and analysis\n",
    "\n",
    "# Creating an empty DataFrame with specified columns\n",
    "# This DataFrame will be used to store the scraped data\n",
    "df = pd.DataFrame(columns=['Player', 'Salary', 'Year'])\n",
    "\n",
    "# Initializing the Chrome WebDriver\n",
    "# This opens up a Chrome browser window for web scraping\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Looping through the years 2017 to 2018\n",
    "for yr in range(2017, 2019):\n",
    "    # Constructing the URL for each year by appending the year range to the base URL\n",
    "    page_num = str(yr) + '-' + str(yr + 1) + '/'\n",
    "    url = 'https://hoopshype.com/salaries/players/' + page_num\n",
    "    driver.get(url)  # Navigating to the constructed URL in the browser\n",
    "    \n",
    "    # Finding all player name elements on the page using their XPATH\n",
    "    # XPATH is a syntax used to navigate through elements and attributes in an XML document\n",
    "    players = driver.find_elements(By.XPATH, '//td[@class=\"name\"]')\n",
    "\n",
    "    # Similarly, finding all salary elements on the page using their XPATH\n",
    "    salaries = driver.find_elements(By.XPATH, '//td[@class=\"hh-salaries-sorted\"]')\n",
    "    \n",
    "    # Extracting the text from each player element and storing in a list\n",
    "    players_list = [player.text for player in players]\n",
    "\n",
    "    # Extracting the text from each salary element and storing in a list\n",
    "    salaries_list = [salary.text for salary in salaries]\n",
    "    \n",
    "    # Pairing each player's name with their salary and year using the zip function\n",
    "    data_tuples = list(zip(players_list[1:], salaries_list[1:]))\n",
    "    \n",
    "    # Creating a temporary DataFrame for the current year\n",
    "    # This DataFrame contains the player names, their salaries, and the year\n",
    "    temp_df = pd.DataFrame(data_tuples, columns=['Player', 'Salary'])\n",
    "    temp_df['Year'] = yr\n",
    "\n",
    "    # Appending the temporary DataFrame to the master DataFrame\n",
    "    # ignore_index=True is used to ensure the index continues correctly in the master DataFrame\n",
    "    df = df.append(temp_df, ignore_index=True)\n",
    "\n",
    "# Closing the WebDriver after completing the scraping\n",
    "# This is important to free up resources and avoid potential memory leaks\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business challenge: **Análisis del Mercado de Alquileres en Barcelona: Un Proyecto de Web Scraping y Visualización de Datos**\n",
    "\n",
    "## Objetivo:\n",
    "El objetivo es desarrollar un web scraper basado en Python para extraer datos de propiedades de alquiler de Idealista para diferentes barrios de Barcelona. Estos datos serán analizados usando Power BI para descubrir insights sobre el mercado de alquileres de la ciudad.\n",
    "\n",
    "## Alcance:\n",
    "- **Web Scraping**: Extraer puntos clave de datos como precios de alquiler, tamaño de la propiedad, número de habitaciones y ubicaciones de barrios de Idealista.\n",
    "- **Análisis y Visualización de Datos**: Analizar los datos raspados para identificar tendencias y patrones, luego visualizar estos hallazgos usando Power BI.\n",
    "\n",
    "## Pasos y Aplicación de la Metodología Ágil:\n",
    "\n",
    "### 1. Iniciación y Planificación del Proyecto (Sprint 0)\n",
    "- **Configuración del Equipo**: Formar equipos multifuncionales con roles como Scrum Master, Propietario del Producto y Analistas de Datos.\n",
    "- **Recolección de Requisitos**: Definir los puntos específicos de datos a ser raspados de Idealista.\n",
    "- **Selección de Herramientas**: Elegir herramientas apropiadas para el web scraping (por ejemplo, Python con bibliotecas como BeautifulSoup, Selenium) y para la visualización de datos (Power BI).\n",
    "- **Creación del Backlog**: Crear un backlog de producto que comprenda historias de usuario (por ejemplo, \"Como analista de datos, quiero raspar precios de alquiler para poder analizar el alquiler promedio en cada barrio\").\n",
    "\n",
    "### 2. Ejecución del Sprint\n",
    "- **Planificación del Sprint**: Desglosar el backlog en tareas más pequeñas y manejables para ser completadas en cada sprint (por ejemplo, configurar el entorno de scraping, diseñar el modelo de datos, etc.).\n",
    "- **Reuniones Diarias**: Realizar breves reuniones diarias para discutir el progreso, obstáculos y próximos pasos.\n",
    "- **Desarrollo y Pruebas**: Realizar desarrollo iterativo, con pruebas regulares para asegurar la precisión y fiabilidad de los datos.\n",
    "- **Revisión del Sprint**: Al final de cada sprint, revisar el trabajo completado y demostrar la funcionalidad.\n",
    "- **Retrospectiva del Sprint**: Reflexionar sobre el proceso del sprint para identificar mejoras para el próximo sprint.\n",
    "\n",
    "### 3. Fase de Web Scraping\n",
    "- Implementar scripts de web scraping para extraer los datos requeridos de Idealista.\n",
    "- Asegurar el cumplimiento de las políticas de web scraping de Idealista y consideraciones legales.\n",
    "\n",
    "### 4. Análisis y Visualización de Datos\n",
    "- Limpiar y preprocesar los datos raspados para el análisis.\n",
    "- Usar Power BI para crear dashboards interactivos y visualizaciones que resalten aspectos clave del mercado de alquileres en Barcelona.\n",
    "\n",
    "### 5. Revisión Final y Presentación\n",
    "- Compilar los hallazgos e insights en un informe comprensivo.\n",
    "- Presentar el análisis de datos y visualizaciones a los stakeholders o en un entorno de clase.\n",
    "\n",
    "### Entregables:\n",
    "- Código fuente para el script de web scraping.\n",
    "- Archivos de dashboard de Power BI.\n",
    "- Informe final detallando la metodología, hallazgos e insights.\n",
    "\n",
    "### Resultados de Aprendizaje:\n",
    "- Aplicación práctica de web scraping y análisis de datos.\n",
    "- Experiencia en el uso de metodología Ágil para la gestión de proyectos.\n",
    "- Mejora de la colaboración y habilidades de trabajo en equipo.\n",
    "- Competencia en el uso de Python para la recolección de datos y Power BI para la visualización de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Respuestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the BBC technology news section\n",
    "url = \"https://www.bbc.co.uk/news/technology\"\n",
    "\n",
    "# Send a GET request and parse the HTML content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Define categories and associated keywords\n",
    "categories = {\n",
    "    'Apple': ['Apple', 'iPhone', 'iPad'],\n",
    "    'Microsoft': ['Microsoft', 'Windows', 'Bill Gates'],\n",
    "    'Google': ['Google', 'Android', 'Alphabet']\n",
    "    # Add more categories as needed\n",
    "}\n",
    "\n",
    "# Function to determine the category of a headline\n",
    "def categorize_headline(headline):\n",
    "    for category, keywords in categories.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in headline:\n",
    "                return category\n",
    "    return 'Other'\n",
    "\n",
    "# Scrape and process the headlines\n",
    "# Look for 'h3' tags or other relevant tags\n",
    "headlines = soup.find_all('h3')\n",
    "category_counts = {category: 0 for category in categories.keys()}\n",
    "category_counts['Other'] = 0\n",
    "\n",
    "for h in headlines:\n",
    "    headline_text = h.get_text().strip()\n",
    "    category = categorize_headline(headline_text)\n",
    "    category_counts[category] += 1\n",
    "    print(f\"Headline: {headline_text}\\nCategory: {category}\\n\")\n",
    "\n",
    "# Print the count of headlines in each category\n",
    "print(\"Headline Counts by Category:\")\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"{category}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
