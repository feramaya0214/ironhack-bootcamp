{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning II\n",
    "\n",
    "1. Redes Neuronales\n",
    "    * Redes Neuronales de varias capas (multilayer)\n",
    "\n",
    "2. DeepLearning en Keras\n",
    "\n",
    "3. Convolutional neural network (CNN)\n",
    "\n",
    "4. Redes Neuronales Recurrentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t08IWzpL-6jF"
   },
   "source": [
    "# 1. Redes Neuronales\n",
    "\n",
    "Las redes neuronales son un método específico de aprendizaje a partir de datos, un método que se basa en un elemento muy simple, la *unidad neuronal*. Una unidad neuronal (o red neuronal de 1 capa) es una función matemática de este tipo:\n",
    "\n",
    "${{mathbf y} = \\sigma(\\mathbf{w}^T \\cdot {{mathbf x} + b)$\n",
    "\n",
    "donde ${\\mathbf x}$ representa un elemento de entrada en forma vectorial, $\\mathbf{w}$ es un vector de pesos, $\\sigma$ es una función no lineal y $b$ un valor escalar. $(\\mathbf{w},b)$ se denominan los parámetros de la función. La salida de esta función se llama la *activación* de la neurona. \n",
    "\n",
    "En cuanto a la función no lineal, históricamente la más común era la función Sigmoide, pero hoy en día existen varias alternativas que se suponen más adecuadas para el aprendizaje a partir de datos, como ReLU y variantes.\n",
    "\n",
    "> **PREGUNTA:** ¿Qué tipo de funciones de decisión están representadas por una nn de 1 capa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 925,
     "status": "ok",
     "timestamp": 1647940942543,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "FX5NwyDo_A_P",
    "outputId": "6db56d70-c551-4f68-d122-60a53957b6e0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def ReLU(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "plt.ylim(-1.5, 10)\n",
    "x = np.linspace(-10.0,10.0,100)\n",
    "y1 = sigmoid(x)\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(x,y1)\n",
    "y2 = ReLU(x)\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(x,y2,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1647940943094,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "RX5LERmbBQaR",
    "outputId": "2a00806e-162f-40fb-a1f0-64aa01e2937b"
   },
   "outputs": [],
   "source": [
    "x = np.array([0.4,1.2,3.5])\n",
    "\n",
    "w = np.array([1.0,2.0,1.0])\n",
    "b = 1.3\n",
    "\n",
    "y = sigmoid(np.dot(x,w) + b)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2P7yHSro_Inc"
   },
   "source": [
    "## 1.1. Redes Neuronales de varias capas (multilayer)\n",
    "\n",
    "Las neuronas simples pueden organizarse en estructuras mayores aplicando al mismo vector de datos diferentes conjuntos de pesos, formando lo que se denomina una *capa*, y apilando capas una sobre la salida de la otra.  \n",
    "\n",
    "Es importante tener en cuenta que una red neuronal multicapa puede verse como una composición de productos matriciales (las matrices representan pesos) y activaciones de funciones no lineales. Para el caso de una red de 2 capas el resultado es:\n",
    "\n",
    "$ {\\mathbf y} = {\\mathbf \\sigma}\\Big( W^1 {\\mathbf \\sigma}\\Big( W^0 {\\mathbf x} + {\\mathbf b}^0 \\Big) + {\\mathbf b}^1 \\Big)$\n",
    "\n",
    "donde ${\\mathbf \\sigma}$ representa una versión vectorial de la función sigmoidea y $W^i$ son los pesos de cada capa en forma matricial.  \n",
    "\n",
    "Lo interesante de este tipo de estructuras es que se ha demostrado que incluso una red neuronal con una sola capa oculta que contiene un número finito de neuronas puede aproximar cualquier función continua de $\\mathbf{R}^n$. Este hecho convierte a las redes neuronales en un buen candidato para aplicar métodos de aprendizaje a partir de datos. La pregunta es entonces: ¿cómo encontrar los parámetros óptimos, ${\\mathbf w} = (W^i,{\\mathbf b})$, para aproximar una función que está implícitamente definida por un conjunto de muestras $\\{({\\mathbf x}_1, {\\mathbf y}_1), \\dots, ({\\mathbf x}_n, {\\mathbf y}_n)\\}$?\n",
    "\n",
    "Desde un punto de vista técnico, no sólo las redes neuronales, sino la mayoría de los algoritmos que se han propuesto para inferir modelos a partir de grandes conjuntos de datos se basan en la solución iterativa de un problema matemático que implica datos y un modelo matemático. Si existiera una solución analítica al problema, ésta debería ser la adoptada, pero no es así en la mayoría de los casos. Las técnicas que se han diseñado para abordar estos problemas se agrupan en un campo que se denomina optimización. La técnica más importante para resolver problemas de optimización es el *gradient descend*.\n",
    "\n",
    "> El entrenamiento de modelos como $ {\\mathbf y} = {\\mathbf \\sigma}\\Big( W^1  {\\mathbf \\sigma}\\Big( W^0  {\\mathbf x} + {\\mathbf b}^0 \\Big) + {\\mathbf b}^1 \\Big)$ (o mayor!) puede realizarse fácilmente aplicando *automatic differentiation* a una función de pérdida. \n",
    "\n",
    "> En el caso de la regresión: $L = \\frac{1}{n} \\sum_{i=1}^n \\Big({\\mathbf y}_i - {\\mathbf \\sigma}\\Big( W^1  {\\mathbf \\sigma}\\Big( W^0  {\\mathbf x}_i + {\\mathbf b}^0 \\Big) + {\\mathbf b}^1 )\\Big)\\Big)^2 $\n",
    "\n",
    "> En el caso de la clasificación en dos clases: $L = \\frac{1}{n} log(1 + exp(-y_i {\\mathbf \\sigma}\\Big( W^1  {\\mathbf \\sigma}\\Big( W^0  {\\mathbf x} + {\\mathbf b}^0 \\Big) + {\\mathbf b}^1 \\Big))) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMoVJNwZB81F"
   },
   "source": [
    "### Jugando con redes neuronales.\n",
    "+ Clases concéntricas, 1 capa, Sigmoide.\n",
    "+ Clases concéntricas, 1 capa, ReLu.\n",
    "+ X-or, 0 capas.\n",
    "+ X-or, 1 capa.\n",
    "+ Datos en espiral.\n",
    "+ Regresión.\n",
    "\n",
    "\n",
    "http://playground.tensorflow.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqXdykt2DYo0"
   },
   "source": [
    "# 2. Deep Learning en `keras`\n",
    "\n",
    "> Keras es una biblioteca de redes neuronales de alto nivel, escrita en Python y capaz de ejecutarse sobre TensorFlow. Fue desarrollado con foco en permitir la experimentación rápida.\n",
    "\n",
    "El núcleo de la estructura de datos de Keras es un modelo, una forma de organizar las capas. El principal tipo de modelo es el ``Modelo secuencial``, una pila lineal de capas. \n",
    "\n",
    "```Python\n",
    "from tensorflow.keras.models import Sequential\n",
    "model = Sequential()\n",
    "```\n",
    "\n",
    "Apilar capas es tan fácil como ``.add()``:\n",
    "\n",
    "```Python\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "model.add(Dense(output_dim=64, input_dim=100))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "```\n",
    "\n",
    "Una vez que tu modelo tenga buen aspecto, configura su proceso de aprendizaje con ``.compile()``:\n",
    "\n",
    "```Python\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='sgd', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "Si lo necesitas, puedes configurar aún más el optimizador.\n",
    "\n",
    "```Python\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True))\n",
    "```\n",
    "\n",
    "Ahora puedes iterar sobre tus datos de entrenamiento por lotes:\n",
    "\n",
    "```Python\n",
    "model.fit(X_train, Y_train, nb_epoch=5, batch_size=32)\n",
    "```\n",
    "\n",
    "Evalúa tu rendimiento en una línea:\n",
    "```Python\n",
    "loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "```\n",
    "\n",
    "O generar predicciones sobre nuevos datos:\n",
    "\n",
    "```Python\n",
    "classes = model.predict_classes(X_test, batch_size=32)\n",
    "proba = model.predict_proba(X_test, batch_size=32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 4570,
     "status": "ok",
     "timestamp": 1647973580497,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "uqHskK3EZQx_",
    "outputId": "090838ac-541c-419a-b597-dd3bcc3f8da6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1647973580498,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "3htJMJEualOT",
    "outputId": "7327f3f4-8978-411d-9c90-c2631ba216bf"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrena una NN profunda simple en el conjunto de datos MNIST.\n",
    "Alcanza una precisión de prueba del 98,40% tras 20 epochs.\n",
    "(hay *mucho* margen para ajustar los parámetros).\n",
    "2 segundos por epoch en una GPU K520."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84137,
     "status": "ok",
     "timestamp": 1647974669140,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "xEufgoGHDytG",
    "outputId": "88b3db0a-0c46-4686-d506-67ef9d3232a5"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljANZSaIYe8I"
   },
   "source": [
    "### 2.1. Dropout\n",
    "\n",
    "El dropout es una forma de regularizar la red neuronal. Durante el entrenamiento, puede ocurrir que las neuronas de una capa concreta siempre se vean influidas únicamente por la salida de una neurona concreta de la capa anterior. En ese caso, la red neuronal se sobreajustaría.\n",
    "\n",
    "Dropout evita el sobreajuste y regulariza cortando aleatoriamente las conexiones (también conocido como dropping the connection) entre neuronas de capas sucesivas durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i22znQv4bA1Q"
   },
   "source": [
    "### 2.2. Keras optimizers\n",
    "\n",
    "Hay varias variantes del gradient descend, que difieren en cómo calculamos el paso.\n",
    "\n",
    "Keras soporta siete optimizadores:\n",
    "\n",
    "```python\n",
    "my_opt = tensorflow.keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "my_opt = tensorflow.keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "my_opt = tensorflow.keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "my_opt = tensorflow.keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "my_opt = tensorflow.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "my_opt = tensorflow.keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "my_opt = tensorflow.keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "```\n",
    "\n",
    "#### Momentum\n",
    "\n",
    "Por ejemplo, el SGD tiene problemas para navegar por barrancos, es decir, zonas en las que la superficie se curva de forma mucho más pronunciada en una dimensión que en otra, lo que es habitual en torno a los óptimos locales. En estos casos, SGD oscila por las pendientes del barranco y sólo avanza vacilante por el fondo hacia el óptimo local.\n",
    "\n",
    "**Momentum** es un método que ayuda a acelerar el SGD en la dirección pertinente y amortigua las oscilaciones. Lo hace añadiendo una fracción del vector de actualización del paso de tiempo anterior al vector de actualización actual:\n",
    "\n",
    "$$ v_t = m v_{t-1} + \\alpha \\nabla_w f $$$$ w = w - v_t $$\n",
    "\n",
    "El momento $m$ suele fijarse en $0,9$.\n",
    "\n",
    "#### Adagrad\n",
    "\n",
    "SGD manipula la tasa de aprendizaje de forma global e igual para todos los parámetros. Ajustar los ritmos de aprendizaje es un proceso costoso, por lo que se ha trabajado mucho en la concepción de métodos que puedan ajustar los ritmos de aprendizaje de forma adaptativa, e incluso hacerlo por parámetro.\n",
    "\n",
    "Adagrad es un algoritmo de optimización basado en el gradiente que hace precisamente esto: Adapta la tasa de aprendizaje a los parámetros, realizando actualizaciones mayores para los parámetros poco frecuentes y menores para los frecuentes.\n",
    "\n",
    "$$ c = c + (\\nabla_w f)^2 $$$$ w = w - \\frac{\\alpha}{\\sqrt{c}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3cSy4_H6aEi"
   },
   "source": [
    "## 2.3. Convolutional neural network (CNN)\n",
    "\n",
    "Los perceptrones multicapa mencionados anteriormente representan el modelo de red neuronal de avance más general y potente posible; se organizan en capas, de forma que cada neurona de una capa recibe como entrada su propia copia de todas las salidas de la capa anterior. Este tipo de modelo es perfecto para el tipo de problema adecuado: aprender a partir de un número fijo de parámetros (más o menos) no estructurados.\n",
    "\n",
    "> Sin embargo, considera lo que ocurre con el número de parámetros (pesos) de un modelo de este tipo cuando se le alimentan datos de imagen sin procesar (por ejemplo, una imagen de 200$ por 200$ píxeles conectada a 1024 neuronas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1647941009442,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "gxR3h2-d6nGn",
    "outputId": "d885def5-f38f-4b96-85d8-b2343f436f6a"
   },
   "outputs": [],
   "source": [
    "200 * 200 * 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlCN_f8E7OZ2"
   },
   "source": [
    "La situación rápidamente se vuelve inmanejable a medida que el tamaño de las imágenes crece, mucho antes de alcanzar el tipo de imágenes con las que la gente usualmente quiere trabajar en aplicaciones reales.\n",
    "\n",
    "Una solución común es reducir el tamaño de las imágenes a un tamaño donde las redes neuronales de varias capas (MLP) puedan aplicarse de manera segura. Sin embargo, si reducimos directamente el tamaño de la imagen, potencialmente perdemos una gran cantidad de información; sería ideal si pudiéramos hacer algún procesamiento útil de la imagen (sin causar una explosión en el conteo de parámetros) antes de realizar la reducción de tamaño.\n",
    "\n",
    "Resulta que hay una forma muy eficiente de lograr esto, y se aprovecha de la estructura de la información codificada dentro de una imagen: se asume que los píxeles que están espacialmente más cercanos colaborarán mucho más en la formación de una característica particular de interés que aquellos en esquinas opuestas de la imagen. Además, si se encuentra que una característica (más pequeña) es de gran importancia al definir la etiqueta de una imagen, será igualmente importante si esta característica se encontrara en cualquier lugar dentro de la imagen, independientemente de la ubicación.\n",
    "\n",
    "Aquí entra el operador de convolución. Dada una imagen bidimensional, $I$, y una pequeña matriz, $K$ de tamaño $h \\times w$ (conocida como kernel de convolución), que asumimos codifica una forma de extraer una característica interesante de la imagen, calculamos la imagen convolucionada, $I∗K$, superponiendo el kernel sobre la imagen de todas las maneras posibles y registrando la suma de los productos elemento a elemento entre la imagen y el kernel:\n",
    "\n",
    "$$\n",
    "output(x,y) = (I \\otimes K)(x,y) = \\sum_{m=0}^{M-1} \\sum_{n=1}^{N-1} K(m,n) I(x-n, y-m)\n",
    "$$\n",
    "\n",
    "El operador de convolución forma la base fundamental de la capa convolucional de una CNN. La capa está completamente especificada por cierto número de kernels, $K$, y opera calculando la convolución de las imágenes de salida de una capa anterior con cada uno de esos kernels, luego añadiendo los sesgos (uno por cada imagen de salida). Finalmente, puede aplicarse una función de activación, $\\sigma$, a todos los píxeles de las imágenes de salida.\n",
    "\n",
    "Típicamente, la entrada a una capa convolucional tendrá $d$ canales (por ejemplo, rojo/verde/azul en la capa de entrada), en cuyo caso los kernels se extienden para tener este número de canales también.\n",
    "\n",
    "Dado que todo lo que estamos haciendo aquí es adición y escalado de los píxeles de entrada, los kernels pueden ser aprendidos de un conjunto de datos de entrenamiento mediante descenso de gradiente, exactamente como los pesos de un MLP. De hecho, un MLP es perfectamente capaz de replicar una capa convolucional, pero requeriría mucho más tiempo de entrenamiento (y datos) para aprender a aproximar ese modo de operación.\n",
    "\n",
    "\n",
    "https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2\n",
    "\n",
    "### Pooling\n",
    "\n",
    "De hecho, tras una capa convolucional se suelen aplicar dos tipos de funciones no lineales: funciones de activación no lineales como sigmoides o ReLU y *pooling*. Las capas de pooling se utilizan con el fin de reducir progresivamente el tamaño espacial de la imagen para lograr la invariancia de escala. La capa más común es la capa *maxpool*. Básicamente, un maxpool de 2 veces 2$ hace que un filtro de 2 por 2 recorra toda la matriz de entrada y elija el elemento más grande de la ventana para incluirlo en el siguiente mapa de representación. El pooling también puede implementarse utilizando otros criterios, como promediar en lugar de tomar el elemento max. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84854,
     "status": "ok",
     "timestamp": 1647975599801,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "KvbTpVNS7COv",
    "outputId": "739fdd8d-96b0-4f23-ca0a-d741e46b2245"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\"\"\"\n",
    "## Prepare the data\n",
    "\"\"\"\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\"\"\"\n",
    "## Build the model\n",
    "\"\"\"\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\"\"\"\n",
    "## Train the model\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "\"\"\"\n",
    "## Evaluate the trained model\n",
    "\"\"\"\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8-AH0Zy96Aa"
   },
   "source": [
    "## 3. Redes Neuronales Recurrentes\n",
    "\n",
    "\n",
    "Las redes neuronales clásicas, incluidas las convolucionales, sufren de dos limitaciones severas:\n",
    "\n",
    "+ Solo aceptan un vector de tamaño fijo como entrada y producen un vector de tamaño fijo como salida.\n",
    "+ No consideran la naturaleza secuencial de algunos datos (lenguaje, cuadros de video, series temporales, etc.)\n",
    "\n",
    "Las redes neuronales recurrentes (RNN) superan estas limitaciones al permitir operar sobre secuencias de vectores (en la entrada, en la salida o en ambos). Las RNN se llaman recurrentes porque realizan la misma tarea para cada elemento de la secuencia, con la salida dependiendo de los cálculos previos. Las fórmulas básicas de una RNN simple son:\n",
    "\n",
    "$$ s_t = f_1 (Ux_t + W s_{t-1}) $$\n",
    "$$ y_t = f_2 (V s_t) $$\n",
    "\n",
    "Estas ecuaciones básicamente dicen que el estado actual de la red, comúnmente conocido como estado oculto, $s_t$, es una función $f_1$ del estado oculto anterior $s_{t-1}$ y la entrada actual $x_t$. Las matrices $U, V, W$ son los parámetros de la función.\n",
    "\n",
    "Dada una secuencia de entrada, aplicamos las fórmulas de RNN de manera recurrente hasta que procesamos todos los elementos de entrada. La RNN comparte los parámetros $U,V,W$ en todos los pasos recurrentes. Podemos pensar en el estado oculto como una memoria de la red que captura información sobre los pasos anteriores.\n",
    "\n",
    "La novedad de este tipo de red es que hemos codificado en la arquitectura misma de la red un esquema de modelado de secuencias que se ha utilizado en el pasado para predecir series temporales y modelar el lenguaje. A diferencia de las arquitecturas anteriores que hemos introducido, ahora las capas ocultas están indexadas tanto por índice 'espacial' como 'temporal'.\n",
    "\n",
    "Las entradas de una red recurrente siempre son vectores, pero podemos procesar secuencias de símbolos/palabras representando estos símbolos mediante vectores numéricos.\n",
    "\n",
    "Supongamos que queremos clasificar una frase o una serie de palabras. Sean $x^1, ...,x^{C}$ los vectores de palabras correspondientes a un corpus con $C$ símbolos. Entonces, la relación para calcular las características de salida de la capa oculta en cada paso de tiempo $t$ es $h_t = \\sigma(W s_{t-1} + U x_{t})$, donde:\n",
    "\n",
    "+ $x_{t} \\in \\mathbf{R}^{d}$ es el vector de palabra de entrada en el tiempo $t$.\n",
    "+ $U \\in \\mathbf{R}^{D_h \\times d}$ es la matriz de pesos del vector de palabra de entrada, $x_t$.\n",
    "+ $W \\in \\mathbf{R}^{D_h \\times D_h}$ es la matriz de pesos de la salida del paso de tiempo anterior, $t-1$.\n",
    "+ $s_{t-1}  \\in \\mathbf{R}^{D_h}$ es la salida de la función no lineal en el paso de tiempo anterior, $t-1$.\n",
    "+ $\\sigma()$ es la función de no linealidad (normalmente, ``tanh``).\n",
    "\n",
    "La salida de esta red es $\\hat{y}_t = softmax (V h_t)$, que representa la distribución de probabilidad de salida sobre el vocabulario en cada paso de tiempo $t$.\n",
    "\n",
    "Esencialmente, $\\hat{y}_t$ es la próxima palabra predicha dado el contexto del documento hasta ahora (es decir, $h_{t-1}$) y el último vector de palabra observado $x^{(t)}$.\n",
    "\n",
    "La función de pérdida utilizada en las RNN es a menudo el error de entropía cruzada:\n",
    "\n",
    "$\n",
    "\tL^{(t)}(W) = - \\sum_{j=1}^{|V|} y_{t,j} \\times log (\\hat{y}_{t,j})\n",
    "$\n",
    "\n",
    "El error de entropía cruzada sobre un corpus de tamaño $C$ es:\n",
    "\n",
    "$\n",
    "\tL = \\frac{1}{C} \\sum_{c=1}^{C} L^{(c)}(W) = - \\frac{1}{C} \\sum_{c=1}^{C} \\sum_{j=1}^{|V|} y_{c,j} \\times log (\\hat{y}_{c,j})\n",
    "$\n",
    "\n",
    "Estas arquitecturas simples de RNN han demostrado ser demasiado propensas a olvidar información cuando las secuencias son largas y también son muy inestables cuando se entrenan. Por esta razón, se han propuesto varias arquitecturas alternativas. Estas alternativas se basan en la presencia de *gated units* (unidades con puertas). Las puertas son una forma de permitir opcionalmente que la información fluya. Están compuestas por una capa de red neuronal sigmoidea y una operación de multiplicación punto a punto. Las dos arquitecturas alternativas de RNN más importantes son las redes de Memorias a Largo Corto Plazo (LSTM) y las Unidades Recurrentes con Puertas (GRU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script de ejemplo para generar texto a partir de los escritos de Nietzsche.\n",
    "Se necesitan al menos 20 epochs antes de que el texto generado\n",
    "empiece a sonar coherente.\n",
    "Se recomienda ejecutar este script en la GPU, ya que las redes\n",
    "son bastante intensivas computacionalmente.\n",
    "Si pruebas este script con datos nuevos, asegurate de que tu corpus\n",
    "tenga al menos ~100k caracteres. ~1M es mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 202071,
     "status": "error",
     "timestamp": 1647941500999,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "Ayj2JDJ87zKu",
    "outputId": "83c01988-b334-432a-a180-2a161e08f018"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "\n",
    "path = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "with io.open(path, encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=60,\n",
    "          callbacks=[print_callback])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NeuralNetworks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
