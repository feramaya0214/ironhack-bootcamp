{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "affefcbc-d980-401d-a342-5bf3ae125480",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exploratory Data Analysis (EDA) with Housing Price Dataset\n",
    "\n",
    "Exploratory Data Analysis (EDA) is a crucial step in the data analysis pipeline. It helps us understand the data, discover patterns, spot anomalies, and frame hypotheses. In this lesson, we'll use a housing price dataset to explore various EDA techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc3e65f-dcb9-43ea-8dc7-74829a6204ee",
   "metadata": {},
   "source": [
    "## Initial Steps for Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65704bf4-cf6e-443f-af0d-2e5fcac5a5a9",
   "metadata": {},
   "source": [
    "The initial steps for data analysis in Python include:\n",
    "\n",
    "1. **Data Acquisition:** This involves gathering data from various sources such as local files, databases, APIs, websites, etc.\n",
    " \n",
    "2. **Loading the Data:** Common formats to consider are CSV (Comma Separated Values), JSON, XLS, HTML, XML, and more.\n",
    "\n",
    "3. **Exploratory Data Analysis (EDA):** EDA is a systematic approach to initial data inspection. It leverages **descriptive analysis** techniques to understand the data better, identify outliers, highlight significant variables, and generally uncover underlying data patterns. Additionally, EDA helps in organizing the data, spotting errors, and assessing missing values.\n",
    "\n",
    "4. **Data Cleaning:** It's crucial to check the available data and perform tasks such as removing empty columns, standardizing terms, imputing missing data where appropriate, and more.\n",
    "\n",
    "5. After cleaning, you should conduct a more in-depth exploratory data analysis to further understand the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3350b116-324f-4dee-b416-9c98cbdb81c6",
   "metadata": {},
   "source": [
    "## Methods in EDA\n",
    "\n",
    "EDA methodologies can be broadly categorized into:\n",
    "\n",
    "- **Numerical Measures:** These can include coefficients, frequency counts, and other statistical metrics.\n",
    "  \n",
    "- **Visual Representations:** Examples are histograms, scatter plots, pie charts, and more.\n",
    "\n",
    "Additionally, based on the number of variables in focus, methods can be:\n",
    "\n",
    "- **Univariate:** Describing the characteristics of a single variable at a time.\n",
    "  \n",
    "- **Bivariate:** Analyzing the relationship between two variables, either in tandem or understanding one variable based on the other (examining the influence of one independent variable in relation to the dependent variable).\n",
    "  \n",
    "- **Multivariate:** An extension of bivariate analysis but for multiple variables. It explores the relationships among them or the impact of two or more independent variables (sometimes along with associated variables or covariates) on one or more dependent variables.\n",
    "\n",
    "**Note:**\n",
    "It's crucial to ensure that all our analytical methods are tailored to the type of variable under consideration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67de913-bcf2-4628-89c0-94fc3ce4535d",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "Before we dive into EDA, let's gather our data. In this case, we will load our dataset and take a quick look at its structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c874862-45f3-4242-b1b7-864ba3c4ceac",
   "metadata": {},
   "source": [
    "The dataset can be found [here](https://raw.githubusercontent.com/data-bootcamp-v4/data/main/housing_price_eda.csv) and the information about the dataset [here](https://github.com/data-bootcamp-v4/data/blob/main/housing_price_dataset_info.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24a189e-9d1d-4d12-bc5c-33f50c4de1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e6c16-b2a9-48e2-a436-f45846226e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the housing price dataset (assuming the file name is \"housing_price.csv\")\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/data-bootcamp-v4/data/main/housing_price_eda.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca70225-3478-4642-a093-13a5e35a5770",
   "metadata": {},
   "source": [
    "## Initial Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ec05d-e1cb-4fce-9254-3971ac438a5b",
   "metadata": {},
   "source": [
    "Before diving into the specifics of univariate analysis, it's essential to get acquainted with our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cee01d-cdc8-4407-80e0-6ce2a8d4405e",
   "metadata": {
    "id": "f0BcwWCfkTQR",
    "outputId": "72c16c61-79a9-4309-c177-c24529bbc3cb"
   },
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd2ca93-68e9-4569-a472-30bcf88f7abf",
   "metadata": {
    "id": "nbcdOIuekTQR",
    "outputId": "192d7c46-8fd4-4ec0-aea6-07fda7328711"
   },
   "outputs": [],
   "source": [
    "# Retrieving the number of rows and columns in the dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1406cf-6a8b-45ca-870a-226314b393a9",
   "metadata": {
    "id": "p7kW1SpbkTQS",
    "outputId": "a8a0b351-996c-48d1-935a-fbf73481b626",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Displaying the data types of each column in the dataframe\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c013ad8-b766-425c-99c1-e862955b22f1",
   "metadata": {
    "id": "7-elp1qykTQS"
   },
   "source": [
    "### Exploring numerical and categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e855ee3-153c-49f6-9fc2-5ab6f7105a7e",
   "metadata": {},
   "source": [
    "We'll explore numerical and categorical variables, and create two dataframes, one for each type of variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62f86af-3b6b-4b05-983e-cf089d73c4ba",
   "metadata": {},
   "source": [
    "**Note**: \n",
    "- **Numerical variables**: These can encompass both quantitative and qualitative information. Often, discrete numerical variables with limited distinct values hint at qualitative (categorical) variable encoded as numbers.\n",
    "\n",
    "- **Object variables**: Typically, these consist of qualitative data, numeric data in a string format, or data that might not be directly relevant to the analysis. Examples include identifiers like 'ID' numbers or 'Names'. Variables with a broad range of unique values, especially in string format, often fall into this category. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f8c395-f7b8-48c5-8b7c-1069904e1f1b",
   "metadata": {
    "id": "wlZ_7m1SkTQS",
    "outputId": "14ec1484-e49d-4e7f-c571-83aadf41192d"
   },
   "outputs": [],
   "source": [
    "# Retrieving the unique data types present in the dataframe columns\n",
    "list(set(df.dtypes.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e5d294-fbc1-4e94-82b1-3dfcdfb7e2b8",
   "metadata": {
    "id": "IUn7DvASkTQS",
    "outputId": "deccb1b4-d405-477e-ce49-b0022b8ebbe7"
   },
   "outputs": [],
   "source": [
    "# Extracting column names with numerical data types from the dataframe\n",
    "df.select_dtypes(\"number\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2477f6-91a8-4329-b824-4ee4a15333d8",
   "metadata": {
    "id": "CiFIrdRFkTQS",
    "outputId": "44447e1d-08df-4268-afbe-2c9196a345ce"
   },
   "outputs": [],
   "source": [
    "# Counting and sorting the unique values for each numerical column in descending order\n",
    "df.select_dtypes(\"number\").nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df86487e-9d72-41d1-b936-d0a8f031d35f",
   "metadata": {
    "id": "Vm_XfWxwkTQS",
    "outputId": "2fcdc8be-5a27-4b16-bf9d-054569e4455a"
   },
   "outputs": [],
   "source": [
    "# Separating between discrete and continuous variables, as discrete ones could potentially be treated as categorical.\n",
    "# Remember to adjust the threshold (in this case, < 20) based on your dataset's specific characteristics and domain knowledge.\n",
    "potential_categorical_from_numerical = df.select_dtypes(\"number\").loc[:, df.select_dtypes(\"number\").nunique() < 20]\n",
    "potential_categorical_from_numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6553ed31-e142-4444-8369-f1c995c8d18e",
   "metadata": {
    "id": "XH8W-ilykTQS",
    "outputId": "a54a9d25-67f1-4a95-be0b-e26123193ea0"
   },
   "outputs": [],
   "source": [
    "# Retrieving column names with object (typically string) data types from the dataframe\n",
    "df.select_dtypes(\"object\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e0803f-41fa-4127-96cf-32562988a018",
   "metadata": {
    "id": "dzjEgM5wkTQT"
   },
   "outputs": [],
   "source": [
    "# Counting and sorting the unique values for each object (string) column in descending order\n",
    "df.select_dtypes(\"object\").nunique().sort_values(ascending=False)\n",
    "\n",
    "# All columns seem categorical, as there isn't a wide variability of values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337a95b8-8ce8-4487-be38-d0c3515c981d",
   "metadata": {},
   "source": [
    "Decide based on domain knowledge and the above explorations which numerical columns are better as categorical\n",
    " and vice versa. \n",
    " \n",
    " For demonstration purposes, let's assume the *potential_categorical_from_numerical* are categorical, even though this might not be the case in a real scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d625e-3a50-4e6e-818a-d4794ca0e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting columns with object (typically string) data types to create a categorical dataframe\n",
    "# For demonstration purposes, let's consider the columns in potential_categorical_from_numerical as categorical variables.\n",
    "df_categorical = pd.concat([df.select_dtypes(\"object\"), potential_categorical_from_numerical], axis=1)\n",
    "\n",
    "# Adjusting the numerical dataframe by removing the moved columns\n",
    "df_numerical = df.select_dtypes(\"number\").drop(columns=potential_categorical_from_numerical.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df40e2c7-e058-4387-9861-54230cfb3a16",
   "metadata": {
    "id": "QuDEppqXkTQT",
    "outputId": "aaceaa2f-734f-4b94-931b-c193f516780b"
   },
   "outputs": [],
   "source": [
    "# Verifying that the total number of columns in the dataframe is the sum of object (string) and numerical columns\n",
    "len(df.columns) == len(df.select_dtypes(\"object\").columns) + len(df.select_dtypes(\"number\").columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f7e96c-c467-4757-b66e-173e4eb2484b",
   "metadata": {
    "id": "Cu1COqyKkTQS"
   },
   "source": [
    "In the data cleaning phase, it's important to focus on several essential aspects. First, verify that data is in the correct type and format (**Data Typing/Formatting**). Then, identify and address any duplicates to eliminate redundancy (**Duplicates**). Next, tackle missing values by finding and managing null or absent data (**Missing Values**). For categorical variables, like gender, review the categories (e.g., M, F, Masculine) and their distributions to determine if cleaning is needed (**Categorical Variables**). Similarly, assess numerical data for consistency. Finally, evaluate outliers to decide how to handle these extreme values (**Outliers**). Effective exploration of these areas is crucial for comprehensive data cleaning.\n",
    "\n",
    "We won't be delving into this now, as we already reviewed it in data cleaning lessons (expect for outliers, which we'll be reviewing later in this lesson). For now, we will just clean null values, so we can focus in univariate analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc70578-64f5-468e-8896-96e4083f452f",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3af3189-a8d2-4e16-9ace-40dae6c3aa7c",
   "metadata": {},
   "source": [
    "\n",
    "### Checking for Missing Data\n",
    "\n",
    "Missing data can influence our analysis. It's essential to identify and handle them appropriately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e51a035-a6da-4eac-96c3-60e503c429be",
   "metadata": {
    "id": "7sVQJcKYkTQS",
    "outputId": "4cb96bcb-ee40-497e-df6c-93d10293c8a5"
   },
   "outputs": [],
   "source": [
    "# Checking for missing data\n",
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa30e6f-b116-40f7-8019-1ba2af5fc1cf",
   "metadata": {
    "id": "8tk5MSKzkTQS",
    "outputId": "eee1e800-fd3d-456b-a4ec-302552b54b62"
   },
   "outputs": [],
   "source": [
    "# Identifying columns in the dataframe where over 80% of the values are missing\n",
    "df.columns[df.isnull().mean() > 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29ac673-33e7-4286-a4cb-300ba9f44ddc",
   "metadata": {
    "id": "PybQfXbPkTQS"
   },
   "outputs": [],
   "source": [
    "# Filtering out columns in the dataframe where more than 80% of the values are missing\n",
    "df = df[df.columns[df.isnull().mean() < 0.8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a27001-85a3-414f-8ab9-2dffc68d7232",
   "metadata": {
    "id": "jtNX_co5kTQS"
   },
   "outputs": [],
   "source": [
    "# Removing the \"Id\" column from the dataframe\n",
    "df.drop(\"Id\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6b581e-8474-4253-9c22-e0335aed25d7",
   "metadata": {},
   "source": [
    "## Univariate Analysis\n",
    "\n",
    "Univariate analysis, as its name suggests, concentrates on one variable at a time, giving us a deep understanding of its characteristics. This fundamental step in Exploratory Data Analysis (EDA) lays the groundwork for subsequent analyses involving multiple variables. Let's explore various techniques for both categorical and numerical variables.\n",
    "\n",
    "**Categorical variables**:\n",
    "- Frequency tables. Counts and proportions.\n",
    "- Visualizations: Bar charts, pie charts\n",
    "\n",
    "**Numerical variables**: \n",
    "- Measures of centrality: Mean, median, mode\n",
    "- Measures of dispersion: Variance,  standard deviation, minimum, maximum, range, quantiles\n",
    "- Shape of the distribution: Symmetry and kurtosis\n",
    "- Visualizations: Histograms, box plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c0d3b7-3fe9-48d8-b3e4-c2f841a47317",
   "metadata": {},
   "source": [
    "### Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf113ec-fbb9-45e5-abe8-6c265da2497c",
   "metadata": {},
   "source": [
    "Categorical variables represent categories or labels, like types or groups. Analyzing categorical data involves understanding the frequency or proportion of each category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2f37be-7b05-41b9-8377-ab218c997be4",
   "metadata": {},
   "source": [
    "#### Frequency Tables\n",
    "\n",
    "Frequency tables are tabular representations that display the number of occurrences of each category. They help in understanding the distribution of categories in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010d09bd-c182-431c-aede-85d7ab4b128b",
   "metadata": {},
   "source": [
    "In python, we can use:\n",
    "- `value_counts()`\n",
    "- `pd.crosstab()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ec4272-a4ec-41ad-a0f7-96e5596f4e7a",
   "metadata": {},
   "source": [
    "Let's consider *MSZoning* as our categorical variable of interest, which represents the general zoning classification of the sale.\n",
    "\n",
    "We'll look at `value_counts()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2cf903-41dd-4438-ae70-a1969d8a1d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency table for 'MSZoning'\n",
    "frequency_table = df['MSZoning'].value_counts()\n",
    "\n",
    "# Calculating the proportion of each unique value in the 'MSZoning'\n",
    "proportion_table = df['MSZoning'].value_counts(normalize=True)\n",
    "\n",
    "frequency_table, proportion_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e961c0c-db36-446d-8650-b311ce9304ad",
   "metadata": {},
   "source": [
    "The frequency table gives the count of each zoning type, while the proportion table provides the percentage representation of each category in the dataset. This helps to quickly identify dominant and minority categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b83e68-77ac-4bc2-9223-58e7c7a1c4ea",
   "metadata": {},
   "source": [
    "Let's look at `pd.crosstab()`. The crosstab function can be useful to compute a cross-tabulation of two (or more) factors. Here, it's used to count the occurrences of each 'MSZoning' type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc18040-faba-456a-af59-ce4056895c44",
   "metadata": {
    "id": "Llf1vw9OkTQc"
   },
   "outputs": [],
   "source": [
    "# Creating a crosstab table for the 'MSZoning' column, counting occurrences for each unique value\n",
    "my_table = pd.crosstab(index = df_categorical[\"MSZoning\"],  # Make a crosstab\n",
    "                              columns=\"count\")      # Name the count column\n",
    "my_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f637b9-f9c1-40ad-8c5d-ede629375fc9",
   "metadata": {
    "id": "J6b9pYKikTQc"
   },
   "source": [
    "We can also get the proportion_table the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b247f81-b55a-41dd-babb-77deb09f6cc6",
   "metadata": {
    "id": "H17YQDnekTQc",
    "outputId": "3d9b6cbf-c474-4160-ab02-26f1aa1680ae"
   },
   "outputs": [],
   "source": [
    "# Calculating the proportions for each value in 'my_table' and rounding the results to two decimal places\n",
    "(my_table/my_table.sum()).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221f4e3d-f13e-4f0f-8cb9-aa6ddea5766a",
   "metadata": {},
   "source": [
    "The crosstab table displays the number of occurrences of each 'MSZoning' type, just like the frequency table. Computing the proportion table showcases the relative percentage of each category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7927f4-b8d5-4cb1-84e8-d94e2c31e731",
   "metadata": {},
   "source": [
    "**Insights** for 'MSZoning':\n",
    "\n",
    "- The most common zoning classification is 'RL', which stands for Residential Low Density, comprising approximately 78.8% of the properties in the dataset.\n",
    "- The second most frequent zoning classification is 'RM' (Residential Medium Density), making up roughly 14.9%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0290af1f-d89b-4101-88b9-9d2b1167225a",
   "metadata": {},
   "source": [
    "#### Visualizations\n",
    "\n",
    "Visualizations offer a more intuitive understanding of categorical data distribution. Bar charts and pie charts are common methods to visually represent categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c4d984-11c1-4efb-b506-ad89e4b7ef99",
   "metadata": {},
   "source": [
    "##### Bar charts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b92e9e-7767-49bf-8ead-2c678420dc7b",
   "metadata": {},
   "source": [
    "Bar charts can display the frequency or proportion of each category using bars of varying lengths. Here, the same data is visualized using three different methods: `sns.barplot()` and `sns.countplot()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40f96a6-9b0a-4a5a-baec-079d7596cffa",
   "metadata": {},
   "source": [
    "Let's see how to use the `sns.barplot()` function with the result from `value_counts()` and `pd.crosstab()`. We should expect the same plot for both following lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6cdd7c-66f2-477f-a1b4-3d6f1eefbacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a bar chart using the values from the frequency table, with colors sourced from the \"Set3\" palette\n",
    "sns.barplot(x=frequency_table.index, y=frequency_table.values, palette=\"Set3\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858dd755-625f-446c-9969-0ddcbf124d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a bar chart using the 'count' values from 'my_table', with colors sourced from the \"Set3\" palette\n",
    "sns.barplot(x=my_table.index, y=my_table[\"count\"], palette=\"Set3\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eceded8-ecaf-49cd-a346-12aca1c18e03",
   "metadata": {},
   "source": [
    "Using matplotlib, would just be:\n",
    "\n",
    "```python\n",
    "my_table.plot.bar()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2b25f0-d851-4a81-ab82-7ca6394d111a",
   "metadata": {},
   "source": [
    "**Insights** from the Bar Charts:\n",
    "\n",
    "1. Both bar charts confirm the dominance of the `RL` zoning classification within the dataset. \n",
    "2. The bar representing `RL` is significantly taller than the others, emphasizing its higher frequency.\n",
    "3. The two charts are identical, showcasing that both `value_counts()` and `pd.crosstab()` provide similar counts for the categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba251e5-b2b8-4156-820d-9667dcaa5c3b",
   "metadata": {},
   "source": [
    "##### Countplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551b3a6b-42de-45e4-b4f2-aa617c5fc117",
   "metadata": {},
   "source": [
    "A countplot is a type of bar plot in Seaborn that displays the count of occurrences of unique values in a categorical column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4e30ca-29e8-4cbc-89e2-046181926da7",
   "metadata": {
    "id": "uFGdT_xekTQd"
   },
   "source": [
    "Same result now as the *bar plot* since it just assumes Y axis is the count of frequencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d784cb76-f0e0-47b4-a86c-ad2cd41bf45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a count plot for the 'MSZoning' column from the dataframe 'df', using the \"Set3\" palette for coloring\n",
    "sns.countplot(data=df, x='MSZoning', palette=\"Set3\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e6876a-720c-4eb6-9af1-ac5e7243f7e1",
   "metadata": {},
   "source": [
    "##### Pie charts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1106aa57-3fb5-42d9-81d1-360b1c97f73c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Pie charts provide a circular representation of the data, showing the proportion of each category as slices of a pie. However, they can be challenging to interpret when there are many categories or when categories have similar proportions.\n",
    "\n",
    "Seaborn, as of 2023, does not have a dedicated function for pie charts. Pie charts are more commonly created using `matplotlib`, which Seaborn is built upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13bcdd8-605d-41ec-b145-ab258eea5de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a pie chart of the 'MSZoning' column value counts, with percentage labels, \n",
    "# starting at angle 90, and using colors from the \"Set3\" Seaborn palette\n",
    "df['MSZoning'].value_counts().plot.pie(autopct='%1.1f%%', startangle=90, colors=sns.color_palette(\"Set3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a81c4d-1a66-4799-92f4-3e387bde140d",
   "metadata": {},
   "source": [
    "**Insights**:\n",
    "\n",
    "- The pie chart provides a clear visual representation of the dominance of the 'RL' (Residential Low Density) zoning classification, occupying a significant portion of the chart.\n",
    "- Other zoning types like 'RM', 'FV', 'RH', and 'C (all)' occupy much smaller slices, emphasizing the skewed distribution.\n",
    "- While pie charts can illustrate proportions effectively, the dominance of 'RL' makes it somewhat challenging to discern differences between the smaller categories. This underscores why alternative visualizations, like bar charts, can sometimes be more informative for such distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a9ff20-e1cc-49b3-9f5e-09285825f8f4",
   "metadata": {},
   "source": [
    "### Numerical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81a1b5f-2bab-4374-b15e-7f883ed1fa8b",
   "metadata": {},
   "source": [
    "Numerical variables are quantitative, and their values can be measured. Analyzing numerical data involves understanding its distribution, central tendency, and variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f907a5ab-d9ea-44bd-9b82-1ec70ecf2125",
   "metadata": {},
   "source": [
    "\n",
    "#### Summary Statistics\n",
    "\n",
    "**Centrality and Dispersion Measures**\n",
    "\n",
    "Let's start by getting some basic statistics on our dataset to understand its scale, centrality, and spread.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4650d4c-4e31-4fb0-9abc-3ba9c40c2037",
   "metadata": {
    "id": "ONytB4d0kTQW"
   },
   "source": [
    "- The `.describe()` method provides key statistics for numerical columns (by default) in a dataframe, excluding NaN values; although it primarily targets numeric data, the `include` parameter allows for the selection of other data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786a5b1f-12f9-4fa8-8adb-c92f4ca08bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for the dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719dd6e3-9b00-4741-bab8-799716df1f72",
   "metadata": {},
   "source": [
    "From `describe()` we get:\n",
    "- Measures of centrality: mean, median (indicated as 50%)\n",
    "- Measures of dispersion: standard deviation (std), minimum, maximum, quartiles (Q1, Q2, Q3, indicated as 25%, 50%, and 75% respectively)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49d13cb-f073-4afd-9385-f6a69a9999a4",
   "metadata": {},
   "source": [
    "**Insights** from Summary Statistics for 'SalePrice':\n",
    "\n",
    "- The average (mean) sale price of the houses in the dataset is approximately `$180,921`.\n",
    "\n",
    "- The median sale price (middle value when sorted) stands at `$163,000`. Notably, the median is lower than the mean, suggesting a skew in the distribution of sale prices towards higher values.\n",
    "\n",
    "- The standard deviation, a measure of the amount of variation or dispersion in the sale prices, is approximately `$79,442`. This indicates that sale prices can vary significantly from the average.\n",
    "\n",
    "- The minimum and maximum sale prices are `$34,900` and `$755,000`, respectively, highlighting a wide range of property values in the dataset.\n",
    "\n",
    "- The interquartile range (IQR), given by the values at 25% (Q1) and 75% (Q3), is between `$129,975` and `$214,000`. This means that 50% of the houses in the dataset were sold within this price range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86919e49-bc58-4e18-888e-11e1e9ca7685",
   "metadata": {},
   "source": [
    "#### More Centrality and Dispersion Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a24929-1526-4de1-9819-b099dc5bce9b",
   "metadata": {},
   "source": [
    "Now, suppose we want to calculate individual statistical measures without using the `.describe()` method. Here are some ways to do it:\n",
    "\n",
    "- `df[column].mean()`: Computes the mean of the selected column.\n",
    "- `df[column].median()`: Calculates the median of the selected column.\n",
    "- `df[column].mode()`: Identifies the mode of the selected column.\n",
    "- `df[column].std()`: Determines the standard deviation of the selected column.\n",
    "- `df[column].var()`: Computes the variance of the selected column.\n",
    "- `df[column].min()`: Finds the minimum value in the selected column.\n",
    "- `df[column].max()`: Finds the maximum value in the selected column.\n",
    "- `df[column].count()`: Counts the number of non-NaN entries in the selected column.\n",
    "\n",
    "In these examples, replace `column` with the name of the column you want to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ec8229-d147-4723-b60c-2d86fda0f25c",
   "metadata": {},
   "source": [
    "For this section, we'll focus on 'SalePrice' as our numerical variable of interest, which represents the price at which the house was sold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ad0a86-9dbc-41cf-a4ef-2d9db5abaf1c",
   "metadata": {},
   "source": [
    "**Measures of Centrality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eab9447-72dd-4ef7-bb14-d8b3d8d9a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_price = df['SalePrice'].mean()\n",
    "median_price = df['SalePrice'].median()\n",
    "mode_price = df['SalePrice'].mode()[0]\n",
    "\n",
    "mean_price, median_price, mode_price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed711095-af57-44bd-9282-e9bc1d0b5dad",
   "metadata": {},
   "source": [
    "**Measures of Dispersion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a145d06-f9b3-4286-86dc-dd7d3c3165c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_price = df['SalePrice'].var()\n",
    "std_dev_price = df['SalePrice'].std()\n",
    "min_price = df['SalePrice'].min()\n",
    "max_price = df['SalePrice'].max()\n",
    "range_price = max_price - min_price\n",
    "quantiles_price = df['SalePrice'].quantile([0.25, 0.5, 0.75])\n",
    "\n",
    "variance_price, std_dev_price, min_price, max_price, range_price, quantiles_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8ecf70-d13e-4c5c-af6c-5da207c695d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SalePrice'].quantile(0.1) # We can get any quantile value, not just quartiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00cb3c2-419b-41a3-8690-2500b7fe59c1",
   "metadata": {},
   "source": [
    "**Insights** from Measures of Centrality and Dispersion for 'SalePrice', for those metrics not calcualted in `describe()`:\n",
    "\n",
    "- **Centrality**:\n",
    "  - The most frequent (mode) sale price is $140,000. This value appears more frequently than any other price in the dataset.\n",
    "  \n",
    "- **Dispersion**:\n",
    "  - The variance, a measure of how far each sale price in the set is from the mean, is approximately \\(6,311,111,264\\). A high variance implies that sale prices can be quite different from one another.\n",
    "  - The range of sale prices is $720,100, calculated as the difference between the maximum and minimum prices. This wide range underscores the diversity in property prices within the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db5c868-7c18-4615-af3c-59738959e533",
   "metadata": {},
   "source": [
    "#### Shape of the Distribution\n",
    "\n",
    "Skewness and kurtosis provide insights into the shape of the data distribution. Skewness indicates the asymmetry, and kurtosis tells us about the \"tailedness\" or how peaked the distribution is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4274cb-a96d-4a55-9320-543c5647d9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness_price = df['SalePrice'].skew()\n",
    "kurtosis_price = df['SalePrice'].kurtosis()\n",
    "\n",
    "skewness_price, kurtosis_price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36008113-6fe6-4d64-814e-e5e3def144c5",
   "metadata": {},
   "source": [
    "- Skewness of 'SalePrice': \\(1.88\\)\n",
    "- Kurtosis of 'SalePrice': \\(6.54\\)\n",
    "\n",
    "**Insights**:\n",
    "\n",
    "1. **Skewness**: The positive value of skewness (1.88) for the 'SalePrice' indicates that the distribution is right-skewed. This means that the tail on the right side (higher prices) is longer than the left side (lower prices). In practical terms, this suggests that there are a significant number of houses that are sold at higher prices, which are acting as outliers and pulling the mean upwards.\n",
    "  \n",
    "2. **Kurtosis**: The kurtosis value of 6.54 is greater than 3, which indicates that the 'SalePrice' distribution has heavier tails and a sharper peak compared to a normal distribution. This means that there are more outliers (extreme values) in the 'SalePrice' than one would expect in a normally distributed set.\n",
    "\n",
    "The skewness and kurtosis values suggest that there are some houses that are sold at significantly higher prices than the majority, and these are affecting the overall distribution of house prices in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befeeb5b-0b95-4352-8670-2e7e948ab56a",
   "metadata": {},
   "source": [
    "#### Visualizations\n",
    "\n",
    "Visual tools like histograms and box plots offer insights into the distribution, variability, and potential outliers in numerical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06648e1c-7968-4dae-8e9c-21541ad6a03b",
   "metadata": {},
   "source": [
    "##### Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953101be-e2d6-4155-970a-c26b478ab591",
   "metadata": {},
   "source": [
    "Histograms display the frequency distribution of a dataset. The height of each bar represents the number of data points in each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e684002-928b-46b7-ba9d-2315819d96b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a histogram for the 'SalePrice' column of the 'data' dataframe\n",
    "# 'kde=True' adds a Kernel Density Estimate plot to give an approximation of the data's distribution\n",
    "# 'bins=30' divides the data into 30 bins for more detailed granularity\n",
    "# 'color=\"salmon\"' sets the color of the bars to salmon\n",
    "sns.histplot(df['SalePrice'], kde=True, bins=30, color=\"salmon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4574e86-a9fd-4525-9ad3-e4d49a25b1e8",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "- The histogram reveals that the majority of the houses are sold in the price range of approximately `$100,000` to `$250,000`. However, there's a long tail on the right side, confirming our earlier inference from the skewness value that there are houses sold at much higher prices. The Kernel Density Estimate (the smooth line) also shows the right-skewed nature of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5119cefa-c6f5-4104-866a-da2e3ff27e00",
   "metadata": {},
   "source": [
    "If we wanted to plot at the same time all the numerical variables with histograms, without a for loop, we could do so using matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359588f9-0e23-429e-b399-08bb6bb81512",
   "metadata": {
    "id": "kauVqFc2kTQb",
    "outputId": "c86cd630-811a-404d-a9db-61961cbd0fad"
   },
   "outputs": [],
   "source": [
    "# Creating histograms for each numerical column in 'df_numerical'\n",
    "df_numerical.hist(figsize=(15, 20), bins=60, xlabelsize=10, ylabelsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a7f215-d03b-4dc1-8d32-725db6cb4caf",
   "metadata": {
    "id": "J42HEIGLkTQb"
   },
   "source": [
    "Just by looking at it, which ones do you think could be correlated to SalePrice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042e4152-a538-41c5-a82a-db95d173e561",
   "metadata": {},
   "source": [
    "##### Box plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769335b7-0e7f-4509-b2f0-54b4ef6e9cf5",
   "metadata": {},
   "source": [
    "Box plots, or whisker plots, showcase the central 50% of the data (interquartile range), potential outliers, and other statistical properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8643b1a3-2fef-4bca-a30f-b063fc42ab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a boxplot for the 'SalePrice' column with a light blue color\n",
    "sns.boxplot(data = df['SalePrice'], color=\"lightblue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a198b4-d02b-4031-819b-3fd5d4356c2c",
   "metadata": {},
   "source": [
    "\n",
    "**Insights:**\n",
    "- The box plot gives us a visual representation of the central 50% of the data (the interquartile range) with the median price shown as a line inside the box. The whiskers extend to 1.5 times the interquartile range, and points outside of this range are considered outliers. As observed, there are several outlier points on the higher end of the sale prices, which aligns with our earlier insights about houses sold at significantly higher prices.\n",
    "\n",
    "Both visualizations underscore the presence of outliers in the higher price range. These outliers might be luxury homes or properties in prime locations, and special attention might be needed when building predictive models, as these outliers can influence model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9ded74-b151-42d6-b68e-3329040cdc87",
   "metadata": {},
   "source": [
    "## Converting continuous to discrete variables: Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95362e46-0cda-49e5-a48c-ec56af5cc535",
   "metadata": {},
   "source": [
    "Discretization is the process of converting continuous variables into discrete ones by creating a set of contiguous intervals (or bins) and then categorizing the variables into these intervals. This can be particularly useful when you want to categorize a continuous variable into different groups based on ranges. Note that we usually lose information in this process.\n",
    "\n",
    "For our dataset, let's take the 'SalePrice' column, which is continuous, and discretize it into categories like 'Low', 'Medium', 'High', and 'Very High'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4b2bd0-7277-44d5-92a9-28f63ef10eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretizing 'SalePrice' into 4 categories\n",
    "bins = [0, 100000, 200000, 300000, df['SalePrice'].max()]\n",
    "labels = ['Low', 'Medium', 'High', 'Very High']\n",
    "df['SalePrice_category'] = pd.cut(df['SalePrice'], bins=bins, labels=labels, include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1aee19-5644-4cab-8568-9afe466a904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.SalePrice_category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed3b58c-917a-455a-98f8-8ebc02f00cfd",
   "metadata": {},
   "source": [
    "Another useful option is **discretizing by quantiles**. This means dividing the data into intervals based on specific quantile values. This ensures that each bin has (approximately) the same number of data points. The `pandas` library provides a convenient method, `qcut()`, for this purpose.\n",
    "\n",
    "Discretizing by quantiles can be particularly useful when you want to create categories that represent relative rankings (like low, medium, high, etc.) based on the distribution of the data, rather than fixed numeric ranges.\n",
    "\n",
    "**Step 1**: Choose the number of quantiles (or bins). For example, if you want quartiles, you would choose 4 bins. \n",
    "\n",
    "**Step 2**: Use the `qcut()` function from `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd354287-f6b3-4e4a-a4a6-dcc51859b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretizing 'SalePrice' into quartiles\n",
    "df['SalePrice_quantile'] = pd.qcut(df['SalePrice'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "\n",
    "df.SalePrice_category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4514685c-1aee-449d-b3c7-0af19e12d16f",
   "metadata": {},
   "source": [
    "In the above code:\n",
    "- `q=4` indicates that we want to divide the data into 4 quantiles (quartiles).\n",
    "- `labels=['Q1', 'Q2', 'Q3', 'Q4']` provides custom labels for each quantile bin.\n",
    "\n",
    "The resulting 'SalePrice_quantile' column will categorize each house's sale price into one of the four quartiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ab5c02-19b9-4a37-b6e3-e59ff639e207",
   "metadata": {},
   "source": [
    "By discretizing 'SalePrice', we have transformed a continuous variable into categorical bins. This can simplify the analysis by grouping houses into broad price categories. For example, you can now easily analyze the number of houses in each price range or determine if certain features are more common in high-priced houses compared to low-priced ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1e65ac-595c-433e-8a60-44a3c3d4749d",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### 💡 Check for understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59999a74-d893-4b18-b2f0-3dd9633e36b7",
   "metadata": {},
   "source": [
    "Discretize the '1stFlrSF' column (first-floor square feet) into three categories: 'Small', 'Medium', 'Large'. Set the bins such that 'Small' includes sizes up to the 33rd percentile, 'Medium' includes sizes from the 33rd to the 66th percentile, and 'Large' includes sizes from the 66th percentile onward. How many houses fall into each category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4740fb35-abfe-4b88-b007-abf6b62a5d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081e27b3-bb94-4400-97a9-483df3ab1e12",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lesson, we've conducted a comprehensive univariate analysis:\n",
    "\n",
    "- For **categorical variables**, we visualized the distribution of our zoning classifications with bar and pie charts, backed by frequency tables.\n",
    "- For **numerical variables**, we explored the central tendencies, dispersions and shape of distribution of our sale prices, visualized through histograms and box plots.\n",
    "\n",
    "This analysis allows us to deeply understand each variable, laying a strong foundation for subsequent multivariate analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e61c560-bd90-42a7-94a9-d19c50d1b25e",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## 💡 Check for understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbdddec-f56b-44d0-87aa-a7570101d769",
   "metadata": {},
   "source": [
    "**Scenario**:\n",
    "Given the 'TotRmsAbvGrd' column (total rooms above ground), let's dive deep into its univariate characteristics.\n",
    "\n",
    "**Tasks**:\n",
    "\n",
    "1. **Data Aggregation**:\n",
    "    - Create a frequency table for 'TotRmsAbvGrd' to understand the distribution of the number of rooms in houses.\n",
    "    - Calculate the mean, median, mode, variance, and standard deviation of 'TotRmsAbvGrd'.\n",
    "\n",
    "2. **Visualization**:\n",
    "    - Plot a histogram for 'TotRmsAbvGrd' to understand its distribution.\n",
    "    - Plot a box plot for 'TotRmsAbvGrd' to visualize its central tendency, spread, and potential outliers.\n",
    "\n",
    "3. **Interpretation**:\n",
    "    - Is the distribution of the number of rooms skewed? If so, in which direction?\n",
    "    - Based on the histogram and box plot, what can you infer about the common number of rooms above ground in houses? \n",
    "    - Are there any noticeable outliers in the number of rooms? If so, are there more houses with unusually many rooms or unusually few?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff95663d-01b7-47b7-9285-869b8792ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
