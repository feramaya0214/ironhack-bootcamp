{"cells":[{"cell_type":"markdown","metadata":{"id":"66pqqERbR_TC"},"source":["## Ensemble Methods"]},{"cell_type":"markdown","metadata":{"id":"bNwm83KER_TF"},"source":["In our continuous quest to enhance the accuracy and robustness of our predictive models for California housing prices, we delve into the realm of ensemble methods. Ensemble methods, renowned for their capability to combine multiple models to achieve superior predictive performance, offer a promising avenue for refining our housing price predictions."]},{"cell_type":"markdown","metadata":{"id":"cQFKrSS2R_TF"},"source":["#### Loading and preparing the data"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"WQl3rX7NR_TF"},"outputs":[],"source":["from sklearn.datasets import  fetch_california_housing\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.ensemble import BaggingRegressor, RandomForestRegressor,AdaBoostRegressor, GradientBoostingRegressor\n","\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"XDLWGDmER_TG","outputId":"08f3e5b4-d48a-4f27-d30f-57a173d1561a"},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{"id":"at4JizyDR_TI"},"source":["#### Normalization & Feature Selection"]},{"cell_type":"markdown","metadata":{"id":"z06GELOUR_TI"},"source":["Like we did in Feature Engineering lesson, we are going to normalize our data and select a subset of columns as our features."]},{"cell_type":"markdown","metadata":{"id":"mrVsJNFhR_TH"},"source":["#### Train Test Split"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"5UOl_QkoR_TI"},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{"id":"YmBD4IINR_TI"},"source":["Create an instance of the normalizer"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"pOd3-i6FR_TI"},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{"id":"iMZ5ouIsR_TJ"},"source":["## Bagging and Pasting"]},{"cell_type":"markdown","metadata":{},"source":["Bagging involves training multiple instances of the same base model on different subsets of the training data. The final prediction is obtained by averaging or voting over predictions from these models."]},{"cell_type":"markdown","metadata":{"id":"m9MQEw6qR_TJ"},"source":["Just for baseline, our current best model is a Decision Tree with R-Squared of 0.70, lets see how ensembles works"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"hrB1f4B2R_TJ"},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{"id":"14hd5Qq3R_TJ"},"source":["Training Bagging model with our normalized data"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"zuImEavmR_TJ","outputId":"58acd191-3d3d-4af3-de29-cf5bab8852ea"},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{"id":"R9zsbDM8R_TJ"},"source":["Evaluate model's performance"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"oDQMWcvJR_TJ","outputId":"9d0745ec-f60e-4780-af35-828050bb5e26"},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{"id":"OZ1nbY9zR_TJ"},"source":["Combining multiple trees, in this case 100, indeed yield a stronger model, now we are at 0.72 R-Squared!\n","\n","Let's explore more!"]},{"cell_type":"markdown","metadata":{"id":"L_pXy5Z4R_TN"},"source":["In Bagging methods, we have many base estimators, so there is no feature importance method implemented."]},{"cell_type":"markdown","metadata":{},"source":["## Random Patches"]},{"cell_type":"markdown","metadata":{},"source":["While in Bagging/Pasting, we randomize the training data that each predictor (estimator) learns from. However, in a Random Patches Method, we go a step further by also **randomizing the features** that each predictor trains with."]},{"cell_type":"markdown","metadata":{},"source":["- Initialize a Random Forest"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{},"source":["- Training the model"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{},"source":["- Evaluate the model"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{},"source":["By randomizing data also features that every estimators will learn from, we obtain even a better model!\n","\n","We are now at 0.82 R-Squared."]},{"cell_type":"markdown","metadata":{},"source":["## AdaBoost"]},{"cell_type":"markdown","metadata":{},"source":["Now, instead of training our estimators independently by training them in parallel, each estimators will learn at its predecessor's errors and focus on those datapoints where it failed."]},{"cell_type":"markdown","metadata":{},"source":["- Initialize a AdaBoost model"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{},"source":["- Training the model"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{},"source":["- Evaluate the model"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{},"source":["Even better! By randomizing training set, features and also focusing where the previous estimator failed, we obtained a better model!"]},{"cell_type":"markdown","metadata":{},"source":["## Gradient Boosting"]},{"cell_type":"markdown","metadata":{},"source":["Now, each estimator will predict the error caused by its predecessor."]},{"cell_type":"markdown","metadata":{},"source":["- Initialize a AdaBoost model"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{},"source":["- Training the model"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["#your code here"]},{"cell_type":"markdown","metadata":{},"source":["- Evaluate the model"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["#your code here\n"]},{"cell_type":"markdown","metadata":{},"source":["Gradient Boosting compared with AdaBoosting, really doesnt seems doing a great job.\n","\n","**However, note that none of the hyperparameters of all models we've tried where fine tunned.**\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}
